<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/github-pages/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/github-pages/_next/static/media/23041868.55bda878.jpeg"/><link rel="stylesheet" href="/github-pages/_next/static/css/c20b283d6d558cbe.css" data-precedence="next"/><link rel="stylesheet" href="/github-pages/_next/static/css/5283f8f8abdc6c70.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/github-pages/_next/static/chunks/webpack-cb6fde8b9a46de44.js"/><script src="/github-pages/_next/static/chunks/fd9d1056-c96c49782430d626.js" async=""></script><script src="/github-pages/_next/static/chunks/117-2c6d7a579c413924.js" async=""></script><script src="/github-pages/_next/static/chunks/main-app-aaa6492bf6b572b3.js" async=""></script><script src="/github-pages/_next/static/chunks/972-125803dc71d1afa1.js" async=""></script><script src="/github-pages/_next/static/chunks/app/content/posts/%5Bslug%5D/page-fad90fe99cb4469e.js" async=""></script><script src="/github-pages/_next/static/chunks/app/layout-8bef1b5bb8d80373.js" async=""></script><title>ACFHarbinger</title><meta name="description" content="Personal Website"/><meta name="next-size-adjust"/><script src="/github-pages/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_b0711f bg-slate-50 dark:bg-slate-950 h-full"><header class="w-full bg-white/80 dark:bg-slate-900/80 backdrop-blur-md border-b border-slate-200 dark:border-slate-800 sticky top-0 z-50 **md:hidden**"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><div class="flex-shrink-0"><a class="font-bold text-xl text-slate-900 dark:text-white flex items-center gap-2" href="/github-pages"><span class="text-blue-500">ACF</span><span>Harbinger</span></a></div><div class="flex items-center"><button class="p-2 rounded-md text-slate-600 dark:text-slate-400 hover:bg-slate-100 dark:hover:bg-slate-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-blue-500"><span class="sr-only">Open main menu</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button></div></div></div></header><div class="h-screen overflow-hidden transition-colors duration-300 font-sans bg-slate-50 text-slate-800"><div class="flex h-full max-w-7xl mx-auto"><div class="hidden lg:block h-full"><aside class="hidden lg:flex flex-col w-20 h-full overflow-y-auto border-r border-slate-200 dark:border-slate-800 bg-white/50 dark:bg-slate-900/50 backdrop-blur-xl px-4 transition-all duration-300 ease-in-out relative"><button class="absolute top-4 -right-3 z-10 p-1 rounded-full bg-slate-100 dark:bg-slate-800 border border-slate-200 dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700 transition-colors hidden lg:block" aria-label="Expand Sidebar"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left transition-transform duration-300 rotate-180"><path d="m15 18-6-6 6-6"></path></svg></button><div class="pt-4"><div><div class="relative rounded-full overflow-hidden shadow-lg ring-4 ring-white dark:ring-slate-800 transition-all duration-300 mb-4 bg-slate-200 dark:bg-slate-700 h-12 w-12"><img src="/github-pages/_next/static/media/23041868.55bda878.jpeg" alt="ACFHarbinger" class="h-full w-full object-cover"/></div></div><nav class="space-y-2 mt-8"><a href="/github-pages" title="Home" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-home"><path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path><polyline points="9 22 9 12 15 12 15 22"></polyline></svg></a><a href="/github-pages/content/about" title="About" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-user"><path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg></a><a href="/github-pages/content/projects" title="Projects" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-code"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg></a><a href="/github-pages/content/reports" title="Reports" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg></a><a href="/github-pages/content/tools" title="Tools" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-wrench"><path d="M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z"></path></svg></a><a href="/github-pages/content/posts" title="Posts" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        bg-blue-50 text-blue-600 dark:bg-slate-800 dark:text-blue-400"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-pen-tool"><path d="M15.707 21.293a1 1 0 0 1-1.414 0l-1.586-1.586a1 1 0 0 1 0-1.414l5.586-5.586a1 1 0 0 1 1.414 0l1.586 1.586a1 1 0 0 1 0 1.414z"></path><path d="m18 13-1.375-6.874a1 1 0 0 0-.746-.776L3.235 2.028a1 1 0 0 0-1.207 1.207L5.35 15.879a1 1 0 0 0 .776.746L13 18"></path><path d="m2.3 2.3 7.286 7.286"></path><circle cx="11" cy="11" r="2"></circle></svg></a><a href="/github-pages/content/media" title="Media" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        justify-center px-0 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-video"><path d="m16 13 5.223 3.482a.5.5 0 0 0 .777-.416V7.87a.5.5 0 0 0-.752-.432L16 10.5"></path><rect x="2" y="6" width="14" height="12" rx="2"></rect></svg></a></nav></div><div class="flex-grow"></div><div class="pb-4"><div class="grid grid-cols-1 gap-y-3 justify-items-center w-full text-slate-400"><a href="https://github.com/acfharbinger" target="_blank" rel="noopener noreferrer" class="hover:text-slate-900 dark:hover:text-white transition-colors flex items-center justify-center p-2 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-800" aria-label="Github"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="#" class="hover:text-blue-400 transition-colors flex items-center justify-center p-2 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-800" aria-label="Twitter"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a><a href="#" class="hover:text-blue-600 transition-colors flex items-center justify-center p-2 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-800" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><button class="hover:text-yellow-500 transition-colors flex items-center justify-center border-0 focus:outline-none p-2 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-800" aria-label="Toggle Dark Mode"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></button></div></div></aside></div><div class="flex-1 h-full overflow-y-auto"><main class="flex-1 p-6 lg:p-12 w-full max-w-4xl mx-auto pt-20 lg:pt-12"><div class="max-w-4xl mx-auto py-12 px-4"><div class="mb-6"><a class="flex items-center text-slate-600 dark:text-slate-400 hover:text-blue-500 transition-colors" href="/github-pages/content/posts"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left w-4 h-4 mr-2"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg>Back to Posts</a></div><div class="bg-white dark:bg-slate-900 rounded-xl shadow-lg overflow-hidden border border-slate-200 dark:border-slate-800 p-8"><div class="post-container">

<header>
    <h1>Notes on Reinforcement Learning: An Introduction (2nd edition)</h1>
    <div class="meta">
        <strong>Date:</strong> 2024-10-31 <br>
        <strong>Categories:</strong> ML, RL, DL
    </div>
</header>

<main>
    <div style="display:none">
        $$
        \DeclareMathOperator*{\argmin}{arg\,min}
        \DeclareMathOperator*{\argmax}{arg\,max}
        $$
    </div>

    <p>Here are some notes I took when reading the second edition of the <a href="http://acfharbinger.github.io/github-pages/assets/docs/literature/books/RLbook2020.pdf" onerror="this.href='http://localhost:4000/assets/docs/literature/books/RLbook2020.pdf'">Reinforcement Learning: An Introduction</a> book.<br>
    If you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!<br>
    It does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there.</p>

    <h1>Chapter Index</h1>
    <ol>
        <li><a href="#chapter-1-introduction">Chapter 1: Introduction</a></li>
        <li><a href="#part-i-tabular-solution-methods">Part I: Tabular Solution Methods</a>
            <ol>
                <li><a href="#chapter-2-multi-armed-bandits">Chapter 2: Multi-armed Bandits</a></li>
                <li><a href="#chapter-3-finite-markov-decision-processes">Chapter 3: Finite Markov Decision Processes</a></li>
                <li><a href="#chapter-4-dynamic-programming">Chapter 4: Dynamic Programming</a></li>
                <li><a href="#chapter-5-monte-carlo-methods">Chapter 5: Monte Carlo Methods</a></li>
                <li><a href="#chapter-6-temporal-difference-learning">Chapter 6: Temporal-Difference Learning</a></li>
                <li><a href="#chapter-7-n-step-bootstrapping">Chapter 7: n-step Bootstrapping</a></li>
                <li><a href="#chapter-8-planning-and-learning-with-tabular-methods">Chapter 8: Planning and Learning with Tabular Methods</a></li>
            </ol>
        </li>
        <li><a href="#part-ii-approximate-solution-methods">Part II: Approximate Solution Methods</a>
            <ol>
                <li><a href="#chapter-9-on-policy-prediction-with-approximation">Chapter 9: On-policy Prediction with Approximation</a></li>
                <li><a href="#chapter-10-on-policy-control-with-approximation">Chapter 10: On-policy Control with Approximation</a></li>
                <li><a href="#chapter-11-off-policy-methods-with-approximation">Chapter 11: *Off-policy Methods with Approximation</a></li>
                <li><a href="#chapter-12-eligibility-traces">Chapter 12: Eligibility Traces</a></li>
                <li><a href="#chapter-13-policy-gradient-methods">Chapter 13: Policy Gradient Methods</a></li>
            </ol>
        </li>
        <li><a href="#part-iii-looking-deeper">Part III: Looking Deeper</a>
            <ol>
                <li><a href="#chapter-14-psychology">Chapter 14: Psychology</a></li>
                <li><a href="#chapter-15-neuroscience">Chapter 15: Neuroscience</a></li>
                <li><a href="#chapter-16-applications-and-case-studies">Chapter 16: Applications and Case Studies</a></li>
                <li><a href="#chapter-17-frontiers">Chapter 17: Frontiers</a></li>
            </ol>
        </li>
    </ol>

    <h1 id="chapter-1-introduction">Chapter 1: Introduction</h1>
    <p>Def. <strong>Reinforcement Learning (RL)</strong>: an agent learns how to map situations to actions through <em>trial-and-error</em> or <em>planned</em> interaction with a (possibly) uncertain environment, so as to maximize a numerical reward value (i.e., achieve his goal or goals).</p>
    <ul>
        <li><em>Delayed reward</em> is another important characteristic of RL, since any action taken may influence (not only the immediate reward value, but also) any subsequent rewards;</li>
        <li>RL can be formalized as the optimal control of incompletely-known Markov Decision Processes (MDPs).</li>
    </ul>
    <p>Besides RL, other <strong>Machine Learning (ML)</strong> paradigms include <em>Supervised Learning</em> - predicting the correct label, given the corresponding set of features - and <em>Unsupervised Learning</em> - finding hidden patterns in a collection of unlabeled features.</p>
    <p>A challenge unique to the RL paradigm is that of the trade-off between <strong>exploration versus exploitation</strong>. This challenge arises due to the fact that an agent prefers to take the actions that have previously given the highest rewards (<em>exploitation</em>), but it must also try out other actions in order to have more knowledge about which actions it should select (<em>exploration</em>).</p>
    <p>A RL system has four main elements beyond the interactive <strong>agent</strong> and the <strong>environment</strong>, which are:</p>
    <ul>
        <li>A <strong>policy</strong> $\pi_t: s \rightarrow a$, which in stochastic cases specifies a probability for each action;</li>
        <li>A <strong>reward</strong> $r(s, a)$, an immediate signal that specifies how good it is for an agent to have chosen a certain action in a given state (may also be stochastic);</li>
        <li>A <strong>value function</strong> $v(s)$ that specifies the total reward an agent is expected to accumulate in the future if he starts at a given state, i.e., predicted long-term reward;</li>
        <li>A (optional) <strong>world model</strong> used by model-based methods (opposed to purely trial-and-error model-free methods) for planning.</li>
    </ul>

    <h1 id="part-i-tabular-solution-methods">Part I: Tabular Solution Methods</h1>

    <h2 id="chapter-2-multi-armed-bandits">Chapter 2: Multi-armed Bandits</h2>
    <p><em>Non-associative</em> setting: a problem setting that involves learning to act in only 1 situation.</p>
    <p><em>Associative</em> setting: a problem setting where the best action depends on the situation.</p>

    <h3>Section 2.1: A $k$-armed Bandit Problem</h3>
    <p>Setting of the $k$-armed bandit learning problem (analogous to a slot machine with $k$ levers):</p>
    <ol>
        <li>Choose 1 action from among $k$ different options;</li>
        <li>Receive a (numerical) reward from a stationary probability distribution which depends on the action selected;</li>
        <li>Repeat steps 1 and 2 with the purpose of maximizing the expected total reward over some time period (e.g., 1000 action selections or <em>time steps</em>).</li>
    </ol>
    <p><strong>Value</strong> of an action: the expected or mean reward received if that action is selected</p>
    <p>Letting $A_t$ be the action taken at time step $t$ and $R_t$ the corresponding reward, then the value $q^{*}(a)$ of an arbitrary action $a$ is given by:</p>
    $$
    \begin{equation}
        q^{*} (a) \doteq \mathbb{E} [R_t | A_t = a].
    \end{equation}
    $$
    <p>Since we do not know the true value of each action, we need to estimate them in such a way that the estimates are close to the real values. The estimated value of an action $a$ at time step $t$ is denoted by $Q_t (a)$.</p>
    <p><strong>Greedy</strong> action: the action with the highest estimated value at a given time step</p>
    <ul>
        <li>Choosing this action equates to the agent <strong>exploiting</strong> his current knowledge of the values of the actions;</li>
        <li>Selecting 1 of the non-greedy actions enables the agent to improve his estimates of the non-greedy action's value, i.e., <strong>exploration</strong>;</li>
        <li>Exploitation maximizes the reward on 1 step, but it needs to be intercalated with exploration steps so as to maximize the greater total reward in the long term.</li>
    </ul>

    <h3>Section 2.2: Action-value Methods</h3>
    <p>Def. <strong>Action-value Methods</strong>: methods used to estimate the values of actions and to use those estimates to select an action to take at a given time step.</p>
    <p>Letting $\mathbb{1}_{predicate}$ be the random variable which equals $1$ if the $predicate$ is true and $0$ otherwise, the value of an action can be estimated by averaging the rewards received:</p>
    $$
    \begin{equation}
    Q_t (a) \doteq \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac{\sum_{i = 1}^{t - 1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i = 1}^{t - 1} \mathbb{1}_{A_i = a}}.
    \end{equation}
    $$
    <p>If the denominator is zero (action has never been taken), then $Q_t(a)$ is defined as an arbitrary default value (e.g., zero). By the law of large numbers, as the denominator goes to infinity, $Q_t(a)$ converges to $q^{*}(a)$. This is called the <em>sample-average</em> method for estimating action values.</p>
    <p>The simplest action selection rule is to always select a greedy action and - if there is more than 1 action with the same highest value - to break ties in some arbitrary way (e.g., randomly). This action selection method can be written as:</p>
    $$
    \begin{equation}
    A_t = \argmax_a Q_t (a).
    \end{equation}
    $$
    <p>This selection method never performs exploration. A simple alternative that does so is to select the greedy action most of the time (probability $1 - \epsilon$) and (with probability $\epsilon$) to randomly select any possible action with equal probability. Methods that use this near-greedy action selection rule are dubbed $\epsilon$-greedy methods.</p>

    <h3>Section 2.3: The 10-armed Test-bed</h3>
    <p><strong>Non-stationary</strong> setting: problem setting where the true values of the actions (or the reward probabilities) change over time.</p>
    <p>Given a set of 2000 randomly generated $k$-armed bandit problems (with $k = 10$), for each problem in the set, the action values $q^{*}(a), \ a = \{1, 2, \dots, 10\},$ were selected from a normal (Gaussian) distribution with $\mu = 0, \  \sigma^2 = 1$. When a learning method is applied to this problem selects action $A_t$ at time step $t$, the actual reward ($R_t$) was drawn from a normal distribution with $\mu = q^{*}(A_t), \ \sigma^2 = 1$.</p>
    <p>The performance of the learning methods is measured as it improves with experience over 1000 time steps of the bandit problem, which makes up a single run. To obtain an accurate measure of the learning algorithms' behavior, 2000 runs are performed and the results for the bandit problems are averaged.</p>
    <p>A greedy action selection method is compared against 2 $\epsilon$-greedy methods (with $\epsilon = 0.01 \lor \epsilon = 0.1$). All methods begin with initial action-value estimates of zero and update these estimates using the sample-average technique.</p>
    <p>While the greedy method improved slightly faster than the other 2, it converged to a reward-per-step of 1, which is lower than the best value of around 1.54 achieved by the $\epsilon$-greedy method (with $\epsilon = 0.1$). The method with $\epsilon = 0.1$ improved faster than the method with $\epsilon = 0.01$, since it explored more earlier. However, the method with $\epsilon = 0.01$ converges to a higher reward-per-step in the long run, since the method with $\epsilon = 0.1$ never selects the optimal action more than 91% of the time. <br>
    It is possible to perform $\epsilon$ annealing to try to get fast learning at the start combined with convergence to a higher reward average.</p>
    <p>It takes more exploration to find the optimal actions in cases with noisy rewards (i.e., high reward variance), meaning that $\epsilon$-greedy methods perform even better in those cases, when compared to the greedy method. Also, although the greedy method is theoretically optimal in the deterministic case (i.e., with $\sigma^2 = 0$), this property does not hold in non-stationary bandit problems, making exploration a necessity even in deterministic settings.</p>

    <h3>Section 2.4: Incremental Implementation</h3>
    <p>For a single action, let $R_i$ denote the reward received after the $i^{th}$ selection of <em>this action</em> and $Q_n$ the estimate of its action value after it has been selected $n - 1$ times, written as:</p>
    $$
    \begin{equation}
    Q_n \doteq \frac{R_1 + R_2 + \dots + R_{n - 1}}{n - 1}.
    \end{equation}
    $$
    <p>Instead of maintaining a record of all the rewards and performing the computation for the estimated value whenever needed (resulting in the growth of both computational and memory requirements), we can devise incremental formulas to update the averages with a small and constant computation to process each new reward. Given $Q_n$ and the $n^{th}$ reward $R_n$, the new average of all $n$ rewards can be computed as:</p>
    $$
    \begin{align}
        Q_{n + 1} &= \frac{1}{n} \sum_{i = 1}^n R_i \nonumber\\
        &= \frac{1}{n}(R_n + \sum_{i = 1}^{n - 1} R_i) \nonumber\\
        &= \frac{1}{n}(R_n + (n - 1) \cdot \frac{1}{n - 1} \cdot \sum_{i = 1}^{n - 1} R_i) \nonumber\\
        &= \frac{1}{n} (R_n + (n - 1) \cdot Q_n) \nonumber\\
        &= \frac{1}{n} (R_n + n \cdot Q_n - Q_n) \nonumber\\
        &= Q_n + \frac{1}{n} [R_n - Q_n], \ n > 1 \\
        Q_2 &= R_1, \ Q_1 \in \mathbb{R}.
    \end{align}
    $$
    <p>This implementation only needs memory for $Q_n$ and $n$, and only performs a small computation for each new reward. 
    The general form of the previous update rule is given by:</p>
    $$
    \begin{equation}
    NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate],
    \end{equation}
    $$
    <p>where $[Target - OldEstimate]$ is an <em>error</em> in the estimate, which is reduced by taking a step towards the (possibly noisy) target value.
    The step-size parameter is generally denoted by $\alpha$ or $\alpha_t (a)$.</p>

    <pre><code>def bandit_problem(int k, float epsilon, bandits):
    Q_a = [0]*k;
    N_a = [0]*k;

    while(True):
        if random_float(0, 1) <= epsilon:
            A = random_int(0, k)
        else:
            A = argmax(Q_a)
        R = bandits[A].get_reward()
        N_a[A]++;
        Q_a[A] += (1/N_a[A]) * (R - Q_a[A]);</code></pre>

    <h2 id="chapter-3-finite-markov-decision-processes">Chapter 3: Finite Markov Decision Processes</h2>
    <p>Markov Decision Processes (MDPs) are a formalization of sequential decision making where actions influence not only the immediate reward, but also future rewards. As such, this is an associative problem that takes into account the need to trade-off immediate and delayed reward. While in bandit problems we estimated the value $q^{*}(a), \ \forall a \in \mathcal{A},$ in an MDP, we estimate the value $q^{*}(s, a), \ \forall a \in \mathcal{A}, \forall s \in \mathcal{S},$ or the value $v^{*}(s), \forall s \in \mathcal{S}$ given optimal action selections. Such state-dependent values are important to assign credit for long-term rewards to individual action selections.</p>

    <h3>Section 3.1: The Agent-Environment Interface</h3>
    <p>An MDP involves a learner and decision maker (i.e., the <em>agent</em>) that interacts with its surroundings (i.e., the <em>environment</em>) by continually selecting actions and having the environment respond by presenting new situations (or states) to the agent and giving rise to rewards, which the agent seeks to maximize over time. This process is illustrated in Figure 1.</p>

    <figure align='center'>
        <img alt="The agent-environment interaction in a Markov decision process." src="../../images/rl_mdp.png">
        <figcaption>Figure 1: The agent-environment interaction in a MDP.</figcaption>
    </figure>

    <p>The agent and environment interact with each other in a sequence of discrete time steps $t = 0, 1, \dots, n.$ At each time step $t,$ the agent receives a representation of the environment's <em>state</em> $S_t \in \mathcal{S},$ and on that basis selects an <em>action</em> $A_t \in \mathcal{A}(s).$ At the next time step $t + 1,$ the agent receives a reward $R_{t + 1} \in \mathcal{R} \subset \mathbb{R}$ and finds itself in another state $s_{t+1}.$  Then, the MDP and agent give rise to a sequence or <em>trajectory</em> like this:</p>
    $$
    \begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots
    \end{equation}
    $$
    <p>In a <em>finite</em> MDP, the random variables $R_t$ and $S_t$ have well defined discrete probability distributions that depend only on the previous state and action, i.e., for particular values of these random variables $s' \in \mathcal{A}, \ r \in \mathcal{R},$ there is a probability of those values occurring at time step $t,$ given particular values of the previous state and action:</p>
    $$
    \begin{equation}
    p(s', r \vert s, a) \doteq P(S_t = s', R_t \vert S_{t-1} = s, A_{t-1} = a), \ \forall s', s \in \mathcal{S}, \forall r \in \mathcal{R}, \forall a \in \mathcal{A}(s).
    \end{equation}
    $$
    <p>The function $p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$, which completely characterizes the MDP environment's <em>dynamics</em>, is an ordinary deterministic function with four arguments. This function $p$ specifies a probability distribution for each choice of $s$ and $a$, i.e.,</p>
    $$
    \begin{equation}
    \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \vert s, a) = 1, \ \forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s).
    \end{equation}
    $$
    <p><strong>Markov property</strong>: the state must include information about all aspects of the past agent-environment interaction that make a difference for the future. In practice, this means that the probability of each possible value for $S_t$ and $R_t$ depends only on the previous state $S_{t-1}$ and action $A_{t-1}$.</p>
    <ul>
        <li>While most methods in this book assume this property to be true, there are methods that don't rely on it. <a href="#chapter-17-frontiers">Chapter 17</a> considers how to efficiently learn a Markov state from non-Markov observations.</li>
    </ul>
    <p>From the dynamics function $p,$ we can compute the <em>state-transition probabilities</em> $p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1],$</p>
    $$
    \begin{equation}
    p(s' \vert s, a) \doteq P(S_t = s' \vert S_{t-1} = s, A_{t-1} = a) = \sum_{r \in \mathcal{R}} p(s', r \vert s, a).
    \end{equation}
    $$
    <p>We can also compute the expected rewards for state-action pairs as a two-argument function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$:</p>
    $$
    \begin{equation}
    r(s, a) \doteq \mathbb{E} [R_t \vert S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r \vert s, a),
    \end{equation}
    $$
    <p>and the expected rewards for the state-action-new_state triples as a three-argument function $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R},$</p>
    $$
    \begin{equation}
    r(s, a, s') \doteq \mathbb{E} [R_t \vert S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r \vert s, a)}{p(s' \vert s, a)}.
    \end{equation}
    $$

    <h2 id="chapter-4-dynamic-programming">Chapter 4: Dynamic Programming</h2>
    <p>The key idea of DP (and RL in general) is the use of value functions to both structure and organize the search for good policies. DP algorithms are obtained by turning Bellman equations into assignments, i.e., into update rules for improving approximations of the desired value functions. Remember that we can easily obtain optimal policies if we find the optimal value functions, $v^* \lor q^*$, which satisfy the Bellman optimality equations, $\forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s), s' \in \mathcal{S}^+$, such that:</p>
    $$
    \begin{align}
    v^*(s) &= \max_a \mathbb{E}[R_{t+1} + \gamma \cdot v^*(S_{t+1} \vert S_t = s, A_t = a)] \nonumber\\
        &= \max_a \sum_{s', r} p(s' r, \vert s, a) [r + \gamma \cdot v^*(s')],\\
        &\lor \nonumber\\
    q^*(s, a) &= \mathbb{E}[R_{t+1} + \gamma \max_{'a} q^*(S_{t+1}, a') \vert S_t = s, S_t =a], \nonumber\\
        &= \sum_{s', r} p(s', r \vert s, a) [r + \gamma \max_{a'} q^*(s', a')].
    \end{align}
    $$

    <h3>Section 4.1: Policy Evaluation (Prediction)</h3>
    <p><strong>Policy evaluation:</strong> computing the state-value function $v^{\pi}$ for an arbitrary policy $\pi$. This is also referred to as the <em>prediction problem</em>. Recall from <a href="#chapter-3-finite-markov-decision-processes">Chapter 3</a> that, $\forall s \in \mathcal{S}$,</p>
    $$
    \begin{align}
    v^{\pi}(s) &\doteq \mathbb{E}_{\pi}[G_t \vert S_t = s], \nonumber\\
        &= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \vert S_t = s], \nonumber\\
        &= \mathbb{E}_{\pi}[R_{t+1} + \gamma v^{\pi} (S_{t+1} \vert S_t = s)], \\
        &= \sum_a \pi(a \vert s) \sum_{s', r} p(s', r \vert s, a)[r + \gamma v^{\pi}(s')],
    \end{align}
    $$
    <p>where $\pi(a\vert s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$, and the expectations are subscripted by $\pi$ to indicate that they are conditional on following policy $\pi$. If $\gamma < 1$ or eventual termination is guaranteed from all states under the policy $\pi$, then w.h.t. that $v^{\pi}$ exists and is unique.</p>

    <pre><code class="language-python">def iterative_policy_evaluation(policy, mdp, theta, gamma):
    assert theta > 0;
    
    state_values[len(mdp.states)] = 0;
    for x in range(len(mdp.states) - 1):
        state_values[x] = random_value();
        
    gradient = 0;
    while gradient >= theta:
        gradient = 0;
        for s in mdp.states:
            v = state_values[s.id];
            state_values[s.id] = 0;
            for tmp_s in mdp.state:
                for a in mdp.actions:
                    state_values[s.id] += policy[s.id][a.id] * mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);
            
            gradient = max(gradient, absolute_value(v - state_values[s.id]));

    return state_values;</code></pre>

    <h2 id="chapter-13-policy-gradient-methods">Chapter 13: Policy Gradient Methods</h2>
    <p><strong>Policy gradient</strong> methods seek to learn an approximation to the policy by maximizing performance. Their updates approximate gradient ascent such as:</p>
    $$
    \begin{equation}
    \theta_{t+1} = \theta_t + \alpha \cdot \hat{\nabla J(\theta_t)}, \ \hat{\nabla J(\theta_t)} \in \mathbb{R}^{d'}.
    \end{equation}
    $$
    <p>Methods that learn an approximation of both policy and value functions are called <strong>actor-critic</strong> methods. The <em>actor</em> is a reference to the learned policy and <em>critic</em> a reference to the learned (state-)value function.</p>

    <h3>Section 13.1: Policy Approximation and its Advantages</h3>
    <p>The policy can be parameterized in any way, as long as 2 conditions are met</p>
    <ul>
        <li>As long as $\pi(a\vert s, \theta)$ is differentiable w.r.t. its parameters:</li>
    </ul>
    $$
    \forall s \in S \ \forall a \in A(s) \ \exists \ \nabla \pi (a|s, \theta) : |\nabla \pi (a|s, \theta)| < \infty \ \wedge \theta \in \mathbb{R}^{d'};
    $$
    <ul>
        <li>As long as it continues to perform exploration (to avoid a deterministic policy):</li>
    </ul>
    $$
    \pi (a|s, \theta) \ \in \  ]0, 1[.
    $$
    <p>For small-to-medium discrete action spaces, it is common to form parameterized numerical preferences $h(s, a, \theta) \in \mathbb{R}$ for each $(s, a)$ pair. The probabilities of each action being selected can be calculated with, e.g., an exponential softmax distribution:</p>
    $$
    \begin{equation}
    \pi (a|s, \theta) \doteq \frac{\exp(h(s,a,\theta))}{\sum_b \exp(h(s,b,\theta))}.
    \end{equation}
    $$
    <p>This kind of policy parameterization is called <em>softmax in action preferences</em>.</p>

    <!-- Content continues similarly for other chapters -->

</main>

</div></div></div><footer class="border-t border-slate-200 dark:border-slate-800 pt-8 mt-20 pb-8 text-center md:text-left text-slate-500 text-sm"><div class="flex flex-col md:flex-row justify-between items-center"><p>Â© <!-- -->2025<!-- --> ACFHarbinger. All rights reserved.</p><div class="mt-4 md:mt-0 flex items-center gap-6"><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">RSS</a><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">Privacy</a><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">Sitemap</a><button class="hover:text-yellow-500 transition-colors flex items-center justify-center" aria-label="Toggle Dark Mode"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></button></div></div></footer></main></div></div></div><script src="/github-pages/_next/static/chunks/webpack-cb6fde8b9a46de44.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/github-pages/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/github-pages/_next/static/css/c20b283d6d558cbe.css\",\"style\"]\n3:HL[\"/github-pages/_next/static/css/5283f8f8abdc6c70.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[2846,[],\"\"]\n6:I[2972,[\"972\",\"static/chunks/972-125803dc71d1afa1.js\",\"318\",\"static/chunks/app/content/posts/%5Bslug%5D/page-fad90fe99cb4469e.js\"],\"\"]\n7:I[8548,[\"972\",\"static/chunks/972-125803dc71d1afa1.js\",\"318\",\"static/chunks/app/content/posts/%5Bslug%5D/page-fad90fe99cb4469e.js\"],\"default\"]\n9:I[4707,[],\"\"]\nb:I[6423,[],\"\"]\nc:I[8212,[\"972\",\"static/chunks/972-125803dc71d1afa1.js\",\"185\",\"static/chunks/app/layout-8bef1b5bb8d80373.js\"],\"default\"]\nd:I[1403,[\"972\",\"static/chunks/972-125803dc71d1afa1.js\",\"185\",\"static/chunks/app/layout-8bef1b5bb8d80373.js\"],\"default\"]\nf:I[1060,[],\"\"]\n8:T5e38,"])</script><script>self.__next_f.push([1,"\n\n\u003cheader\u003e\n    \u003ch1\u003eNotes on Reinforcement Learning: An Introduction (2nd edition)\u003c/h1\u003e\n    \u003cdiv class=\"meta\"\u003e\n        \u003cstrong\u003eDate:\u003c/strong\u003e 2024-10-31 \u003cbr\u003e\n        \u003cstrong\u003eCategories:\u003c/strong\u003e ML, RL, DL\n    \u003c/div\u003e\n\u003c/header\u003e\n\n\u003cmain\u003e\n    \u003cdiv style=\"display:none\"\u003e\n        $$\n        \\DeclareMathOperator*{\\argmin}{arg\\,min}\n        \\DeclareMathOperator*{\\argmax}{arg\\,max}\n        $$\n    \u003c/div\u003e\n\n    \u003cp\u003eHere are some notes I took when reading the second edition of the \u003ca href=\"http://acfharbinger.github.io/github-pages/assets/docs/literature/books/RLbook2020.pdf\" onerror=\"this.href='http://localhost:4000/assets/docs/literature/books/RLbook2020.pdf'\"\u003eReinforcement Learning: An Introduction\u003c/a\u003e book.\u003cbr\u003e\n    If you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!\u003cbr\u003e\n    It does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there.\u003c/p\u003e\n\n    \u003ch1\u003eChapter Index\u003c/h1\u003e\n    \u003col\u003e\n        \u003cli\u003e\u003ca href=\"#chapter-1-introduction\"\u003eChapter 1: Introduction\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#part-i-tabular-solution-methods\"\u003ePart I: Tabular Solution Methods\u003c/a\u003e\n            \u003col\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-2-multi-armed-bandits\"\u003eChapter 2: Multi-armed Bandits\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-3-finite-markov-decision-processes\"\u003eChapter 3: Finite Markov Decision Processes\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-4-dynamic-programming\"\u003eChapter 4: Dynamic Programming\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-5-monte-carlo-methods\"\u003eChapter 5: Monte Carlo Methods\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-6-temporal-difference-learning\"\u003eChapter 6: Temporal-Difference Learning\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-7-n-step-bootstrapping\"\u003eChapter 7: n-step Bootstrapping\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-8-planning-and-learning-with-tabular-methods\"\u003eChapter 8: Planning and Learning with Tabular Methods\u003c/a\u003e\u003c/li\u003e\n            \u003c/ol\u003e\n        \u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#part-ii-approximate-solution-methods\"\u003ePart II: Approximate Solution Methods\u003c/a\u003e\n            \u003col\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-9-on-policy-prediction-with-approximation\"\u003eChapter 9: On-policy Prediction with Approximation\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-10-on-policy-control-with-approximation\"\u003eChapter 10: On-policy Control with Approximation\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-11-off-policy-methods-with-approximation\"\u003eChapter 11: *Off-policy Methods with Approximation\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-12-eligibility-traces\"\u003eChapter 12: Eligibility Traces\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-13-policy-gradient-methods\"\u003eChapter 13: Policy Gradient Methods\u003c/a\u003e\u003c/li\u003e\n            \u003c/ol\u003e\n        \u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#part-iii-looking-deeper\"\u003ePart III: Looking Deeper\u003c/a\u003e\n            \u003col\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-14-psychology\"\u003eChapter 14: Psychology\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-15-neuroscience\"\u003eChapter 15: Neuroscience\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-16-applications-and-case-studies\"\u003eChapter 16: Applications and Case Studies\u003c/a\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ca href=\"#chapter-17-frontiers\"\u003eChapter 17: Frontiers\u003c/a\u003e\u003c/li\u003e\n            \u003c/ol\u003e\n        \u003c/li\u003e\n    \u003c/ol\u003e\n\n    \u003ch1 id=\"chapter-1-introduction\"\u003eChapter 1: Introduction\u003c/h1\u003e\n    \u003cp\u003eDef. \u003cstrong\u003eReinforcement Learning (RL)\u003c/strong\u003e: an agent learns how to map situations to actions through \u003cem\u003etrial-and-error\u003c/em\u003e or \u003cem\u003eplanned\u003c/em\u003e interaction with a (possibly) uncertain environment, so as to maximize a numerical reward value (i.e., achieve his goal or goals).\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003e\u003cem\u003eDelayed reward\u003c/em\u003e is another important characteristic of RL, since any action taken may influence (not only the immediate reward value, but also) any subsequent rewards;\u003c/li\u003e\n        \u003cli\u003eRL can be formalized as the optimal control of incompletely-known Markov Decision Processes (MDPs).\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eBesides RL, other \u003cstrong\u003eMachine Learning (ML)\u003c/strong\u003e paradigms include \u003cem\u003eSupervised Learning\u003c/em\u003e - predicting the correct label, given the corresponding set of features - and \u003cem\u003eUnsupervised Learning\u003c/em\u003e - finding hidden patterns in a collection of unlabeled features.\u003c/p\u003e\n    \u003cp\u003eA challenge unique to the RL paradigm is that of the trade-off between \u003cstrong\u003eexploration versus exploitation\u003c/strong\u003e. This challenge arises due to the fact that an agent prefers to take the actions that have previously given the highest rewards (\u003cem\u003eexploitation\u003c/em\u003e), but it must also try out other actions in order to have more knowledge about which actions it should select (\u003cem\u003eexploration\u003c/em\u003e).\u003c/p\u003e\n    \u003cp\u003eA RL system has four main elements beyond the interactive \u003cstrong\u003eagent\u003c/strong\u003e and the \u003cstrong\u003eenvironment\u003c/strong\u003e, which are:\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003eA \u003cstrong\u003epolicy\u003c/strong\u003e $\\pi_t: s \\rightarrow a$, which in stochastic cases specifies a probability for each action;\u003c/li\u003e\n        \u003cli\u003eA \u003cstrong\u003ereward\u003c/strong\u003e $r(s, a)$, an immediate signal that specifies how good it is for an agent to have chosen a certain action in a given state (may also be stochastic);\u003c/li\u003e\n        \u003cli\u003eA \u003cstrong\u003evalue function\u003c/strong\u003e $v(s)$ that specifies the total reward an agent is expected to accumulate in the future if he starts at a given state, i.e., predicted long-term reward;\u003c/li\u003e\n        \u003cli\u003eA (optional) \u003cstrong\u003eworld model\u003c/strong\u003e used by model-based methods (opposed to purely trial-and-error model-free methods) for planning.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003ch1 id=\"part-i-tabular-solution-methods\"\u003ePart I: Tabular Solution Methods\u003c/h1\u003e\n\n    \u003ch2 id=\"chapter-2-multi-armed-bandits\"\u003eChapter 2: Multi-armed Bandits\u003c/h2\u003e\n    \u003cp\u003e\u003cem\u003eNon-associative\u003c/em\u003e setting: a problem setting that involves learning to act in only 1 situation.\u003c/p\u003e\n    \u003cp\u003e\u003cem\u003eAssociative\u003c/em\u003e setting: a problem setting where the best action depends on the situation.\u003c/p\u003e\n\n    \u003ch3\u003eSection 2.1: A $k$-armed Bandit Problem\u003c/h3\u003e\n    \u003cp\u003eSetting of the $k$-armed bandit learning problem (analogous to a slot machine with $k$ levers):\u003c/p\u003e\n    \u003col\u003e\n        \u003cli\u003eChoose 1 action from among $k$ different options;\u003c/li\u003e\n        \u003cli\u003eReceive a (numerical) reward from a stationary probability distribution which depends on the action selected;\u003c/li\u003e\n        \u003cli\u003eRepeat steps 1 and 2 with the purpose of maximizing the expected total reward over some time period (e.g., 1000 action selections or \u003cem\u003etime steps\u003c/em\u003e).\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003e\u003cstrong\u003eValue\u003c/strong\u003e of an action: the expected or mean reward received if that action is selected\u003c/p\u003e\n    \u003cp\u003eLetting $A_t$ be the action taken at time step $t$ and $R_t$ the corresponding reward, then the value $q^{*}(a)$ of an arbitrary action $a$ is given by:\u003c/p\u003e\n    $$\n    \\begin{equation}\n        q^{*} (a) \\doteq \\mathbb{E} [R_t | A_t = a].\n    \\end{equation}\n    $$\n    \u003cp\u003eSince we do not know the true value of each action, we need to estimate them in such a way that the estimates are close to the real values. The estimated value of an action $a$ at time step $t$ is denoted by $Q_t (a)$.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eGreedy\u003c/strong\u003e action: the action with the highest estimated value at a given time step\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003eChoosing this action equates to the agent \u003cstrong\u003eexploiting\u003c/strong\u003e his current knowledge of the values of the actions;\u003c/li\u003e\n        \u003cli\u003eSelecting 1 of the non-greedy actions enables the agent to improve his estimates of the non-greedy action's value, i.e., \u003cstrong\u003eexploration\u003c/strong\u003e;\u003c/li\u003e\n        \u003cli\u003eExploitation maximizes the reward on 1 step, but it needs to be intercalated with exploration steps so as to maximize the greater total reward in the long term.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003ch3\u003eSection 2.2: Action-value Methods\u003c/h3\u003e\n    \u003cp\u003eDef. \u003cstrong\u003eAction-value Methods\u003c/strong\u003e: methods used to estimate the values of actions and to use those estimates to select an action to take at a given time step.\u003c/p\u003e\n    \u003cp\u003eLetting $\\mathbb{1}_{predicate}$ be the random variable which equals $1$ if the $predicate$ is true and $0$ otherwise, the value of an action can be estimated by averaging the rewards received:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    Q_t (a) \\doteq \\frac{\\text{sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$ taken prior to $t$}} = \\frac{\\sum_{i = 1}^{t - 1} R_i \\cdot \\mathbb{1}_{A_i = a}}{\\sum_{i = 1}^{t - 1} \\mathbb{1}_{A_i = a}}.\n    \\end{equation}\n    $$\n    \u003cp\u003eIf the denominator is zero (action has never been taken), then $Q_t(a)$ is defined as an arbitrary default value (e.g., zero). By the law of large numbers, as the denominator goes to infinity, $Q_t(a)$ converges to $q^{*}(a)$. This is called the \u003cem\u003esample-average\u003c/em\u003e method for estimating action values.\u003c/p\u003e\n    \u003cp\u003eThe simplest action selection rule is to always select a greedy action and - if there is more than 1 action with the same highest value - to break ties in some arbitrary way (e.g., randomly). This action selection method can be written as:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    A_t = \\argmax_a Q_t (a).\n    \\end{equation}\n    $$\n    \u003cp\u003eThis selection method never performs exploration. A simple alternative that does so is to select the greedy action most of the time (probability $1 - \\epsilon$) and (with probability $\\epsilon$) to randomly select any possible action with equal probability. Methods that use this near-greedy action selection rule are dubbed $\\epsilon$-greedy methods.\u003c/p\u003e\n\n    \u003ch3\u003eSection 2.3: The 10-armed Test-bed\u003c/h3\u003e\n    \u003cp\u003e\u003cstrong\u003eNon-stationary\u003c/strong\u003e setting: problem setting where the true values of the actions (or the reward probabilities) change over time.\u003c/p\u003e\n    \u003cp\u003eGiven a set of 2000 randomly generated $k$-armed bandit problems (with $k = 10$), for each problem in the set, the action values $q^{*}(a), \\ a = \\{1, 2, \\dots, 10\\},$ were selected from a normal (Gaussian) distribution with $\\mu = 0, \\  \\sigma^2 = 1$. When a learning method is applied to this problem selects action $A_t$ at time step $t$, the actual reward ($R_t$) was drawn from a normal distribution with $\\mu = q^{*}(A_t), \\ \\sigma^2 = 1$.\u003c/p\u003e\n    \u003cp\u003eThe performance of the learning methods is measured as it improves with experience over 1000 time steps of the bandit problem, which makes up a single run. To obtain an accurate measure of the learning algorithms' behavior, 2000 runs are performed and the results for the bandit problems are averaged.\u003c/p\u003e\n    \u003cp\u003eA greedy action selection method is compared against 2 $\\epsilon$-greedy methods (with $\\epsilon = 0.01 \\lor \\epsilon = 0.1$). All methods begin with initial action-value estimates of zero and update these estimates using the sample-average technique.\u003c/p\u003e\n    \u003cp\u003eWhile the greedy method improved slightly faster than the other 2, it converged to a reward-per-step of 1, which is lower than the best value of around 1.54 achieved by the $\\epsilon$-greedy method (with $\\epsilon = 0.1$). The method with $\\epsilon = 0.1$ improved faster than the method with $\\epsilon = 0.01$, since it explored more earlier. However, the method with $\\epsilon = 0.01$ converges to a higher reward-per-step in the long run, since the method with $\\epsilon = 0.1$ never selects the optimal action more than 91% of the time. \u003cbr\u003e\n    It is possible to perform $\\epsilon$ annealing to try to get fast learning at the start combined with convergence to a higher reward average.\u003c/p\u003e\n    \u003cp\u003eIt takes more exploration to find the optimal actions in cases with noisy rewards (i.e., high reward variance), meaning that $\\epsilon$-greedy methods perform even better in those cases, when compared to the greedy method. Also, although the greedy method is theoretically optimal in the deterministic case (i.e., with $\\sigma^2 = 0$), this property does not hold in non-stationary bandit problems, making exploration a necessity even in deterministic settings.\u003c/p\u003e\n\n    \u003ch3\u003eSection 2.4: Incremental Implementation\u003c/h3\u003e\n    \u003cp\u003eFor a single action, let $R_i$ denote the reward received after the $i^{th}$ selection of \u003cem\u003ethis action\u003c/em\u003e and $Q_n$ the estimate of its action value after it has been selected $n - 1$ times, written as:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    Q_n \\doteq \\frac{R_1 + R_2 + \\dots + R_{n - 1}}{n - 1}.\n    \\end{equation}\n    $$\n    \u003cp\u003eInstead of maintaining a record of all the rewards and performing the computation for the estimated value whenever needed (resulting in the growth of both computational and memory requirements), we can devise incremental formulas to update the averages with a small and constant computation to process each new reward. Given $Q_n$ and the $n^{th}$ reward $R_n$, the new average of all $n$ rewards can be computed as:\u003c/p\u003e\n    $$\n    \\begin{align}\n        Q_{n + 1} \u0026= \\frac{1}{n} \\sum_{i = 1}^n R_i \\nonumber\\\\\n        \u0026= \\frac{1}{n}(R_n + \\sum_{i = 1}^{n - 1} R_i) \\nonumber\\\\\n        \u0026= \\frac{1}{n}(R_n + (n - 1) \\cdot \\frac{1}{n - 1} \\cdot \\sum_{i = 1}^{n - 1} R_i) \\nonumber\\\\\n        \u0026= \\frac{1}{n} (R_n + (n - 1) \\cdot Q_n) \\nonumber\\\\\n        \u0026= \\frac{1}{n} (R_n + n \\cdot Q_n - Q_n) \\nonumber\\\\\n        \u0026= Q_n + \\frac{1}{n} [R_n - Q_n], \\ n \u003e 1 \\\\\n        Q_2 \u0026= R_1, \\ Q_1 \\in \\mathbb{R}.\n    \\end{align}\n    $$\n    \u003cp\u003eThis implementation only needs memory for $Q_n$ and $n$, and only performs a small computation for each new reward. \n    The general form of the previous update rule is given by:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    NewEstimate \\leftarrow OldEstimate + StepSize [Target - OldEstimate],\n    \\end{equation}\n    $$\n    \u003cp\u003ewhere $[Target - OldEstimate]$ is an \u003cem\u003eerror\u003c/em\u003e in the estimate, which is reduced by taking a step towards the (possibly noisy) target value.\n    The step-size parameter is generally denoted by $\\alpha$ or $\\alpha_t (a)$.\u003c/p\u003e\n\n    \u003cpre\u003e\u003ccode\u003edef bandit_problem(int k, float epsilon, bandits):\n    Q_a = [0]*k;\n    N_a = [0]*k;\n\n    while(True):\n        if random_float(0, 1) \u003c= epsilon:\n            A = random_int(0, k)\n        else:\n            A = argmax(Q_a)\n        R = bandits[A].get_reward()\n        N_a[A]++;\n        Q_a[A] += (1/N_a[A]) * (R - Q_a[A]);\u003c/code\u003e\u003c/pre\u003e\n\n    \u003ch2 id=\"chapter-3-finite-markov-decision-processes\"\u003eChapter 3: Finite Markov Decision Processes\u003c/h2\u003e\n    \u003cp\u003eMarkov Decision Processes (MDPs) are a formalization of sequential decision making where actions influence not only the immediate reward, but also future rewards. As such, this is an associative problem that takes into account the need to trade-off immediate and delayed reward. While in bandit problems we estimated the value $q^{*}(a), \\ \\forall a \\in \\mathcal{A},$ in an MDP, we estimate the value $q^{*}(s, a), \\ \\forall a \\in \\mathcal{A}, \\forall s \\in \\mathcal{S},$ or the value $v^{*}(s), \\forall s \\in \\mathcal{S}$ given optimal action selections. Such state-dependent values are important to assign credit for long-term rewards to individual action selections.\u003c/p\u003e\n\n    \u003ch3\u003eSection 3.1: The Agent-Environment Interface\u003c/h3\u003e\n    \u003cp\u003eAn MDP involves a learner and decision maker (i.e., the \u003cem\u003eagent\u003c/em\u003e) that interacts with its surroundings (i.e., the \u003cem\u003eenvironment\u003c/em\u003e) by continually selecting actions and having the environment respond by presenting new situations (or states) to the agent and giving rise to rewards, which the agent seeks to maximize over time. This process is illustrated in Figure 1.\u003c/p\u003e\n\n    \u003cfigure align='center'\u003e\n        \u003cimg alt=\"The agent-environment interaction in a Markov decision process.\" src=\"../../images/rl_mdp.png\"\u003e\n        \u003cfigcaption\u003eFigure 1: The agent-environment interaction in a MDP.\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\n    \u003cp\u003eThe agent and environment interact with each other in a sequence of discrete time steps $t = 0, 1, \\dots, n.$ At each time step $t,$ the agent receives a representation of the environment's \u003cem\u003estate\u003c/em\u003e $S_t \\in \\mathcal{S},$ and on that basis selects an \u003cem\u003eaction\u003c/em\u003e $A_t \\in \\mathcal{A}(s).$ At the next time step $t + 1,$ the agent receives a reward $R_{t + 1} \\in \\mathcal{R} \\subset \\mathbb{R}$ and finds itself in another state $s_{t+1}.$  Then, the MDP and agent give rise to a sequence or \u003cem\u003etrajectory\u003c/em\u003e like this:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots\n    \\end{equation}\n    $$\n    \u003cp\u003eIn a \u003cem\u003efinite\u003c/em\u003e MDP, the random variables $R_t$ and $S_t$ have well defined discrete probability distributions that depend only on the previous state and action, i.e., for particular values of these random variables $s' \\in \\mathcal{A}, \\ r \\in \\mathcal{R},$ there is a probability of those values occurring at time step $t,$ given particular values of the previous state and action:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    p(s', r \\vert s, a) \\doteq P(S_t = s', R_t \\vert S_{t-1} = s, A_{t-1} = a), \\ \\forall s', s \\in \\mathcal{S}, \\forall r \\in \\mathcal{R}, \\forall a \\in \\mathcal{A}(s).\n    \\end{equation}\n    $$\n    \u003cp\u003eThe function $p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$, which completely characterizes the MDP environment's \u003cem\u003edynamics\u003c/em\u003e, is an ordinary deterministic function with four arguments. This function $p$ specifies a probability distribution for each choice of $s$ and $a$, i.e.,\u003c/p\u003e\n    $$\n    \\begin{equation}\n    \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\vert s, a) = 1, \\ \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}(s).\n    \\end{equation}\n    $$\n    \u003cp\u003e\u003cstrong\u003eMarkov property\u003c/strong\u003e: the state must include information about all aspects of the past agent-environment interaction that make a difference for the future. In practice, this means that the probability of each possible value for $S_t$ and $R_t$ depends only on the previous state $S_{t-1}$ and action $A_{t-1}$.\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003eWhile most methods in this book assume this property to be true, there are methods that don't rely on it. \u003ca href=\"#chapter-17-frontiers\"\u003eChapter 17\u003c/a\u003e considers how to efficiently learn a Markov state from non-Markov observations.\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eFrom the dynamics function $p,$ we can compute the \u003cem\u003estate-transition probabilities\u003c/em\u003e $p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1],$\u003c/p\u003e\n    $$\n    \\begin{equation}\n    p(s' \\vert s, a) \\doteq P(S_t = s' \\vert S_{t-1} = s, A_{t-1} = a) = \\sum_{r \\in \\mathcal{R}} p(s', r \\vert s, a).\n    \\end{equation}\n    $$\n    \u003cp\u003eWe can also compute the expected rewards for state-action pairs as a two-argument function $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    r(s, a) \\doteq \\mathbb{E} [R_t \\vert S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r \\vert s, a),\n    \\end{equation}\n    $$\n    \u003cp\u003eand the expected rewards for the state-action-new_state triples as a three-argument function $r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R},$\u003c/p\u003e\n    $$\n    \\begin{equation}\n    r(s, a, s') \\doteq \\mathbb{E} [R_t \\vert S_{t-1} = s, A_{t-1} = a, S_t = s'] = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s', r \\vert s, a)}{p(s' \\vert s, a)}.\n    \\end{equation}\n    $$\n\n    \u003ch2 id=\"chapter-4-dynamic-programming\"\u003eChapter 4: Dynamic Programming\u003c/h2\u003e\n    \u003cp\u003eThe key idea of DP (and RL in general) is the use of value functions to both structure and organize the search for good policies. DP algorithms are obtained by turning Bellman equations into assignments, i.e., into update rules for improving approximations of the desired value functions. Remember that we can easily obtain optimal policies if we find the optimal value functions, $v^* \\lor q^*$, which satisfy the Bellman optimality equations, $\\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}(s), s' \\in \\mathcal{S}^+$, such that:\u003c/p\u003e\n    $$\n    \\begin{align}\n    v^*(s) \u0026= \\max_a \\mathbb{E}[R_{t+1} + \\gamma \\cdot v^*(S_{t+1} \\vert S_t = s, A_t = a)] \\nonumber\\\\\n        \u0026= \\max_a \\sum_{s', r} p(s' r, \\vert s, a) [r + \\gamma \\cdot v^*(s')],\\\\\n        \u0026\\lor \\nonumber\\\\\n    q^*(s, a) \u0026= \\mathbb{E}[R_{t+1} + \\gamma \\max_{'a} q^*(S_{t+1}, a') \\vert S_t = s, S_t =a], \\nonumber\\\\\n        \u0026= \\sum_{s', r} p(s', r \\vert s, a) [r + \\gamma \\max_{a'} q^*(s', a')].\n    \\end{align}\n    $$\n\n    \u003ch3\u003eSection 4.1: Policy Evaluation (Prediction)\u003c/h3\u003e\n    \u003cp\u003e\u003cstrong\u003ePolicy evaluation:\u003c/strong\u003e computing the state-value function $v^{\\pi}$ for an arbitrary policy $\\pi$. This is also referred to as the \u003cem\u003eprediction problem\u003c/em\u003e. Recall from \u003ca href=\"#chapter-3-finite-markov-decision-processes\"\u003eChapter 3\u003c/a\u003e that, $\\forall s \\in \\mathcal{S}$,\u003c/p\u003e\n    $$\n    \\begin{align}\n    v^{\\pi}(s) \u0026\\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s], \\nonumber\\\\\n        \u0026= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\vert S_t = s], \\nonumber\\\\\n        \u0026= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v^{\\pi} (S_{t+1} \\vert S_t = s)], \\\\\n        \u0026= \\sum_a \\pi(a \\vert s) \\sum_{s', r} p(s', r \\vert s, a)[r + \\gamma v^{\\pi}(s')],\n    \\end{align}\n    $$\n    \u003cp\u003ewhere $\\pi(a\\vert s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$, and the expectations are subscripted by $\\pi$ to indicate that they are conditional on following policy $\\pi$. If $\\gamma \u003c 1$ or eventual termination is guaranteed from all states under the policy $\\pi$, then w.h.t. that $v^{\\pi}$ exists and is unique.\u003c/p\u003e\n\n    \u003cpre\u003e\u003ccode class=\"language-python\"\u003edef iterative_policy_evaluation(policy, mdp, theta, gamma):\n    assert theta \u003e 0;\n    \n    state_values[len(mdp.states)] = 0;\n    for x in range(len(mdp.states) - 1):\n        state_values[x] = random_value();\n        \n    gradient = 0;\n    while gradient \u003e= theta:\n        gradient = 0;\n        for s in mdp.states:\n            v = state_values[s.id];\n            state_values[s.id] = 0;\n            for tmp_s in mdp.state:\n                for a in mdp.actions:\n                    state_values[s.id] += policy[s.id][a.id] * mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);\n            \n            gradient = max(gradient, absolute_value(v - state_values[s.id]));\n\n    return state_values;\u003c/code\u003e\u003c/pre\u003e\n\n    \u003ch2 id=\"chapter-13-policy-gradient-methods\"\u003eChapter 13: Policy Gradient Methods\u003c/h2\u003e\n    \u003cp\u003e\u003cstrong\u003ePolicy gradient\u003c/strong\u003e methods seek to learn an approximation to the policy by maximizing performance. Their updates approximate gradient ascent such as:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    \\theta_{t+1} = \\theta_t + \\alpha \\cdot \\hat{\\nabla J(\\theta_t)}, \\ \\hat{\\nabla J(\\theta_t)} \\in \\mathbb{R}^{d'}.\n    \\end{equation}\n    $$\n    \u003cp\u003eMethods that learn an approximation of both policy and value functions are called \u003cstrong\u003eactor-critic\u003c/strong\u003e methods. The \u003cem\u003eactor\u003c/em\u003e is a reference to the learned policy and \u003cem\u003ecritic\u003c/em\u003e a reference to the learned (state-)value function.\u003c/p\u003e\n\n    \u003ch3\u003eSection 13.1: Policy Approximation and its Advantages\u003c/h3\u003e\n    \u003cp\u003eThe policy can be parameterized in any way, as long as 2 conditions are met\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003eAs long as $\\pi(a\\vert s, \\theta)$ is differentiable w.r.t. its parameters:\u003c/li\u003e\n    \u003c/ul\u003e\n    $$\n    \\forall s \\in S \\ \\forall a \\in A(s) \\ \\exists \\ \\nabla \\pi (a|s, \\theta) : |\\nabla \\pi (a|s, \\theta)| \u003c \\infty \\ \\wedge \\theta \\in \\mathbb{R}^{d'};\n    $$\n    \u003cul\u003e\n        \u003cli\u003eAs long as it continues to perform exploration (to avoid a deterministic policy):\u003c/li\u003e\n    \u003c/ul\u003e\n    $$\n    \\pi (a|s, \\theta) \\ \\in \\  ]0, 1[.\n    $$\n    \u003cp\u003eFor small-to-medium discrete action spaces, it is common to form parameterized numerical preferences $h(s, a, \\theta) \\in \\mathbb{R}$ for each $(s, a)$ pair. The probabilities of each action being selected can be calculated with, e.g., an exponential softmax distribution:\u003c/p\u003e\n    $$\n    \\begin{equation}\n    \\pi (a|s, \\theta) \\doteq \\frac{\\exp(h(s,a,\\theta))}{\\sum_b \\exp(h(s,b,\\theta))}.\n    \\end{equation}\n    $$\n    \u003cp\u003eThis kind of policy parameterization is called \u003cem\u003esoftmax in action preferences\u003c/em\u003e.\u003c/p\u003e\n\n    \u003c!-- Content continues similarly for other chapters --\u003e\n\n\u003c/main\u003e\n\n"])</script><script>self.__next_f.push([1,"a:[\"slug\",\"2024-10-31-Notes-on-RL-an-Introduction\",\"d\"]\n10:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L4\",null,{\"buildId\":\"9cLm1DBckUGzspWJmek74\",\"assetPrefix\":\"/github-pages\",\"urlParts\":[\"\",\"content\",\"posts\",\"2024-10-31-Notes-on-RL-an-Introduction\"],\"initialTree\":[\"\",{\"children\":[\"content\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"2024-10-31-Notes-on-RL-an-Introduction\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"2024-10-31-Notes-on-RL-an-Introduction\\\"}\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"content\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"2024-10-31-Notes-on-RL-an-Introduction\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L5\",[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto py-12 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-6\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/content/posts\",\"className\":\"flex items-center text-slate-600 dark:text-slate-400 hover:text-blue-500 transition-colors\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-left w-4 h-4 mr-2\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],\"Back to Posts\"]}]}],[\"$\",\"$L7\",null,{\"content\":\"$8\",\"title\":\"2024-10-31-Notes-on-RL-an-Introduction\"}]]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/github-pages/_next/static/css/5283f8f8abdc6c70.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[null,[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"content\",\"children\",\"posts\",\"children\",\"$a\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"content\",\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"content\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/github-pages/_next/static/css/c20b283d6d558cbe.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_b0711f bg-slate-50 dark:bg-slate-950 h-full\",\"children\":[[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"ACFHarbinger\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Personal Website\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>