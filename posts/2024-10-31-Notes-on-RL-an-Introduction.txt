2:I[4707,[],""]
4:I[6423,[],""]
5:I[2658,["185","static/chunks/app/layout-f469fd698b4549d4.js"],"default"]
3:["slug","2024-10-31-Notes-on-RL-an-Introduction","d"]
0:["o8pt4vi2PpPKcfi-L34Nz",[[["",{"children":["posts",{"children":[["slug","2024-10-31-Notes-on-RL-an-Introduction","d"],{"children":["__PAGE__?{\"slug\":\"2024-10-31-Notes-on-RL-an-Introduction\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["posts",{"children":[["slug","2024-10-31-Notes-on-RL-an-Introduction","d"],{"children":["__PAGE__",{},[["$L1",["$","div",null,{"className":"space-y-8 animate-in fade-in duration-500","children":[["$","div",null,{"className":"space-y-4 border-b border-slate-200 dark:border-slate-800 pb-8","children":[["$","div",null,{"className":"flex flex-wrap gap-2 mb-4","children":[["$","span",null,{"className":"inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent ","children":"ML"}],["$","span",null,{"className":"inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent ","children":"RL"}],["$","span",null,{"className":"inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent ","children":"DL"}]]}],["$","h1",null,{"className":"text-4xl font-display font-bold text-slate-900 dark:text-white leading-tight","children":"Notes on Reinforcement Learning: An Introduction (2nd edition)"}],["$","div",null,{"className":"flex items-center gap-4 text-slate-500 text-sm","children":["$","span",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":16,"height":16,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar","children":[["$","path","1cmpym",{"d":"M8 2v4"}],["$","path","4m81vk",{"d":"M16 2v4"}],["$","rect","1hopcy",{"width":"18","height":"18","x":"3","y":"4","rx":"2"}],["$","path","8toen8",{"d":"M3 10h18"}],"$undefined"]}]," Oct 31, 2024"]}]}]]}],["$","div",null,{"className":"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-8 prose dark:prose-invert max-w-none text-slate-700 dark:text-slate-300","children":[["$","p",null,{"className":"mb-4","children":["Here are some notes I took when reading the second edition of the ",["$","a",null,{"href":"/assets/docs/literature/books/RLbook2020.pdf","className":"text-blue-600 dark:text-blue-400 hover:underline","children":"Reinforcement Learning: An Introduction"}]," book."]}],["$","p",null,{"className":"mb-4","children":"If you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!"}],["$","p",null,{"children":"It does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there."}]]}],["$","section",null,{"children":[["$","div",null,{"className":"flex items-center space-x-3 mb-4 ","children":[["$","div",null,{"className":"p-2 rounded-lg bg-slate-100 dark:bg-slate-800 shadow-md flex items-center justify-center","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-book-open text-green-500","children":[["$","path","vv98re",{"d":"M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z"}],["$","path","1cyq3y",{"d":"M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z"}],"$undefined"]}]}],["$","h2",null,{"className":"text-2xl font-bold font-display text-slate-900 dark:text-white","children":"Chapter 1: Introduction"}]]}],["$","div",null,{"className":"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6 text-slate-700 dark:text-slate-300","children":["$","p",null,{"children":"Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them."}]}]]}],["$","section",null,{"children":[["$","h2",null,{"className":"text-2xl font-bold font-display text-slate-900 dark:text-white mb-4 mt-8","children":"Part I: Tabular Solution Methods"}],["$","div",null,{"className":"space-y-6","children":[["$","div",null,{"className":"pl-4 border-l-2 border-blue-500","children":[["$","h3",null,{"className":"text-xl font-bold text-slate-800 dark:text-slate-200 mb-2","children":"Chapter 2: Multi-armed Bandits"}],["$","div",null,{"className":"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6","children":[["$","h4",null,{"className":"font-bold text-slate-900 dark:text-white mb-2","children":"Section 2.1: A k-armed Bandit Problem"}],["$","p",null,{"className":"text-sm text-slate-600 dark:text-slate-400","children":"Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps."}]]}]]}],["$","div",null,{"className":"pl-4 border-l-2 border-purple-500","children":[["$","h3",null,{"className":"text-xl font-bold text-slate-800 dark:text-slate-200 mb-2","children":"Chapter 3: Finite Markov Decision Processes"}],["$","div",null,{"className":"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6","children":[["$","div",null,{"className":"mb-4","children":[["$","img",null,{"src":"/assets/images/literature/rl_mdp.png","alt":"The agent-environment interaction in a Markov decision process.","className":"rounded-lg shadow-md max-w-full md:max-w-md mx-auto"}],["$","p",null,{"className":"text-center text-xs text-slate-500 mt-2","children":"The agent-environment interaction in a Markov decision process."}]]}],["$","p",null,{"className":"text-slate-700 dark:text-slate-300 mb-4","children":"MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward."}],["$","h4",null,{"className":"font-bold text-slate-900 dark:text-white mb-2","children":"The Agent-Environment Interface"}],["$","div",null,{"className":"bg-slate-100 dark:bg-slate-800 p-3 rounded-lg font-mono text-sm overflow-x-auto mb-4","children":["p(s', r | s, a) \\doteq \\text\\","{","Pr\\","}","\\","{","S_t = s', R_t = r | S_\\","{","t-1\\","}"," = s, A_\\","{","t-1\\","}"," = a\\","}"]}],["$","p",null,{"className":"text-sm text-slate-600 dark:text-slate-400","children":["The function ",["$","code",null,{"className":"text-pink-500","children":"p"}]," defines the dynamics of the MDP."]}]]}]]}]]}]]}],["$","section",null,{"children":[["$","h2",null,{"className":"text-2xl font-bold font-display text-slate-900 dark:text-white mb-4 mt-8","children":"Part II: Approximate Solution Methods"}],["$","div",null,{"className":"pl-4 border-l-2 border-orange-500","children":[["$","h3",null,{"className":"text-xl font-bold text-slate-800 dark:text-slate-200 mb-2","children":"Chapter 13: Policy Gradient Methods"}],["$","div",null,{"className":"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6 text-slate-700 dark:text-slate-300","children":[["$","p",null,{"className":"mb-4","children":"Methods that learn a parameterized policy that can select actions without consulting a value function. A value function may still be used to learn the policy parameter, but is not required for action selection."}],["$","div",null,{"className":"bg-slate-100 dark:bg-slate-800 p-3 rounded-lg font-mono text-sm overflow-x-auto mb-4","children":["\\pi(a|s, \\theta) = \\text\\","{","Pr\\","}","\\","{","A_t = a | S_t = s, \\theta_t = \\theta\\","}"]}],["$","p",null,{"children":["A state-value function baseline reduces the variance of the REINFORCE method without introducing bias. If the state-value function is (also) used to assess the policy's action selections, then it is called a ",["$","em",null,{"children":"critic"}],", the policy is called an ",["$","em",null,{"children":"actor"}],", and the overall algorithm is called an ",["$","em",null,{"children":"actor-critic method"}],"."]}]]}]]}]]}]]}],null],null],null]},[null,["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$3","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/github-pages/_next/static/css/58f6eb1add37a79e.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_b0711f __variable_c0ebee","children":[["$","head",null,{"children":["$","style",null,{"children":"\n          :root {\n            --font-sans: var(--font-inter);\n            --font-display: var(--font-lexend);\n          }\n        "}]}],["$","body",null,{"children":["$","$L5",null,{"children":["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}]}]]}]],null],null],["$L6",null]]]]
6:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","meta","2",{"name":"next-size-adjust"}]]
1:null
