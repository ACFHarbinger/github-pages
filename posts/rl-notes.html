<!DOCTYPE html><html lang="en" class="__variable_b0711f __variable_c0ebee"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/github-pages/_next/static/media/793968fa3513f5d6-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/github-pages/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/github-pages/_next/static/media/23041868.55bda878.jpeg"/><link rel="preload" as="image" href="/assets/images/literature/rl_mdp.png"/><link rel="stylesheet" href="/github-pages/_next/static/css/fc975114e5dc40a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/github-pages/_next/static/chunks/webpack-014c7bd7f964e188.js"/><script src="/github-pages/_next/static/chunks/fd9d1056-cce117dc4e21e608.js" async=""></script><script src="/github-pages/_next/static/chunks/117-711bfa2d5e5aca5e.js" async=""></script><script src="/github-pages/_next/static/chunks/main-app-aaa6492bf6b572b3.js" async=""></script><script src="/github-pages/_next/static/chunks/app/layout-e5912bd90c51f1ee.js" async=""></script><meta name="next-size-adjust"/><style>
          :root {
            --font-sans: var(--font-inter);
            --font-display: var(--font-lexend);
          }
        </style><script src="/github-pages/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="min-h-screen transition-colors duration-300 font-sans bg-slate-50 text-slate-800"><div class="lg:hidden flex items-center justify-between p-4 bg-white/90 dark:bg-slate-900/90 backdrop-blur-sm sticky top-0 z-50 border-b border-slate-200 dark:border-slate-800"><span class="font-bold text-xl tracking-tight font-display">ACF Peacekeeper</span><button class="p-2 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-800 transition-colors" aria-label="Open Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button></div><div class="flex max-w-7xl mx-auto"><aside class="hidden lg:flex flex-col w-72 h-screen sticky top-0 border-r border-slate-200 dark:border-slate-800 bg-white/50 dark:bg-slate-900/50 backdrop-blur-xl p-8 justify-between transition-all duration-300 ease-in-out relative"><button class="absolute top-4 -right-3 z-10 p-1 rounded-full bg-slate-100 dark:bg-slate-800 border border-slate-200 dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700 transition-colors hidden lg:block" aria-label="Collapse Sidebar"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left transition-transform duration-300 "><path d="m15 18-6-6 6-6"></path></svg></button><div><div class="mb-12"><div class="relative rounded-full overflow-hidden shadow-lg ring-4 ring-white dark:ring-slate-800 transition-all duration-300 mb-4 bg-slate-200 dark:bg-slate-700 h-20 w-20"><img src="/github-pages/_next/static/media/23041868.55bda878.jpeg" alt="ACF Peacekeeper" class="h-full w-full object-cover"/></div><h1 class="text-2xl font-bold text-slate-900 dark:text-white font-display">ACF Peacekeeper</h1><p class="text-sm text-slate-500 dark:text-slate-400 mt-1 font-medium dark:text-white">Researcher &amp; Developer</p><p class="text-xs text-slate-400 mt-2 uppercase tracking-wider dark:text-white">RL &amp; Combinatorial Opt</p></div><nav class="space-y-2"><a href="/github-pages" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-home"><path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path><polyline points="9 22 9 12 15 12 15 22"></polyline></svg><span>Home</span></a><a href="/github-pages/content/about" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-user"><path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg><span>About</span></a><a href="/github-pages/content/projects" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-code"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg><span>Projects</span></a><a href="/github-pages/content/tools" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-wrench"><path d="M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z"></path></svg><span>Tools</span></a><a href="/github-pages/posts" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        bg-blue-50 text-blue-600 dark:bg-slate-800 dark:text-blue-400"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-pen-tool"><path d="M15.707 21.293a1 1 0 0 1-1.414 0l-1.586-1.586a1 1 0 0 1 0-1.414l5.586-5.586a1 1 0 0 1 1.414 0l1.586 1.586a1 1 0 0 1 0 1.414z"></path><path d="m18 13-1.375-6.874a1 1 0 0 0-.746-.776L3.235 2.028a1 1 0 0 0-1.207 1.207L5.35 15.879a1 1 0 0 0 .776.746L13 18"></path><path d="m2.3 2.3 7.286 7.286"></path><circle cx="11" cy="11" r="2"></circle></svg><span>Notes</span></a><a href="/github-pages/content/media" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-video"><path d="m16 13 5.223 3.482a.5.5 0 0 0 .777-.416V7.87a.5.5 0 0 0-.752-.432L16 10.5"></path><rect x="2" y="6" width="14" height="12" rx="2"></rect></svg><span>Media</span></a></nav></div><div class="flex justify-center space-x-4 text-slate-400"><a href="https://github.com/acfpeacekeeper" target="_blank" rel="noopener noreferrer" class="hover:text-slate-900 dark:hover:text-white transition-colors flex items-center justify-center p-1" aria-label="Github"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="#" class="hover:text-blue-400 transition-colors flex items-center justify-center p-1" aria-label="Twitter"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a><a href="#" class="hover:text-blue-600 transition-colors flex items-center justify-center p-1" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><button class="hover:text-yellow-500 transition-colors flex items-center justify-center p-1" aria-label="Toggle Dark Mode"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></button></div></aside><main class="flex-1 p-6 lg:p-12 w-full max-w-4xl mx-auto"><div class="space-y-8 animate-in fade-in duration-500"><div class="space-y-4 border-b border-slate-200 dark:border-slate-800 pb-8"><div class="flex flex-wrap gap-2 mb-4"><span class="inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent ">ML</span><span class="inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent ">RL</span><span class="inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent ">DL</span></div><h1 class="text-4xl font-display font-bold text-slate-900 dark:text-white leading-tight">Notes on Reinforcement Learning: An Introduction (2nd edition)</h1><div class="flex items-center gap-4 text-slate-500 text-sm"><span class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path></svg> Oct 31, 2024</span></div></div><div class="backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-8 prose dark:prose-invert max-w-none text-slate-700 dark:text-slate-300"><p class="mb-4">Here are some notes I took when reading the second edition of the <a href="/assets/docs/literature/books/RLbook2020.pdf" class="text-blue-600 dark:text-blue-400 hover:underline">Reinforcement Learning: An Introduction</a> book.</p><p class="mb-4">If you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!</p><p>It does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there.</p></div><section><div class="flex items-center space-x-3 mb-4 "><div class="p-2 rounded-lg bg-slate-100 dark:bg-slate-800 shadow-md flex items-center justify-center"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-book-open text-green-500"><path d="M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z"></path><path d="M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z"></path></svg></div><h2 class="text-2xl font-bold font-display text-slate-900 dark:text-white">Chapter 1: Introduction</h2></div><div class="backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6 text-slate-700 dark:text-slate-300"><p>Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</p></div></section><section><h2 class="text-2xl font-bold font-display text-slate-900 dark:text-white mb-4 mt-8">Part I: Tabular Solution Methods</h2><div class="space-y-6"><div class="pl-4 border-l-2 border-blue-500"><h3 class="text-xl font-bold text-slate-800 dark:text-slate-200 mb-2">Chapter 2: Multi-armed Bandits</h3><div class="backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6"><h4 class="font-bold text-slate-900 dark:text-white mb-2">Section 2.1: A k-armed Bandit Problem</h4><p class="text-sm text-slate-600 dark:text-slate-400">Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p></div></div><div class="pl-4 border-l-2 border-purple-500"><h3 class="text-xl font-bold text-slate-800 dark:text-slate-200 mb-2">Chapter 3: Finite Markov Decision Processes</h3><div class="backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6"><div class="mb-4"><img src="/assets/images/literature/rl_mdp.png" alt="The agent-environment interaction in a Markov decision process." class="rounded-lg shadow-md max-w-full md:max-w-md mx-auto"/><p class="text-center text-xs text-slate-500 mt-2">The agent-environment interaction in a Markov decision process.</p></div><p class="text-slate-700 dark:text-slate-300 mb-4">MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.</p><h4 class="font-bold text-slate-900 dark:text-white mb-2">The Agent-Environment Interface</h4><div class="bg-slate-100 dark:bg-slate-800 p-3 rounded-lg font-mono text-sm overflow-x-auto mb-4">p(s&#x27;, r | s, a) \doteq \text\<!-- -->{<!-- -->Pr\<!-- -->}<!-- -->\<!-- -->{<!-- -->S_t = s&#x27;, R_t = r | S_\<!-- -->{<!-- -->t-1\<!-- -->}<!-- --> = s, A_\<!-- -->{<!-- -->t-1\<!-- -->}<!-- --> = a\<!-- -->}</div><p class="text-sm text-slate-600 dark:text-slate-400">The function <code class="text-pink-500">p</code> defines the dynamics of the MDP.</p></div></div></div></section><section><h2 class="text-2xl font-bold font-display text-slate-900 dark:text-white mb-4 mt-8">Part II: Approximate Solution Methods</h2><div class="pl-4 border-l-2 border-orange-500"><h3 class="text-xl font-bold text-slate-800 dark:text-slate-200 mb-2">Chapter 13: Policy Gradient Methods</h3><div class="backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6 text-slate-700 dark:text-slate-300"><p class="mb-4">Methods that learn a parameterized policy that can select actions without consulting a value function. A value function may still be used to learn the policy parameter, but is not required for action selection.</p><div class="bg-slate-100 dark:bg-slate-800 p-3 rounded-lg font-mono text-sm overflow-x-auto mb-4">\pi(a|s, \theta) = \text\<!-- -->{<!-- -->Pr\<!-- -->}<!-- -->\<!-- -->{<!-- -->A_t = a | S_t = s, \theta_t = \theta\<!-- -->}</div><p>A state-value function baseline reduces the variance of the REINFORCE method without introducing bias. If the state-value function is (also) used to assess the policy&#x27;s action selections, then it is called a <em>critic</em>, the policy is called an <em>actor</em>, and the overall algorithm is called an <em>actor-critic method</em>.</p></div></div></section></div><footer class="border-t border-slate-200 dark:border-slate-800 pt-8 mt-20 pb-8 text-center md:text-left text-slate-500 text-sm"><div class="flex flex-col md:flex-row justify-between items-center"><p>Â© <!-- -->2025<!-- --> ACF Peacekeeper. All rights reserved.</p><div class="mt-4 md:mt-0 space-x-6 flex"><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">RSS</a><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">Privacy</a><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">Sitemap</a></div></div></footer></main></div></div><script src="/github-pages/_next/static/chunks/webpack-014c7bd7f964e188.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/github-pages/_next/static/media/793968fa3513f5d6-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/github-pages/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/github-pages/_next/static/css/fc975114e5dc40a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[2846,[],\"\"]\n6:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\n9:I[2658,[\"185\",\"static/chunks/app/layout-e5912bd90c51f1ee.js\"],\"default\"]\nb:I[1060,[],\"\"]\n7:[\"slug\",\"rl-notes\",\"d\"]\nc:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L4\",null,{\"buildId\":\"701F3W98c36XAugb94sra\",\"assetPrefix\":\"/github-pages\",\"urlParts\":[\"\",\"posts\",\"rl-notes\"],\"initialTree\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"rl-notes\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"rl-notes\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"rl-notes\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L5\",[\"$\",\"div\",null,{\"className\":\"space-y-8 animate-in fade-in duration-500\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-y-4 border-b border-slate-200 dark:border-slate-800 pb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent \",\"children\":\"ML\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent \",\"children\":\"RL\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex items-center rounded-full px-3 py-1 text-xs font-medium transition-colors border border-slate-300 dark:border-slate-700 text-slate-700 dark:text-slate-300 bg-transparent \",\"children\":\"DL\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl font-display font-bold text-slate-900 dark:text-white leading-tight\",\"children\":\"Notes on Reinforcement Learning: An Introduction (2nd edition)\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 text-slate-500 text-sm\",\"children\":[\"$\",\"span\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":16,\"height\":16,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-calendar\",\"children\":[[\"$\",\"path\",\"1cmpym\",{\"d\":\"M8 2v4\"}],[\"$\",\"path\",\"4m81vk\",{\"d\":\"M16 2v4\"}],[\"$\",\"rect\",\"1hopcy\",{\"width\":\"18\",\"height\":\"18\",\"x\":\"3\",\"y\":\"4\",\"rx\":\"2\"}],[\"$\",\"path\",\"8toen8\",{\"d\":\"M3 10h18\"}],\"$undefined\"]}],\" Oct 31, 2024\"]}]}]]}],[\"$\",\"div\",null,{\"className\":\"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-8 prose dark:prose-invert max-w-none text-slate-700 dark:text-slate-300\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":[\"Here are some notes I took when reading the second edition of the \",[\"$\",\"a\",null,{\"href\":\"/assets/docs/literature/books/RLbook2020.pdf\",\"className\":\"text-blue-600 dark:text-blue-400 hover:underline\",\"children\":\"Reinforcement Learning: An Introduction\"}],\" book.\"]}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"If you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!\"}],[\"$\",\"p\",null,{\"children\":\"It does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there.\"}]]}],[\"$\",\"section\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-3 mb-4 \",\"children\":[[\"$\",\"div\",null,{\"className\":\"p-2 rounded-lg bg-slate-100 dark:bg-slate-800 shadow-md flex items-center justify-center\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-book-open text-green-500\",\"children\":[[\"$\",\"path\",\"vv98re\",{\"d\":\"M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z\"}],[\"$\",\"path\",\"1cyq3y\",{\"d\":\"M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z\"}],\"$undefined\"]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold font-display text-slate-900 dark:text-white\",\"children\":\"Chapter 1: Introduction\"}]]}],[\"$\",\"div\",null,{\"className\":\"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6 text-slate-700 dark:text-slate-300\",\"children\":[\"$\",\"p\",null,{\"children\":\"Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.\"}]}]]}],[\"$\",\"section\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold font-display text-slate-900 dark:text-white mb-4 mt-8\",\"children\":\"Part I: Tabular Solution Methods\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"pl-4 border-l-2 border-blue-500\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-slate-800 dark:text-slate-200 mb-2\",\"children\":\"Chapter 2: Multi-armed Bandits\"}],[\"$\",\"div\",null,{\"className\":\"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"font-bold text-slate-900 dark:text-white mb-2\",\"children\":\"Section 2.1: A k-armed Bandit Problem\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-slate-600 dark:text-slate-400\",\"children\":\"Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"pl-4 border-l-2 border-purple-500\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-slate-800 dark:text-slate-200 mb-2\",\"children\":\"Chapter 3: Finite Markov Decision Processes\"}],[\"$\",\"div\",null,{\"className\":\"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-4\",\"children\":[[\"$\",\"img\",null,{\"src\":\"/assets/images/literature/rl_mdp.png\",\"alt\":\"The agent-environment interaction in a Markov decision process.\",\"className\":\"rounded-lg shadow-md max-w-full md:max-w-md mx-auto\"}],[\"$\",\"p\",null,{\"className\":\"text-center text-xs text-slate-500 mt-2\",\"children\":\"The agent-environment interaction in a Markov decision process.\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-slate-700 dark:text-slate-300 mb-4\",\"children\":\"MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.\"}],[\"$\",\"h4\",null,{\"className\":\"font-bold text-slate-900 dark:text-white mb-2\",\"children\":\"The Agent-Environment Interface\"}],[\"$\",\"div\",null,{\"className\":\"bg-slate-100 dark:bg-slate-800 p-3 rounded-lg font-mono text-sm overflow-x-auto mb-4\",\"children\":[\"p(s', r | s, a) \\\\doteq \\\\text\\\\\",\"{\",\"Pr\\\\\",\"}\",\"\\\\\",\"{\",\"S_t = s', R_t = r | S_\\\\\",\"{\",\"t-1\\\\\",\"}\",\" = s, A_\\\\\",\"{\",\"t-1\\\\\",\"}\",\" = a\\\\\",\"}\"]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-slate-600 dark:text-slate-400\",\"children\":[\"The function \",[\"$\",\"code\",null,{\"className\":\"text-pink-500\",\"children\":\"p\"}],\" defines the dynamics of the MDP.\"]}]]}]]}]]}]]}],[\"$\",\"section\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold font-display text-slate-900 dark:text-white mb-4 mt-8\",\"children\":\"Part II: Approximate Solution Methods\"}],[\"$\",\"div\",null,{\"className\":\"pl-4 border-l-2 border-orange-500\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-slate-800 dark:text-slate-200 mb-2\",\"children\":\"Chapter 13: Policy Gradient Methods\"}],[\"$\",\"div\",null,{\"className\":\"backdrop-blur-md bg-white/70 dark:bg-slate-800/70 border border-white/50 dark:border-white/10 shadow-sm rounded-xl transition-all duration-300 p-6 text-slate-700 dark:text-slate-300\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"Methods that learn a parameterized policy that can select actions without consulting a value function. A value function may still be used to learn the policy parameter, but is not required for action selection.\"}],[\"$\",\"div\",null,{\"className\":\"bg-slate-100 dark:bg-slate-800 p-3 rounded-lg font-mono text-sm overflow-x-auto mb-4\",\"children\":[\"\\\\pi(a|s, \\\\theta) = \\\\text\\\\\",\"{\",\"Pr\\\\\",\"}\",\"\\\\\",\"{\",\"A_t = a | S_t = s, \\\\theta_t = \\\\theta\\\\\",\"}\"]}],[\"$\",\"p\",null,{\"children\":[\"A state-value function baseline reduces the variance of the REINFORCE method without introducing bias. If the state-value function is (also) used to assess the policy's action selections, then it is called a \",[\"$\",\"em\",null,{\"children\":\"critic\"}],\", the policy is called an \",[\"$\",\"em\",null,{\"children\":\"actor\"}],\", and the overall algorithm is called an \",[\"$\",\"em\",null,{\"children\":\"actor-critic method\"}],\".\"]}]]}]]}]]}]]}],null],null],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/github-pages/_next/static/css/fc975114e5dc40a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_b0711f __variable_c0ebee\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"style\",null,{\"children\":\"\\n          :root {\\n            --font-sans: var(--font-inter);\\n            --font-display: var(--font-lexend);\\n          }\\n        \"}]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$L9\",null,{\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$La\"],\"globalErrorComponent\":\"$b\",\"missingSlots\":\"$Wc\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"2\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>