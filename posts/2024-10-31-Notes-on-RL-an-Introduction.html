<!DOCTYPE html><html lang="en" class="__variable_b0711f __variable_c0ebee"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/github-pages/_next/static/media/793968fa3513f5d6-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/github-pages/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/github-pages/_next/static/media/23041868.55bda878.jpeg"/><link rel="stylesheet" href="/github-pages/_next/static/css/fc975114e5dc40a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/github-pages/_next/static/chunks/webpack-014c7bd7f964e188.js"/><script src="/github-pages/_next/static/chunks/fd9d1056-cce117dc4e21e608.js" async=""></script><script src="/github-pages/_next/static/chunks/117-711bfa2d5e5aca5e.js" async=""></script><script src="/github-pages/_next/static/chunks/main-app-aaa6492bf6b572b3.js" async=""></script><script src="/github-pages/_next/static/chunks/app/layout-cdee4d1479ae2af3.js" async=""></script><meta name="next-size-adjust"/><style>
          :root {
            --font-sans: var(--font-inter);
            --font-display: var(--font-lexend);
          }
        </style><script src="/github-pages/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="min-h-screen transition-colors duration-300 font-sans bg-slate-50 text-slate-800"><div class="lg:hidden flex items-center justify-between p-4 bg-white/90 dark:bg-slate-900/90 backdrop-blur-sm sticky top-0 z-50 border-b border-slate-200 dark:border-slate-800"><span class="font-bold text-xl tracking-tight font-display">ACF Peacekeeper</span><button class="p-2 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-800 transition-colors" aria-label="Open Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button></div><div class="flex max-w-7xl mx-auto"><aside class="hidden lg:flex flex-col w-72 h-screen sticky top-0 border-r border-slate-200 dark:border-slate-800 bg-white/50 dark:bg-slate-900/50 backdrop-blur-xl p-8 justify-between transition-all duration-300 ease-in-out relative"><button class="absolute top-4 -right-3 z-10 p-1 rounded-full bg-slate-100 dark:bg-slate-800 border border-slate-200 dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700 transition-colors hidden lg:block" aria-label="Collapse Sidebar"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left transition-transform duration-300 "><path d="m15 18-6-6 6-6"></path></svg></button><div><div class="mb-12"><div class="relative rounded-full overflow-hidden shadow-lg ring-4 ring-white dark:ring-slate-800 transition-all duration-300 mb-4 bg-slate-200 dark:bg-slate-700 h-20 w-20"><img src="/github-pages/_next/static/media/23041868.55bda878.jpeg" alt="ACF Peacekeeper" class="h-full w-full object-cover"/></div><h1 class="text-2xl font-bold text-slate-900 dark:text-white font-display">ACF Peacekeeper</h1><p class="text-sm text-slate-500 dark:text-slate-400 mt-1 font-medium dark:text-white">Scientist &amp; Engineer</p><p class="text-xs text-slate-400 mt-2 uppercase tracking-wider dark:text-white">CSE &amp; Math</p></div><nav class="space-y-2"><a href="/github-pages" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-home"><path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path><polyline points="9 22 9 12 15 12 15 22"></polyline></svg><span>Home</span></a><a href="/github-pages/content/about" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-user"><path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg><span>About</span></a><a href="/github-pages/content/projects" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-code"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg><span>Projects</span></a><a href="/github-pages/content/tools" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-wrench"><path d="M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z"></path></svg><span>Tools</span></a><a href="/github-pages/posts" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        bg-blue-50 text-blue-600 dark:bg-slate-800 dark:text-blue-400"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-pen-tool"><path d="M15.707 21.293a1 1 0 0 1-1.414 0l-1.586-1.586a1 1 0 0 1 0-1.414l5.586-5.586a1 1 0 0 1 1.414 0l1.586 1.586a1 1 0 0 1 0 1.414z"></path><path d="m18 13-1.375-6.874a1 1 0 0 0-.746-.776L3.235 2.028a1 1 0 0 0-1.207 1.207L5.35 15.879a1 1 0 0 0 .776.746L13 18"></path><path d="m2.3 2.3 7.286 7.286"></path><circle cx="11" cy="11" r="2"></circle></svg><span>Notes</span></a><a href="/github-pages/content/media" class="flex items-center py-3 rounded-lg font-medium transition-all duration-200 
        space-x-3 px-4 
        text-slate-600 hover:bg-slate-50 dark:text-slate-400 dark:hover:bg-slate-800"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-video"><path d="m16 13 5.223 3.482a.5.5 0 0 0 .777-.416V7.87a.5.5 0 0 0-.752-.432L16 10.5"></path><rect x="2" y="6" width="14" height="12" rx="2"></rect></svg><span>Media</span></a></nav></div><div class="flex justify-center space-x-4 text-slate-400"><a href="https://github.com/acfpeacekeeper" target="_blank" rel="noopener noreferrer" class="hover:text-slate-900 dark:hover:text-white transition-colors flex items-center justify-center p-1" aria-label="Github"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="#" class="hover:text-blue-400 transition-colors flex items-center justify-center p-1" aria-label="Twitter"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a><a href="#" class="hover:text-blue-600 transition-colors flex items-center justify-center p-1" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><button class="hover:text-yellow-500 transition-colors flex items-center justify-center p-1" aria-label="Toggle Dark Mode"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></button></div></aside><main class="flex-1 p-6 lg:p-12 w-full max-w-4xl mx-auto"><div class="max-w-4xl mx-auto py-12 px-4"><h1 class="text-4xl font-bold font-display text-slate-900 dark:text-white mb-4">Notes on Reinforcement Learning: An Introduction (2nd edition)</h1><div class="text-sm text-slate-500 mb-8">Published on <!-- -->2024-10-31<!-- --> | Category: <!-- -->Uncategorized</div><div class="prose dark:prose-invert max-w-none"><p>$$
\DeclareMathOperator*{\argmin}{arg,min}
\DeclareMathOperator*{\argmax}{arg,max}
$$</p>
<p>Here are some notes I took when reading the second edition of the <a href="http://acfpeacekeeper.github.io/github-pages/assets/docs/literature/books/RLbook2020.pdf" onerror="this.href='http://localhost:4000/assets/docs/literature/books/RLbook2020.pdf'">Reinforcement Learning: An Introduction</a> book.\
If you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!\
It does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there.</p>
<h1>Chapter Index</h1>
<ol>
<li><a href="#chapter-1-introduction">Chapter 1: Introduction</a></li>
<li><a href="#part-i-tabular-solution-methods">Part I: Tabular Solution Methods</a>
<ol>
<li><a href="#chapter-2-multi-armed-bandits">Chapter 2: Multi-armed Bandits</a>
<ol>
<li><a href="#section-21-a--armed-bandit-problem">Section 2.1: A k-armed Bandit Problem</a></li>
<li><a href="#section-22-action-value-methods">Section 2.2: Action-value Methods</a></li>
<li><a href="#section-23-the-10-armed-test-bed">Section 2.3: The 10-armed Test-bed</a></li>
<li><a href="#section-24-incremental-implementation">Section 2.4: Incremental Implementation</a></li>
<li><a href="#section-25-tracking-a-non-stationary-problem">Section 2.5: Tracking a Non-stationary Problem</a></li>
<li><a href="#section-26-optimistic-initial-values">Section 2.6: Optimistic Initial Values</a></li>
<li><a href="#section-27-upper-confidence-bound-action-selection">Section 2.7: Upper-Confidence-Bound Action Selection</a></li>
<li><a href="#section-28-gradient-bandit-algorithms">Section 2.8: Gradient Bandit Algorithms</a></li>
<li><a href="#section-29-associative-search-contextual-bandits">Section 2.9: Associative Search (Contextual Bandits)</a></li>
<li><a href="#section-210-summary">Section 2.10: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-3-finite-markov-decision-processes">Chapter 3: Finite Markov Decision Processes</a>
<ol>
<li><a href="#section-31-the-agent-environment-interface">Section 3.1: The Agent-Environment Interface</a></li>
<li><a href="#section-32-goals-and-rewards">Section 3.2: Goals and Rewards</a></li>
<li><a href="#section-33-returns-and-episodes">Section 3.3: Returns and Episodes</a></li>
<li><a href="#section-34-unified-notation-for-episodic-and-continuing-tasks">Section 3.4: Unified Notation for Episodic and Continuing Tasks</a></li>
<li><a href="#section-35-policies-and-value-functions">Section 3.5: Policies and Value Functions</a></li>
<li><a href="#section-36-optimal-policies-and-optimal-value-functions">Section 3.6: Optimal Policies and Optimal Value Functions</a></li>
<li><a href="#section-37-optimality-and-approximation">Section 3.7: Optimality and Approximation</a></li>
<li><a href="#section-38-summary">Section 3.8: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-4-dynamic-programming">Chapter 4: Dynamic Programming</a>
<ol>
<li><a href="#section-41-policy-evaluation-prediction">Section 4.1: Policy Evaluation (Prediction)</a></li>
<li><a href="#section-42-policy-improvement">Section 4.2: Policy Improvement</a></li>
<li><a href="#section-43-policy-iteration">Section 4.3: Policy Iteration</a></li>
<li><a href="#section-44-value-iteration">Section 4.4: Value Iteration</a></li>
<li><a href="#section-45-asynchronous-dynamic-programming">Section 4.5: Asynchronous Dynamic Programming</a></li>
<li><a href="#section-46-generalized-policy-iteration">Section 4.6: Generalized Policy Iteration</a></li>
<li><a href="#section-47-efficiency-of-dynamic-programming">Section 4.7: Efficiency of Dynamic Programming</a></li>
<li><a href="#section-48-summary">Section 4.8: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-5-monte-carlo-methods">Chapter 5: Monte Carlo Methods</a>
<ol>
<li><a href="#section-51-monte-carlo-prediction">Section 5.1: Monte Carlo Prediction</a></li>
<li><a href="#section-52-monte-carlo-estimation-of-action-values">Section 5.2: Monte Carlo Estimation of Action Values</a></li>
<li><a href="#section-53-monte-carlo-control">Section 5.3: Monte Carlo Control</a></li>
<li><a href="#section-54-monte-carlo-control-without-exploring-starts">Section 5.4: Monte Carlo Control without Exploring Starts</a></li>
<li><a href="#section-55-off-policy-prediction-via-importance-sampling">Section 5.5: Off-policy Prediction via Importance Sampling</a></li>
<li><a href="#section-56-incremental-implementation">Section 5.6: Incremental Implementation</a></li>
<li><a href="#section-57-off-policy-monte-carlo-control">Section 5.7: Off-policy Monte Carlo Control</a></li>
<li><a href="#section-58-discounting-aware-importance-sampling">Section 5.8: *Discounting-aware Importance Sampling</a></li>
<li><a href="#section-59-per-decision-importance-sampling">Section 5.9: *Per-decision Importance Sampling</a></li>
<li><a href="#section-510-summary">Section 5.10: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-6-temporal-difference-learning">Chapter  6: Temporal-Difference Learning</a>
<ol>
<li><a href="#section-61-td-prediction">Section 6.1: TD Prediction</a></li>
<li><a href="#section-62-advantages-of-td-prediction-methods">Section 6.2: Advantages of TD Prediction Methods</a></li>
<li><a href="#section-63-optimality-of-td0">Section 6.3: Optimality of TD(0)</a></li>
<li><a href="#section-64-sarsa-on-policy-td-control">Section 6.4: Sarsa: On-policy TD Control</a></li>
<li><a href="#section-65-q-learning-off-policy-td-control">Section 6.5: Q-learning: Off-policy TD Control</a></li>
<li><a href="#section-66-expected-sarsa">Section 6.6: Expected Sarsa</a></li>
<li><a href="#section-67-maximization-bias-and-double-learning">Section 6.7: Maximization Bias and Double Learning</a></li>
<li><a href="#section-68-games-afterstates-and-other-special-cases">Section 6.8: Games, Afterstates, and Other Special Cases</a></li>
<li><a href="#section-69-summary">Section 6.9: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-7--step-bootstrapping">Chapter 7: n-step Bootstrapping</a>
<ol>
<li><a href="#section-71--step-td-prediction">Section 7.1: n-step TD Prediction</a></li>
<li><a href="#section-72--step-sarsa">Section 7.2: n-step Sarsa</a></li>
<li><a href="#section-73--step-off-policy-learning">Section 7.3: n-step Off-policy Learning</a></li>
<li><a href="#section-74-per-decision-methods-with-control-variates">Section 7.4: *Per-decision Methods with Control Variates</a></li>
<li><a href="#section-75-off-policy-learning-without-importance-sampling-the--step-tree-backup-algorithm">Section 7.5: Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm</a></li>
<li><a href="#section-76-a-unifying-algorithm--step">Section 7.6: *A Unifying Algorithm: n-step Q(sigma)</a></li>
<li><a href="#section-77-summary">Section 7.7: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-8-planning-and-learning-with-tabula-methods">Chapter 8: Planning and Learning with Tabular Methods</a>
<ol>
<li><a href="#section-81-models-and-planning">Section 8.1: Models and Planning</a></li>
<li><a href="#section-82-dyna-integrated-planning-acting-and-learning">Section 8.2: Dyna: Integrated Planning, Acting, and Learning</a></li>
<li><a href="#section-83-when-the-model-is-wrong">Section 8.3: When the Model Is Wrong</a></li>
<li><a href="#section-84-prioritized-sweeping">Section 8.4: Prioritized Sweeping</a></li>
<li><a href="#section-85-expected-vs-sample-updates">Section 8.5: Expected vs. Sample Updates</a></li>
<li><a href="#section-86-trajectory-sampling">Section 8.6: Trajectory Sampling</a></li>
<li><a href="#section-87-real-time-dynamic-programming">Section 8.7: Real-Time Dynamic Programming</a></li>
<li><a href="#section-88-planning-at-decision-time">Section 8.8: Planning at Decision Time</a></li>
<li><a href="#section-89-heuristic-search">Section 8.9: Heuristic Search</a></li>
<li><a href="#section-810-rollout-algorithms">Section 8.10: Rollout Algorithms</a></li>
<li><a href="#section-811-monte-carlo-tree-search">Section 8.11: Monte Carlo Tree Search</a></li>
<li><a href="#section-812-summary-of-the-chapter">Section 8.12: Summary of the Chapter</a></li>
<li><a href="#section-813-summary-of-part-i-dimensions">Section 8.13: Summary of Part I: Dimensions</a></li>
</ol>
</li>
</ol>
</li>
<li><a href="#part-ii-approximate-solution-methods">Part II: Approximate Solution Methods</a>
<ol>
<li><a href="#chapter-9-on-policy-prediction-with-approximation">Chapter 9: On-policy Prediction with Approximation</a>
<ol>
<li><a href="#section-91-value-function-approximation">Section 9.1: Value-function Approximation</a></li>
<li><a href="#section-92-the-prediction-objective">Section 9.2: The Prediction Objective (VE)</a></li>
<li><a href="#section-93-stochastic-gradient-and-semi-gradient-methods">Section 9.3: Stochastic-gradient and Semi-gradient Methods</a></li>
<li><a href="#section-94-linear-methods">Section 9.4: Linear Methods</a></li>
<li><a href="#section-95-feature-construction-for-linear-methods">Section 9.5: Feature Construction for Linear Methods</a>
<ol>
<li><a href="#section-951-polynomials">Section 9.5.1: Polynomials</a></li>
<li><a href="#section-952-fourier-basis">Section 9.5.2: Fourier Basis</a></li>
<li><a href="#section-953-coarse-coding">Section 9.5.3: Coarse Coding</a></li>
<li><a href="#section-954-tile-coding">Section 9.5.4: Tile Coding</a></li>
<li><a href="#section-955-radial-basis-functions">Section 9.5.5: Radial Basis Functions</a></li>
</ol>
</li>
<li><a href="#section-96-selecting-step-size-parameters-manually">Section 9.6: Selecting Step-Size Parameters Manually</a></li>
<li><a href="#section-97-non-linear-function-approximation-artificial-neural-networks">Section 9.7: Non-linear Function Approximation: Artificial Neural Networks</a></li>
<li><a href="#section-98-least-squares-td">Section 9.8: Least-Squares TD</a></li>
<li><a href="#section-99-memory-based-function-approximation">Section 9.9: Memory-based Function Approximation</a></li>
<li><a href="#section-910-kernel-based-function-approximation">Section 9.10: Kernel-based Function Approximation</a></li>
<li><a href="#section-911-looking-deeper-at-on-policy-learning-interests-and-emphasis">Section 9.11: Looking Deeper at On-policy Learning: Interests and Emphasis</a></li>
<li><a href="#section-912-summary">Section 9.12: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-10-on-policy-control-with-approximation">Chapter 10: On-policy Control with Approximation</a>
<ol>
<li><a href="#section-101-episodic-semi-gradient-control">Section 10.1: Episodic Semi-gradient Control</a></li>
<li><a href="#section-102-semi-gradient--step-sarsa">Section 10.2: Semi-gradient n-step Sarsa</a></li>
<li><a href="#section-103-average-reward-a-new-problem-setting-for-continuing-tasks">Section 10.3: Average Reward: A New Problem Setting for Continuing Tasks</a></li>
<li><a href="#section-104-deprecating-the-discounted-setting">Section 10.4: Deprecating the Discounted Setting</a></li>
<li><a href="#section-105-differential-semi-gradient--step-sarsa">Section 10.5: Differential Semi-gradient n-step Sarsa</a></li>
<li><a href="#section-106-summary">Section 10.6: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-11-off-policy-methods-with-approximation">Chapter 11: *Off-policy Methods with Approximation</a>
<ol>
<li><a href="#section-111-semi-gradient-methods">Section 11.1: Semi-gradient Methods</a></li>
<li><a href="#section-112-examples-of-off-policy-divergence">Section 11.2: Examples of Off-policy Divergence</a></li>
<li><a href="#section-113-the-deadly-triad">Section 11.3: The Deadly Triad</a></li>
<li><a href="#section-114-linear-value-function-geometry">Section 11.4: Linear Value-function Geometry</a></li>
<li><a href="#section-115-gradient-descent-in-the-bellman-error">Section 11.5: Gradient Descent in the Bellman Error</a></li>
<li><a href="#section-116-the-bellman-error-is-not-learnable">Section 11.6: The Bellman Error is Not Learnable</a></li>
<li><a href="#section-117-gradient-td-methods">Section 11.7: Gradient-TD Methods</a></li>
<li><a href="#section-118-emphatic-td-methods">Section 11.8: Emphatic-TD Methods</a></li>
<li><a href="#section-119-reducing-variance">Section 11.9: Reducing Variance</a></li>
<li><a href="#section-1110-summary">Section 11.10: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-12-eligibility-traces">Chapter 12: Eligibility Traces</a>
<ol>
<li><a href="#section-121-the--return">Section 12.1: The lambda-return</a></li>
<li><a href="#section-122-td">Section 12.2: TD(lambda)</a></li>
<li><a href="#section-123--step-truncated--return-methods">Section 12.3: n-step Truncated lambda-return Methods</a></li>
<li><a href="#section-124-redoing-updates-online--return-algorithm">Section 12.4: Redoing Updates: Online lambda-return Algorithm</a></li>
<li><a href="#section-125-true-online-td">Section 12.5: True Online TD(lambda)</a></li>
<li><a href="#section-126-dutch-traces-in-monte-carlo-learning">Section 12.6: *Dutch Traces in Monte Carlo Learning</a></li>
<li><a href="#section-127-sarsa">Section 12.7: Sarsa(lambda)</a></li>
<li><a href="#section-128-variable-and">Section 12.8: Variable lambda and gamma</a></li>
<li><a href="#section-129-off-policy-traces-with-control-variates">Section 12.9: Off-policy Traces with Control Variates</a></li>
<li><a href="#section-1210-watkins-q-to-tree-backup">Section 12.10: Watkin's Q(lambda) to Tree-Backup(lambda)</a></li>
<li><a href="#section-1211-stable-off-policy-methods-with-traces">Section 12.11: Stable Off-policy Methods with Traces</a></li>
<li><a href="#section-1212-implementation-issues">Section 12.12: Implementation Issues</a></li>
<li><a href="#section-1213-conclusions">Section 12.13: Conclusions</a></li>
</ol>
</li>
<li><a href="#chapter-13-policy-gradient-methods">Chapter 13: Policy Gradient Methods</a>
<ol>
<li><a href="#section-131-policy-approximation-and-its-advantages">Section 13.1: Policy Approximation and its Advantages</a></li>
<li><a href="#section-132-the-policy-gradient-theorem">Section 13.2: The Policy Gradient Theorem</a></li>
<li><a href="#section-133-reinforce-monte-carlo-policy-gradient">Section 13.3: REINFORCE: Monte Carlo Policy Gradient</a></li>
<li><a href="#section-134-reinforce-with-baseline">Section 13.4: REINFORCE with Baseline</a></li>
<li><a href="#section-135-actor-critic-methods">Section 13.5: Actor-Critic Methods</a></li>
<li><a href="#section-136-policy-gradient-for-continuing-problems">Section 13.6: Policy Gradient for Continuing Problems</a></li>
<li><a href="#section-137-policy-parameterization-for-continuous-actions">Section 13.7: Policy Parameterization for Continuous Actions</a></li>
<li><a href="#section-138-summary">Section 13.8: Summary</a></li>
</ol>
</li>
</ol>
</li>
<li><a href="#part-iii-looking-deeper">Part III: Looking Deeper</a>
<ol>
<li><a href="#chapter-14-psychology">Chapter 14: Psychology</a>
<ol>
<li><a href="#section-141-prediction-and-control">Section 14.1: Prediction and Control</a></li>
<li><a href="#section-142-classical-conditioning">Section 14.2: Classical Conditioning</a>
<ol>
<li><a href="#section-1421-blocking-and-higher-order-conditioning">Section 14.2.1: Blocking and Higher-order Conditioning</a></li>
<li><a href="#section-1422-the-rescorla-wagner-model">Section 14.2.2: The Rescorla-Wagner Model</a></li>
<li><a href="#section-1423-the-td-model">Section 14.2.3: The TD Model</a></li>
<li><a href="#section-1424-td-model-simulations">Section 14.2.4: TD Model Simulations</a></li>
</ol>
</li>
<li><a href="#section-143-instrumental-conditioning">Section 14.3: Instrumental Conditioning</a></li>
<li><a href="#section-144-delayed-reinforcement">Section 14.4: Delayed Reinforcement</a></li>
<li><a href="#section-145-cognitive-maps">Section 14.5: Cognitive Maps</a></li>
<li><a href="#section-146-habitual-and-goal-directed-behavior">Section 14.6: Habitual and Goal-directed Behavior</a></li>
<li><a href="#section-147-summary">Section 14.7: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-15-neuroscience">Chapter 15: Neuroscience</a>
<ol>
<li><a href="#section-151-neuroscience-basics">Section 15.1: Neuroscience Basics</a></li>
<li><a href="#section-152-reward-signals-reinforcement-signals-values-and-prediction-errors">Section 15.2: Reward Signals, Reinforcement Signals, Values, and Prediction Errors</a></li>
<li><a href="#section-153-the-reward-prediction-error-hypothesis">Section 15.3: The Reward Prediction Error Hypothesis</a></li>
<li><a href="#section-154-dopamine">Section 15.4: Dopamine</a></li>
<li><a href="#section-155-experimental-support-for-the-reward-prediction-error-hypothesis">Section 15.5: Experimental Support for the Reward Prediction Error Hypothesis</a></li>
<li><a href="#section-156-td-errordopamine-correspondence">Section 15.6: TD Error/Dopamine Correspondence</a></li>
<li><a href="#section-157-neural-actor%E2%80%93critic">Section 15.7: Neural Actorâ€“Critic</a></li>
<li><a href="#section-158-actor-and-critic-learning-rules">Section 15.8: Actor and Critic Learning Rules</a></li>
<li><a href="#section-159-hedonistic-neurons">Section 15.9: Hedonistic Neurons</a></li>
<li><a href="#section-1510-collective-reinforcement-learning">Section 15.10: Collective Reinforcement Learning</a></li>
<li><a href="#section-1511-model-based-methods-in-the-brain">Section 15.11: Model-based Methods in the Brain</a></li>
<li><a href="#section-1512-addiction">Section 15.12: Addiction</a></li>
<li><a href="#section-1513-summary">Section 15.13: Summary</a></li>
</ol>
</li>
<li><a href="#chapter-16-applications-and-case-studies">Chapter 16: Applications and Case Studies</a>
<ol>
<li><a href="#section-161-td-gammon">Section 16.1: TD-Gammon</a></li>
<li><a href="#section-162-samuels-checkers-player">Section 16.2: Samuel's Checkers Player</a></li>
<li><a href="#section-163-watsons-daily-double-wagering">Section 16.3: Watson's Daily-Double Wagering</a></li>
<li><a href="#section-164-optimizing-memory-control">Section 16.4: Optimizing Memory Control</a></li>
<li><a href="#section-165-human-level-video-game-play">Section 16.5: Human-level Video Game Play</a></li>
<li><a href="#section-166-mastering-the-game-of-go">Section 16.6: Mastering the Game of Go</a>
<ol>
<li><a href="#section-1661-alphago">Section 16.6.1: AlphaGo</a></li>
<li><a href="#section-1662-alphago-zero">Section 16.6.2: AlphaGo Zero</a></li>
</ol>
</li>
<li><a href="#section-167-personalized-web-services">Section 16.7: Personalized Web Services</a></li>
<li><a href="#section-168-thermal-soaring">Section 16.8: Thermal Soaring</a></li>
</ol>
</li>
<li><a href="#chapter-17-frontiers">Chapter 17: Frontiers</a>
<ol>
<li><a href="#section-171-general-value-functions-and-auxiliary-tasks">Section 17.1: General Value Functions and Auxiliary Tasks</a></li>
<li><a href="#section-172-temporal-abstraction-via-options">Section 17.2: Temporal Abstraction via Options</a></li>
<li><a href="#section-173-observations-and-state">Section 17.3: Observations and State</a></li>
<li><a href="#section-174-designing-reward-signals">Section 17.4: Designing Reward Signals</a></li>
<li><a href="#section-175-remaining-issues">Section 17.5: Remaining Issues</a></li>
<li><a href="#section-176-reinforcement-learning-and-the-future-of-artificial-intelligence">Section 17.6: Reinforcement Learning and the Future of Artificial Intelligence</a></li>
</ol>
</li>
</ol>
</li>
</ol>
<h1>Chapter 1: Introduction</h1>
<p>Def. <strong>Reinforcement Learning (RL)</strong>: an agent learns how to map situations to actions through <em>trial-and-error</em> or <em>planned</em> interaction with a (possibly) uncertain environment, so as to maximize a numerical reward value (i.e., achieve his goal or goals).</p>
<ul>
<li><em>Delayed reward</em> is another important characteristic of RL, since any action taken may influence (not only the immediate reward value, but also) any subsequent rewards;</li>
<li>RL can be formalized as the optimal control of incompletely-known Markov Decision Processes (MDPs).</li>
</ul>
<p>Besides RL, other <strong>Machine Learning (ML)</strong> paradigms include <em>Supervised Learning</em> - predicting the correct label, given the corresponding set of features - and <em>Unsupervised Learning</em> - finding hidden patterns in a collection of unlabeled features.</p>
<p>A challenge unique to the RL paradigm is that of the trade-off between <strong>exploration versus exploitation</strong>. This challenge arises due to the fact that an agent prefers to take the actions that have previously given the highest rewards (<em>exploitation</em>), but it must also try out other actions in order to have more knowledge about which actions it should select (<em>exploration</em>).</p>
<p>A RL system has four main elements beyond the interactive <strong>agent</strong> and the <strong>environment</strong>, which are:</p>
<ul>
<li>A <strong>policy</strong> $$\pi_t: s \rightarrow a$$, which in stochastic cases specifies a probability for each action;</li>
<li>A <strong>reward</strong> $$r(s, a)$$, an immediate signal that specifies how good it is for an agent to have chosen a certain action in a given state (may also be stochastic);</li>
<li>A <strong>value function</strong> $$v(s)$$ that specifies the total reward an agent is expected to accumulate in the future if he starts at a given state, i.e., predicted long-term reward;</li>
<li>A (optional) <strong>world model</strong> used by model-based methods (opposed to purely trial-and-error model-free methods) for planning.</li>
</ul>
<h1>Part I: Tabular Solution Methods</h1>
<h2>Chapter 2: Multi-armed Bandits</h2>
<p><em>Non-associative</em> setting: a problem setting that involves learning to act in only 1 situation.</p>
<p><em>Associative</em> setting: a problem setting where the best action depends on the situation.</p>
<h3>Section 2.1: A $$k$$-armed Bandit Problem</h3>
<p>Setting of the $$k$$-armed bandit learning problem (analogous to a slot machine with $$k$$ levers):</p>
<ol>
<li>Choose 1 action from among $$k$$ different options;</li>
<li>Receive a (numerical) reward from a stationary probability distribution which depends on the action selected;</li>
<li>Repeat steps 1 and 2 with the purpose of maximizing the expected total reward over some time period (e.g., 1000 action selections or <em>time steps</em>).</li>
</ol>
<p><strong>Value</strong> of an action: the expected or mean reward received if that action is selected</p>
<p>Letting $$A_t$$ be the action taken at time step $$t$$ and $$R_t$$ the corresponding reward, then the value $$q^{*}(a)$$ of an arbitrary action $$a$$ is given by:</p>
<p>$$
\begin{equation}
q^{*} (a) \doteq \mathbb{E} [R_t | A_t = a].
\end{equation}
$$</p>
<p>Since we do not know the true value of each action, we need to estimate them in such a way that the estimates are close to the real values. The estimated value of an action $$a$$ at time step $$t$$ is denoted by $$Q_t (a)$$.</p>
<p><strong>Greedy</strong> action: the action with the highest estimated value at a given time step</p>
<ul>
<li>Choosing this action equates to the agent <strong>exploiting</strong> his current knowledge of the values of the actions;</li>
<li>Selecting 1 of the non-greedy actions enables the agent to improve his estimates of the non-greedy action's value, i.e., <strong>exploration</strong>;</li>
<li>Exploitation maximizes the reward on 1 step, but it needs to be intercalated with exploration steps so as to maximize the greater total reward in the long term.</li>
</ul>
<h3>Section 2.2: Action-value Methods</h3>
<p>Def. <strong>Action-value Methods</strong>: methods used to estimate the values of actions and to use those estimates to select an action to take at a given time step.</p>
<p>Letting $$\mathbb{1}_{predicate}$$ be the random variable which equals $$1$$ if the $$predicate$$ is true and $$0$$ otherwise, the value of an action can be estimated by averaging the rewards received: <!-- TODO: check if equations inside text inside equations need double $ signs--></p>
<p>$$
\begin{equation}
Q_t (a) \doteq \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac{\sum_{i = 1}^{t - 1} R_i \cdot \mathbb{1}<em>{A_i = a}}{\sum</em>{i = 1}^{t - 1} \mathbb{1}_{A_i = a}}.
\end{equation}
$$</p>
<p>If the denominator is zero (action has never been taken), then $$Q_t(a)$$ is defined as an arbitrary default value (e.g., zero). By the law of large numbers, as the denominator goes to infinity, $$Q_t(a)$$ converges to $$q^{*}(a)$$. This is called the <em>sample-average</em> method for estimating action values.</p>
<p>The simplest action selection rule is to always select a greedy action and - if there is more than 1 action with the same highest value - to break ties in some arbitrary way (e.g., randomly). This action selection method can be written as:</p>
<p>$$
\begin{equation}
A_t = \argmax_a Q_t (a).
\end{equation}
$$</p>
<p>This selection method never performs exploration. A simple alternative that does so is to select the greedy action most of the time (probability $$1 - \epsilon$$) and (with probability $$\epsilon$$) to randomly select any possible action with equal probability. Methods that use this near-greedy action selection rule are dubbed $$\epsilon$$-greedy methods.</p>
<h3>Section 2.3: The 10-armed Test-bed</h3>
<p><strong>Non-stationary</strong> setting: problem setting where the true values of the actions (or the reward probabilities) change over time.</p>
<p>Given a set of 2000 randomly generated $$k$$-armed bandit problems (with $$k = 10$$), for each problem in the set, the action values $$q^{<em>}(a), \ a = {1, 2, \dots, 10},$$ were selected from a normal (Gaussian) distribution with $$\mu = 0, \  \sigma^2 = 1$$. When a learning method is applied to this problem selects action $$A_t$$ at time step $$t$$, the actual reward ($$R_t$$) was drawn from a normal distribution with $$\mu = q^{</em>}(A_t), \ \sigma^2 = 1$$.</p>
<p>The performance of the learning methods is measured as it improves with experience over 1000 time steps of the bandit problem, which makes up a single run. To obtain an accurate measure of the learning algorithms' behavior, 2000 runs are performed and the results for the bandit problems are averaged.</p>
<p>A greedy action selection method is compared against 2 $$\epsilon$$-greedy methods (with $$\epsilon = 0.01 \lor \epsilon = 0.1$$). All methods begin with initial action-value estimates of zero and update these estimates using the sample-average technique.</p>
<p>While the greedy method improved slightly faster than the other 2, it converged to a reward-per-step of 1, which is lower than the best value of around 1.54 achieved by the $$\epsilon$$-greedy method (with $$\epsilon = 0.1$$). The method with $$\epsilon = 0.1$$ improved faster than the method with $$\epsilon = 0.01$$, since it explored more earlier. However, the method with $$\epsilon = 0.01$$ converges to a higher reward-per-step in the long run, since the method with $$\epsilon = 0.1$$ never selects the optimal action more than 91% of the time.
It is possible to perform $$\epsilon$$ annealing to try to get fast learning at the start combined with convergence to a higher reward average.</p>
<p>It takes more exploration to find the optimal actions in cases with noisy rewards (i.e., high reward variance), meaning that $$\epsilon$$-greedy methods perform even better in those cases, when compared to the greedy method. Also, although the greedy method is theoretically optimal in the deterministic case (i.e., with $$\sigma^2 = 0$$), this property does not hold in non-stationary bandit problems, making exploration a necessity even in deterministic settings.</p>
<h3>Section 2.4: Incremental Implementation</h3>
<p>For a single action, let $$R_i$$ denote the reward received after the $$i^{th}$$ selection of <em>this action</em> and $$Q_n$$ the estimate of its action value after it has been selected $$n - 1$$ times, written as:</p>
<p>$$
\begin{equation}
Q_n \doteq \frac{R_1 + R_2 + \dots + R_{n - 1}}{n - 1}.
\end{equation}
$$</p>
<p>Instead of maintaining a record of all the rewards and performing the computation for the estimated value whenever needed (resulting in the growth of both computational and memory requirements), we can devise incremental formulas to update the averages with a small and constant computation to process each new reward. Given $$Q_n$$ and the $$n^{th}$$ reward $$R_n$$, the new average of all $$n$$ rewards can be computed as:</p>
<p>$$
\begin{align}
Q_{n + 1} &#x26;= \frac{1}{n} \sum_{i = 1}^n R_i \nonumber\
&#x26;= \frac{1}{n}(R_n + \sum_{i = 1}^{n - 1} R_i) \nonumber\
&#x26;= \frac{1}{n}(R_n + (n - 1) \cdot \frac{1}{n - 1} \cdot \sum_{i = 1}^{n - 1} R_i) \nonumber\
&#x26;= \frac{1}{n} (R_n + (n - 1) \cdot Q_n) \nonumber\
&#x26;= \frac{1}{n} (R_n + n \cdot Q_n - Q_n) \nonumber\
&#x26;= Q_n + \frac{1}{n} [R_n - Q_n], \ n > 1 \
Q_2 &#x26;= R_1, \ Q_1 \in \mathbb{R}.
\end{align}
$$</p>
<p>This implementation only needs memory for $$Q_n$$ and $$n$$, and only performs a small computation for each new reward.
The general form of the previous update rule is given by:</p>
<p>$$
\begin{equation}
NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate],
\end{equation}
$$</p>
<p>where $$[Target - OldEstimate]$$ is an <em>error</em> in the estimate, which is reduced by taking a step towards the (possibly noisy) target value.
The step-size parameter is generally denoted by $$\alpha$$ or $$\alpha_t (a)$$.</p>
<pre><code>def bandit_problem(int k, float epsilon, bandits):
	Q_a = [0]*k;
	N_a = [0]*k;

	while(True):
		if random_float(0, 1) &#x3C;= epsilon:
			A = random_int(0, k)
		else:
			A = argmax(Q_a)
		R = bandits[A].get_reward()
		N_a[A]++;
		Q_a[A] += (1/N_a[A]) * (R - Q_a[A]);
</code></pre>
<h3>Section 2.5: Tracking a Non-stationary Problem</h3>
<p>When reward probabilities change over time, it makes sense to give more weight to recent rewards than to those receive long ago. This can be done by using a constant step-size parameter, e.g., for updating an average $$Q_n$$ of the $$n - 1$$ past rewards w.h.t.:</p>
<p>$$
\begin{equation}
Q_{n + 1} \doteq Q_n + \alpha [R_n - Q_n],
\end{equation}
$$</p>
<p>where the step-size parameter $$\alpha \in \  ]0, 1]$$ is constant. Given this, $$Q_{n + 1}$$ becomes a weighted average (since the sum of weights = 1) of the past rewards and initial estimate $$Q_1$$:</p>
<p>$$
\begin{align}
Q_{n + 1} &#x26;= Q_n + \alpha [R_n - Q_n] \nonumber\
&#x26;= \alpha \cdot R_n + (1- \alpha) Q_n \nonumber\
&#x26;= \alpha \cdot R_n + (1 - \alpha) [\alpha \cdot R_{n - 1} + (1 - \alpha)Q_{n - 1}] \nonumber\
&#x26;= \alpha \cdot R_n + (1 - \alpha) \cdot \alpha \cdot R_{n - 1} + (1 - \alpha)^2 Q_{n - 1} \nonumber\
&#x26;= \alpha \cdot R_n + (1 - \alpha) \cdot \alpha \cdot R_{n - 1} + \dots + (1 - \alpha)^{n - 1} \cdot \alpha \cdot R_1 + (1 - \alpha)^n \cdot Q_1 \nonumber\
&#x26;= (1 - \alpha)^n \cdot Q_1 + \sum_{i = 1}^n \alpha \cdot (1 - \alpha)^{n - i} \cdot R_i.
\end{align}
$$</p>
<p>Since $$1 - \alpha &#x3C; 1$$, the weight given to $$R_i$$ decreases as the number of intervening rewards increases. Also, the weight decays exponentially in proportion to the exponent on $$1 - \alpha$$ and, if $$1 - \alpha = 0$$, the entire weight goes onto the very last reward $$R_n$$. This method is sometimes called an <em>exponential recency-weighted average</em>.</p>
<p>Letting $$\alpha_n (a)$$ denote the step-size parameter to process the reward obtained after the $$n^{th}$$ selection of action $$a$$, for the sample-average method, w.h.t. $$\alpha_n (a) = 1/n$$, whose convergence to the true action values is guaranteed by the law of large numbers. However, convergence is <strong>NOT</strong> guaranteed for all choices of the $${\alpha_n (a)}$$ sequence. Through a result in stochastic approximation theory, we obtain the conditions required to assure convergence with probability 1:</p>
<p>$$
\begin{equation}
\sum_{n = 1}^{\infty} \alpha_n (a) = \infty \quad \land \quad \sum_{n = 1}^{\infty} \alpha_n^2 (a) &#x3C; \infty,
\end{equation}
$$</p>
<p>where the first condition is required to guarantee that the steps are big enough to overcome any initial conditions or random fluctuations that would otherwise result in getting stuck at saddle points, and the second condition guarantees that the steps will eventually become small enough to assure convergence.</p>
<p>The second condition is not met for the constant step-size parameter case, i.e., $$\alpha_n (a) = \alpha$$. This means that the estimates will never completely converge, which is actually a desirable property for non-stationary problems (the most common type of problem in RL), since the estimates continue to vary in response to the most recently received rewards, accounting for the changes in reward probabilities over time. Also, the sequences of step-size parameters that meet both of the above conditions often lead to slow convergence rates, meaning that these are seldomly used in applications and empirical research.</p>
<h3>Section 2.6: Optimistic Initial Values</h3>
<p>All previous methods are somewhat dependent on the initial action-value estimates $$Q_1 (a)$$, i.e., they are <em>biased</em> by their initial estimates. This bias decreases over time as various actions are selected. However, while for sample-average methods the bias eventually disappear after all actions have been taken at least once, the bias is permanent for methods with a constant $$\alpha$$.</p>
<p>This property means that, when using methods with a constant $$\alpha$$, the user must select the values for the initial estimates, which provides a way to supply some prior knowledge about the expected rewards, at the possible cost of being harder to tune.</p>
<p>By selecting optimistic initial action-values, i.e., $$Q_1 (a) >> R_1 (a), \forall a$$, the agent will always be disappointed since the rewards will always be far less than the first estimates, regardless of which actions are selected. This encourages exploration, as the agent will select all possible actions before the value estimates converge, even if greedy actions are selected at every single time step.</p>
<p>This technique for encouraging exploration is named <em>optimistic initial values</em> and is a simple, yet effective trick when used on stationary problems (e.g., with $$Q_1(a) = 5$$ it outperforms a $$\epsilon$$-greedy method with $$Q_1(a) = 0$$ and $$\epsilon = 0.1$$).</p>
<p>An important caveat is that, since the drive for exploration in the previous technique is dependent on the initials conditions and disappears after a certain time, it cannot adequately deal with non-stationary problems, where exploration is always required due to the dynamic nature of the reward probabilities. This drawback is present in all methods that treat the beginning of time as a special event (e.g., the sample-average methods).</p>
<h3>Section 2.7: Upper-Confidence-Bound Action Selection</h3>
<p>While $$\epsilon$$-greedy methods encourage exploration, they do so equally, without any preference for whether the action selected is nearly greedy or particularly uncertain. However, it is possible to select the non-greedy actions while taking into account both how close their value estimates are to the maximal action-value and the estimation uncertainty. An effective way of doing this is to select actions according to the following equation:</p>
<p>$$
\begin{equation}
A_t \doteq \argmax_a [Q_t(a) + c \cdot \sqrt{\frac{\ln t}{N_t(a)}}],
\end{equation}
$$</p>
<p>where $$N_t(a)$$ denotes the number of times the action $$a$$ has been selected prior to time $$t$$ and the number $$c > 0$$ controls the degree of exploration. If $$N_t(a) = 0$$, then $$a$$ is considered to be a maximizing action.</p>
<p>This <strong>upper confidence bound (UCB)</strong> action selection is based on the idea that the square-root term is a measure of the uncertainty or variance of action $$a$$ value's estimate. As such, the max'ed over quantity becomes a sort of upper bound on the possible true value of action $$a$$ with $$c$$ determining the confidence level, and thus the uncertainty is reduced each time the action $$a$$ is selected.</p>
<p>The natural logarithm results in smaller increases over time, meaning that actions with lower value estimates or that have been frequently selected, will be selected with decreasing frequency.</p>
<p>UCB often performs better than $$\epsilon$$-greedy action selection (except in the first $$k$$ steps), but it is harder to extend beyond bandits into the general RL settings. This is due to its difficulties in dealing with more advanced settings, such as non-stationary problems and (function approximation) with large state spaces.</p>
<h3>Section 2.8: Gradient Bandit Algorithms</h3>
<p>Beyond using action-value estimates to select actions, it is also possible to learn a numerical <em>preference</em> for each action $$a$$, denoted $$H_t(a) \in \mathbb{R}$$, which has no interpretation w.r.t. reward. As such, only the relative preference of 1 action over another is important. The action probabilities are determined according to a softmax distribution as follows:</p>
<p>$$
\begin{equation}
Pr{A_t = a} \doteq \frac{\exp(H_t(a))}{\sum_{b = 1}^k \exp(H_t(b))} \doteq \pi_t (a),
\end{equation}
$$</p>
<p>where $$\pi_t(a)$$ is the probability of taking action $$a$$ at time $$t$$. All actions have an equal probability of being selected at first (i.e., $$H_1(a) = 0, \forall a$$).</p>
<p>There exists a natural learning algorithm for softmax action preferences based on the idea of <strong>Stochastic Gradient Ascent (SGA)</strong>, where, at each time step, after selecting action $$A_t$$, and receiving the reward $$R_t$$, the action preferences are updated as follows:</p>
<p>$$
\begin{align}
H_{t + 1}(A_t) &#x26;\doteq H_t(A_t) + \alpha (R_t - \bar{R_t}) (1 - \pi_t(A_t)), &#x26;\text{and}\
H_{t + 1}(a) &#x26;\doteq H_t(a) - \alpha (R_t \bar{R_t}) \pi_t(a), &#x26;\forall a \neq A_t,
\end{align}
$$</p>
<p>where $$\alpha > 0$$ is a step-size parameter and $$\bar{R}_t \in \mathbb{R}$$ - which serves as baseline to compare against the reward - is the average of the rewards up to but not including time $$t$$ (with $$\bar{R}_1 \doteq R_1$$). If $$R_t > \bar{R}_t, \  t \neq 1$$, then the probability of taking $$A_t$$ in the future is increased, otherwise, the probability of taking $$A_t$$ is decreased if $$R_t &#x3C; \bar{R}_t$$. Also, the unselected actions probabilities are updated in the opposite direction.</p>
<p>Since only the relative preferences are taken into account, adding an arbitrary constant value to all the action preferences has no effect on the action probabilities. Also, since the reward baseline term instantaneously adapts to new values of the mean, shifting the mean (e.g., $$\mu_{new} = \mu_{old} + 4$$) of the distribution (while keeping the unit variance) has no effect on the gradient bandit algorithm. However, omitting the baseline term results in a significantly degraded performance.</p>
<h4>The Bandit Gradient Algorithm as SGA</h4>
<p>In exact <strong>Gradient Ascent (GA)</strong>, each action preference $$H_t(a)$$ would be incremented in proportion to the increment's effect on performance, given by:</p>
<p>$$
\begin{equation}
H_{t + 1} (a) \doteq H_t(a) + \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t (a)},
\end{equation}
$$</p>
<p>where the measure of performance is the expected reward:</p>
<p>$$
\begin{equation}
\mathbb{E}[R_t] = \sum_x \pi_t (x) \cdot q^{*} (x),
\end{equation}
$$</p>
<p>and the measure of the increment's effect is the <em>partial derivative</em> of this performance measure w.r.t. the action preference. Since $$q^{*}(x)$$ is not known, it is impossible to use exact GA. As such, the updates will instead take the form of those used in SGA.</p>
<p>The exact performance gradient can be written as:</p>
<p>$$
\begin{equation}
\frac{\partial \mathbb{E} [R_t]}{\partial H_t(a)} = \frac{\partial [\sum_x \pi_t(x) \cdot q^{<em>}(x)]}{\partial H_t(a)} = \sum_x q^{</em>}(x) \frac{\partial \pi_t(x)}{\partial H_t(a)} = \sum_x (q^{*}(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)},
\end{equation}
$$</p>
<p>where the baseline $$B_t$$ can be any scalar value that doesn't depend on $$x$$. Since the sum of probabilities is always one, the sum of the changes $$\sum_x \frac{\partial \pi_t (x)}{\partial H_t (a)} = 0$$, and the baseline can be added without changing the equality.
We continue by multiplying each term of the sum by $$\pi_t(x)/\pi_t(x)$$, as follows:</p>
<p>$$
\begin{align}
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} &#x26;= \sum_x \pi_t(x) \cdot (q^{<em>}(x) - B_t) \cdot \frac{\partial \pi_t (x)}{\partial H_t(x)} / \pi_t(x) \nonumber\
&#x26;= \mathbb{E} [(q^{</em>}(A_t) - B_t) \cdot \frac{\partial \pi_t (A_t)}{\partial H_t(a)}/\pi_t(A_t)] \nonumber\
&#x26;= \mathbb{E}[(R_t - \bar{R_t}) \cdot \frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)] \nonumber\
&#x26;= \mathbb{E} [(R_t - \bar{R}<em>t) \cdot \pi_t(A_t) \cdot (\mathbb{1}</em>{a = A_t} - \pi_t(a))/\pi_t(A_t)] \nonumber\
&#x26;= \mathbb{E}[(R_t - \bar{R}<em>t) \cdot (\mathbb{1}</em>{a = A_t} - \pi_t(a))],
\end{align}
$$</p>
<p>where the chosen baseline is $$B_t = \bar{R_t}$$ and $$R_t$$ is substituted for $$q^{<em>}(A_t)$$, which is allowed since $$\mathbb{E}[R_t|A_t] = q^{</em>}(A_t)$$.
By substituting a sample of the expectation above for the performance gradient, w.h.t.:</p>
<p>$$
\begin{equation}
H_{t + 1}(a) = H_t (a) + \alpha \cdot (R_t - \bar{R}<em>t) \cdot (\mathbb{1}</em>{a = A_t} - \pi_t(a)), \quad \forall a,
\end{equation}
$$</p>
<p>which is equivalent to the original algorithm.
By recalling the standard quotient rule for derivatives</p>
<p>$$
\begin{equation}
\frac{\partial}{\partial x}[\frac{f(x)}{g(x)}] = \frac{\frac{\partial f(x)}{\partial x}g(x) - f(x)\frac{\partial g(x)}{\partial x}}{g(x)^2},
\end{equation}
$$</p>
<p>we can then write</p>
<p>$$
\begin{align}
\frac{\partial \pi_t(x)}{\partial H_t(a)} &#x26;= \frac{\partial}{\partial H_t(a)} \pi_t(x) \nonumber\
&#x26;= \frac{\partial}{\partial H_t(a)} [\frac{\exp(H_t(x))}{\sum_{y=1}^k \exp(H_t(y))}] \nonumber\
&#x26;= \frac{\frac{\partial \exp(H_t(x))}{\partial H_t(a)} \sum_{y=1}^k \exp(H_t(y)) - \exp(H_t(x)) \cdot \exp (H_t(a))}{(\sum_{y=1}^k \exp(H_t(y)))^2} \nonumber\
&#x26;= \frac{\mathbb{1}<em>{a=x \exp(H_t(x))} \sum</em>{y=1}^k \exp(H_t(y)) - \exp(H_t(x)) \exp(H_t(a))}{(\sum_{y=1}^k \exp(H_t(y)))^2} \nonumber\
&#x26;= \frac{\mathbb{1}<em>{a = x \exp(H_t(x))}}{\sum</em>{y=1}^k \exp(H_t(y))} - \frac{\exp(H_t(y)) \exp(H_t(a))}{(\sum_{y=1}^k \exp(H_t(y)))^2} \nonumber\
&#x26;= \mathbb{1}<em>{a = x} \pi_t(x) - \pi_t(x) \pi_t(a) \nonumber\
&#x26;= \pi_t(x) (\mathbb{1}</em>{a=x} - \pi_t(a)),
\end{align}
$$</p>
<p>thus showing that the expected updated of the gradient bandit algorithm is equivalent to the gradient of the expected reward, making the the algorithm a instance of SGA.</p>
<h3>Section 2.9: Associative Search (Contextual Bandits)</h3>
<p><em>Associative search</em> tasks - which involve learning about which actions are the best through trial-and-error and associating these actions with which situations they work the best in - are often called <em>contextual bandits</em>. These tasks serve as an intermediate between the $$k$$-armed bandit problem and the full RL problem, since each action affects only the immediate reward - like the former - and also involves learning a policy, like the latter.</p>
<p>An example of an associative task is a one composed of several $$k$$-armed bandit problems, each identified by a given color, where at each step you are confronted with one of the $$k$$-armed bandit problems at random. If the action values change as the color changes, you can then learn a policy that maps a color to the best associated actions.</p>
<h3>Section 2.10: Summary</h3>
<p>W.r.t. performance (average reward) in the $$k$$-bandit problem, with $$k = 10$$ and taking into account the first 1000 steps, w.h.t. UCB $$\geq$$ Greedy with optimistic initialization $$\alpha = 0.1 \ \geq$$ Gradient bandit $$\geq$$ $$\epsilon$$-greedy.</p>
<p>Another approach to balance exploration and exploitation in $$k$$-armed bandit problems is the Bayesian method known as <em>Gittins</em> index. It assumes a known prior distribution over the actions values and then updates the distribution after each step (assuming that the true action values are stationary).</p>
<h2>Chapter 3: Finite Markov Decision Processes</h2>
<p>Markov Decision Processes (MDPs) are a formalization of sequential decision making where actions influence not only the immediate reward, but also future rewards. As such, this is an associative problem that takes into account the need to trade-off immediate and delayed reward. While in bandit problems we estimated the value $$q^{<em>}(a), \ \forall a \in \mathcal{A},$$ in an MDP, we estimate the value $$q^{</em>}(s, a), \ \forall a \in \mathcal{A}, \forall s \in \mathcal{S},$$ or the value $$v^{*}(s), \forall s \in \mathcal{S}$$ given optimal action selections. Such state-dependent values are important to assign credit for long-term rewards to individual action selections.</p>
<h3>Section 3.1: The Agent-Environment Interface</h3>
<p>An MDP involves a learner and decision maker (i.e., the <em>agent</em>) that interacts with its surroundings (i.e., the <em>environment</em>) by continually selecting actions and having the environment respond by presenting new situations (or states) to the agent and giving rise to rewards, which the agent seeks to maximize over time. This process is illustrated in Figure 1.</p>
<figure align='center'>
    <img alt="The agent-environment interaction in a Markov decision process." src="http://acfpeacekeeper.github.io/github-pages/assets/images/literature/rl_mdp.png" onerror="this.src='http://localhost:4000/assets/images/literature/rl_mdp.png';">
	<figcaption>Figure 1: The agent-environment interaction in a MDP.</figcaption>
</figure>
<p>The agent and environment interact with each other in a sequence of discrete time steps $$t = 0, 1, \dots, n.$$ At each time step $$t$$, the agent receives a representation of the environment's <em>state</em> $$S_t \in \mathcal{S},$$ and on that basis selects an <em>action</em> $$A_t \in \mathcal{A}(s).$$ At the next time step $$t + 1$$, the agent receives a reward $$R_{t + 1} \in \mathcal{R} \subset \mathbb{R}$$ and finds itself in another state $$s_{t+1}.$$  Then, the MDP and agent give rise to a sequence or <em>trajectory</em> like this:</p>
<p>$$
\begin{equation}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots
\end{equation}
$$</p>
<p>In a <em>finite</em> MDP, the random variables $$R_t$$ and $$S_t$$ have well defined discrete probability distributions that depend only on the previous state and action, i.e., for particular values of these random variables $$s' \in \mathcal{A}, \ r \in \mathcal{R},$$ there is a probability of those values occurring at time step $$t,$$ given particular values of the previous state and action:</p>
<p>$$
\begin{equation}
p(s', r \vert s, a) \doteq P(S_t = s', R_t \vert S_{t-1} = s, A_{t-1} = a), \ \forall s', s \in \mathcal{S}, \forall r \in \mathcal{R}, \forall a \in \mathcal{A}(s).
\end{equation}
$$</p>
<p>The function $$p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$$, which completely characterizes the MDP environment's <em>dynamics</em>, is an ordinary deterministic function with four arguments. This function $$p$$ specifies a probability distribution for each choice of $$s$$ and $$a$$, i.e.,</p>
<p>$$
\begin{equation}
\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \vert s, a) = 1, \ \forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s).
\end{equation}
$$</p>
<p><strong>Markov property</strong>: the state must include information about all aspects of the past agent-environment interaction that make a difference for the future. In practice, this means that the probability of each possible value for $$S_t$$ and $$R_t$$ depends only on the previous state $$S_{t-1}$$ and action $$A_{t-1}$$.
- While most methods in this book assume this property to be true, there are methods that don't rely on it. <a href="#chapter-17-frontiers">Chapter 17</a> considers how to efficiently learn a Markov state from non-Markov observations.</p>
<p>From the dynamics function $$p,$$ we can compute the <em>state-transition probabilities</em> $$p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1],$$</p>
<p>$$
\begin{equation}
p(s' \vert s, a) \doteq P(S_t = s' \vert S_{t-1} = s, A_{t-1} = a) = \sum_{r \in \mathcal{R}} p(s', r \vert s, a).
\end{equation}
$$</p>
<p>We can also compute the expected rewards for state-action pairs as a two-argument function $$r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$$:</p>
<p>$$
\begin{equation}
r(s, a) \doteq \mathbb{E} [R_t \vert S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r \vert s, a),
\end{equation}
$$</p>
<p>and the expected rewards for the state-action-new_state triples as a three-argument function $$r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R},$$</p>
<p>$$
\begin{equation}
r(s, a, s') \doteq \mathbb{E} [R_t \vert S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r \vert s, a)}{p(s' \vert s, a)}.
\end{equation}
$$</p>
<p>The MDP framework is flexible and can be applied to problems in many different ways, e.g., time steps don't need to refer to fixed intervals of real time; they can refer to arbitrary sequential stages of decision making and acting. In general, actions can be any decision we want to learn how to make, and states can be anything we can know that might useful in making them.</p>
<p>The boundary between agent and environment is usually drawn closer to the agent than the physical boundary of a robot or an animal's body, e.g., the motors and mechanical linkages of a robot and its sensing hardware should normally be considered parts of the environment, rather than of the agent. In a similar sense, rewards, which are usually computed inside the physical bodies of natural and artificial learning systems, are considered external to the agent. In general, anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.</p>
<p>We don't assume that everything in the environment is unknown to the agent, e.g., the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. However, the reward computation is considered to be external to the agent, because it defines the task facing the agent and must thus be beyond its ability to arbitrarily change.</p>
<p>The MDP framework proposes that any detail of whatever objective (of a problem of learning goal-directed behavior) one is trying to achieve can be reduced to three signal passing back and forth between an agent and its environment:</p>
<ul>
<li>Action: signal that represents the choices made by the agent;</li>
<li>State: signal that represents the basis on which the choices are made;</li>
<li>Reward: signal that defines the agent's goal.</li>
</ul>
<h3>Section 3.2: Goals and Rewards</h3>
<p>In RL, the purpose of the agent is formalized in terms of a simple number, the <em>reward</em> (at each time step $$R_t \in \mathbb{R}$$), which passes from the environment to the agent. The agent's goal is to maximize the total cumulative reward, something stated in the <em>reward hypothesis</em>:</p>
<pre><code>That all of what we mean by goals and purposes can be well thought of as
the maximization of the expected value of the cumulative sum of a received
scalar signal (called reward).
</code></pre>
<p>In order for the agent to achieve our goals, it is critical that the reward signals defined truly indicate what we want accomplished. Of particular importance, one must not use the reward signal to impart prior knowledge to the agent. Using chess as an example, we should naturally define the reward as $$+1$$ for winning, $$-1$$ for losing and $$0$$ for draws and all non-terminal positions. We should <strong>NOT</strong> give rewards for sub-goals like taking an opponent's chess piece, otherwise the agent might find a way to maximize its reward, even at the cost of losing the game. The reward signal defines <strong>what</strong> you want the agent to achieve, not <em>how</em> you want the agent to achieve it.</p>
<h3>Section 3.3: Returns and Episodes</h3>
<p>We seek to maximize the <em>expected return</em>, where the return $$G_t$$ is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:</p>
<p>$$
\begin{equation}
G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T,
\end{equation}
$$</p>
<p>where $$T$$ is a final time step. This approach makes sense in applications where the agent-environment interaction can be naturally broken down into sub-sequences, called <strong>episodes,</strong> like the plays of a game or trips to a maze. Each episode ends in a special state, called the <em>terminal</em> state, and the resets to a starting state or a sample from standard distribution of starting states. Since the next episode begins independently of how the previous one ended, episodes can all be considered to end in the same terminal state, just with different rewards for different outcomes.</p>
<p><strong>Episodic task</strong>: tasks that have discrete independent episodes, i.e., where the outcome of the ending of an episode doesn't effect the start state of the next episode.</p>
<ul>
<li>We sometimes distinguish the set of all non-terminal states $$\mathcal{S}$$ from the set of all states plus the terminal state $$\mathcal{S}^+$$;</li>
<li>The time of termination $$T$$ is a random variable that can vary from episode to episode.</li>
</ul>
<p><strong>Continuing task</strong>: tasks where the agent-environment interaction cannot be naturally broken down into identifiable episodes, and goes on continually without limit.</p>
<ul>
<li>With the current formulation, the final time step for these tasks would be $$T = \infty$$ and thus, the total return we are trying to maximize could easily be infinite;</li>
<li>To deal with this, we add the concept of <em>discounting</em> to the formulation. Now, the agent must try to select actions that maximize the sum of discounted rewards received.</li>
</ul>
<p>Following this formulation, the agent must then chose $$A_t$$ s.t. the expected <em>discounted return</em> is maximal:</p>
<p>$$
\begin{equation}
G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k = 0}^{\infty} \gamma^k \cdot R_{t+k+1},
\end{equation}
$$</p>
<p>where $$\gamma \in [0, 1]$$ is a parameter called the <em>discount rate</em>. This parameter determines the current value of future rewards, i.e., a reward received $$k$$ time steps in future is only worth $$\gamma^{k+1}$$ times what it would be worth if it were received now. If $$\gamma &#x3C; 1$$ and the reward sequence $${R_k}$$ is bounded, then the infinite sum of discounted rewards has a finite value.  If $$\gamma = 0,$$ the agent is only concerned with maximizing its immediate reward, i.e., its objective is to learn how to select $$A_t$$ s.t. it maximizes only $$R_{t+1}.$$ As $$\gamma \rightarrow 1,$$ the return objective takes future rewards more strongly into account, i.e., the agent becomes more farsighted. The returns at successive time steps are related to each other s.t.</p>
<p>$$
\begin{align}
G_t &#x26;\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots \nonumber\
&#x26;= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots) \nonumber\
&#x26;= R_{t+1} + \gamma G_{t+1}.
\end{align}
$$</p>
<p>This works for all time steps $$t &#x3C; T,$$ even if the termination occurs at $$t + 1,$$ provided we define $$G_T = 0$$.
If the reward is a constant $$+1,$$ then the return is</p>
<p>$$
\begin{equation}
G_t = \sum_{k = 0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}.
\end{equation}
$$</p>
<h3>Section 3.4: Unified Notation for Episodic and Continuing Tasks</h3>
<p>To be precise about episodic tasks, instead of considering one long sequence of time steps, we need to consider a series of episodes, where each episode consists of a finite sequence of time steps. As such, we refer to $$S_{t, i}$$ as the state representation at time step $$t$$ of episode $$i$$ (the same for $$A_{t, i}, R_{t, i}, \pi_{t, i}, T_i, \dots$$). In practice however, since we are almost always considering a particular episode or stating a fact that is true for all episodes, we can drop the explicit reference to the episode number.</p>
<p>We can unify the finite sum of terms for the total return in the episodic case and the infinite sum for the total reward in the continuing case by considering episode termination to be the entering of a special <em>absorbing state</em> that transitions only to itself and generates only rewards of zero, as exemplified in Figure 2.</p>
<figure align='center'>
<img alt="Example of an MDP with a absorbing state." src="http://acfpeacekeeper.github.io/github-pages/assets/images/literature/absorbing_state.png" onerror="this.src='http://localhost:4000/assets/images/literature/absorbing_state.png';">
<figcaption>Figure 2: Example of an MDP with an absorbing state.</figcaption>
</figure>
<p>Including the possibility that $$T = \infty \oplus \gamma = 1$$ (where $$\oplus$$ is the XOR operator), we can write</p>
<p>$$
\begin{equation}
G_t \doteq \sum_{k = t + 1}^T \gamma^{k - t - 1} R_k.
\end{equation}
$$</p>
<h3>Section 3.5: Policies and Value Functions</h3>
<p><strong>Value function</strong>: a function of states (or state-action pairs) that estimates how good it it is for the agent to be in a given state, defined in terms of future rewards that can be expected.</p>
<ul>
<li>Since the rewards the agent can expect to receive depend on what actions it takes, value functions are defined w.r.t. a particular way of acting, called a policy.</li>
</ul>
<p><strong>Policy</strong>: a mapping from states to probabilities of selecting each possible action, i.e., if the agent is following policy $$\pi$$ at time $$t,$$ then $$\pi(a \vert s)$$ is the probability that $$A_t = a$$ if $$S_t = s.$$</p>
<ul>
<li>It is a function that defines a probability distribution over $$a \in \mathcal{A}(s)$$ for each $$s \in \mathcal{S}.$$</li>
</ul>
<p>The <em>value function</em> of a state $$s$$ under a policy $$\pi,$$ denoted $$^{\pi}(s),$$ is the expected return when starting in $$s$$ and following $$\pi$$ thereafter. For MDPs, $$v^{\pi}$$ can be formally defined as</p>
<p>$$
\begin{equation}
v^{\pi}(s) \doteq \mathbb{E}<em>{\pi} [G_t \vert S_t = s] = \mathbb{E}</em>{\pi} \bigg[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} \vert S_t = s \bigg], \ \forall s \in \mathcal{S},
\end{equation}
$$</p>
<p>where $$\mathbb{E}_{\pi}[\cdot]$$ denotes the expected value of random variable given that the agent follows policy $$\pi,$$ and $$t$$ is any time step (the value of the terminal state is always zero). The function $$v^{\pi}$$ is called the <em>state-value function for policy $$\pi$$.</em></p>
<p>We can also define the value of taking action $$a$$ in state $$s$$ while following a policy $$\pi,$$ denoted $$q^{\pi}(s, a),$$ as the expected return starting from $$s,$$ taking the action $$a,$$ and following policy $$\pi$$ afterwards:</p>
<p>$$
\begin{equation}
q^{\pi}(s, a) \doteq \mathbb{E}<em>{\pi} [G_t \vert S_t = s, A_t = a] = \mathbb{E}</em>{\pi} \bigg[\sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} \vert S_t = s, A_t =a \bigg].
\end{equation}
$$</p>
<p>The function $$q^{\pi}$$ is called the <em>action-value function for policy $$\pi$$</em>. Both $$q^{\pi}$$ and $$v^{\pi}$$ can be estimated from experience, e.g., using <em>Monte Carlo methods</em>.</p>
<p><strong>Monte Carlo methods</strong>: estimation methods that involve averaging over many random samples of a random variable.</p>
<ul>
<li>For examples, if an agent follows policy $$\pi$$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state's actual value $$v^{\pi}(s),$$ as the number of times that state is encountered approaches infinity.  If separate averages are kept for each action taken in each state, these averages will also converge to the action values $$q^{\pi}(s, a)$$;</li>
<li>Since keeping an average for each state and state-action pair is usually not practical, the agent can instead maintain $$v^{\pi}$$ and $$q^{\pi}$$ as parameterized functions.</li>
</ul>
<p>A fundamental property of commonly used value functions is that they satisfy recursive relationships similar to that of the return. For any policy $$\pi$$ and state $$s,$$ the following consistency condition holds between the value of $$s$$ and the value of its possible successor states:</p>
<p>$$
\begin{align}
v^{\pi}(s) &#x26;\doteq \mathbb{E}<em>{\pi} [G_t \vert S_t = s] \nonumber\
&#x26;= \mathbb{E}</em>{\pi} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \nonumber\
&#x26;= \sum_a \pi(a \vert s) \sum_{s'} \sum_r p(s', r \vert s, a) \bigg[r + \gamma \mathbb{E}<em>{\pi} [G</em>{t+1} \vert S_{t+1} = s']\bigg] \nonumber\
&#x26;= \sum_a \pi(a \vert s) \sum_{s', r} p(s', r \vert s, a) [r + \gamma v^{\pi}(s')], \quad \forall s \in \mathcal{S},
\end{align}
$$</p>
<p>where it is implicit that the actions $$a$$ are taken from the set $$\mathcal{A},$$ that the next state $$s'$$ are taken from the set $$\mathcal{S}$$ (or from $$\mathcal{S}^+$$ in the case of an episodic problem), and that the rewards $$r$$ are taken from the set $$\mathcal{R}.$$ The final expression, which is a sum over all values of the three variables $$a$$, $$s'$$, and $$r$$, can be read as an expected value. For each triple, we compute its probability $$\pi(a \vert s) p(s', r \vert s, a),$$ weight the quantity in brackets by that probability and then sum over all possibilities to get an expected value.</p>
<p>The last equation, called the <em>Bellman equation for $$v^{\pi}$$,</em> expresses a relationship between the value of a state and the values of its successor states (similar to a look-ahead).  The Bellman equation averages over all the possibilities, weighting each by its probability of occurring, and it states that the value of the start state must equal the discounted value of the expected next state, plus the reward expected along the way. The value function $$v^{\pi}$$ is the unique solution to this equation.</p>
<h3>Section 3.6: Optimal Policies and Optimal Value Functions</h3>
<p>Solving a RL task roughly means finding a policy that maximizes the total reward over the long run. For finite MDPs, we can precisely define an <em>optimal policy $$\pi^</em>$$* as follows:</p>
<ul>
<li>Value function define a partial ordering over policies;</li>
<li>A policy $$\pi$$ is defined to be better than or equal to a policy $$\pi'$$, i.e., $$\pi \geq \pi'$$, iff $$v^{\pi}(s) \geq v_{\pi'}(s), \ \forall s \in \mathcal{S}$$;</li>
<li>$$\exists \pi^<em>: \pi^</em> \geq \pi, \ \forall \pi \in \mathcal{S}$$.</li>
</ul>
<p>Although there may be more than one optimal policy, we denote them all by $$\pi^<em>$$. They share the same state-value function, called the <em>optimal state-value function</em>, denoted $$v^</em>$$, and defined as</p>
<p>$$
\begin{equation}
v^*(s) \doteq \max_{\pi} v^{\pi}(s), \quad \forall s \in \mathcal{S}.
\end{equation}
$$</p>
<p>Optimal policies also share the same <em>optimal action-value function</em>, denoted $$q^*$$. For the state-action pair $$(s, a)$$, this function gives the expected return for taking action $$a$$ in state $$s$$ and following an optimal policy afterwards. This function can defined as</p>
<p>$$
\begin{align}
q^<em>(s, a) &#x26;\doteq \max_{\pi} q^{\pi}(s, a), \quad \forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s),\
&#x26;= \mathbb{E}[R_{t+1} + \gamma v^</em>(S_{t+1}) \vert S_t = s, A_t = a].
\end{align}
$$</p>
<h3>Section 3.7: Optimality and Approximation</h3>
<p>Even if we have a complete and accurate model of the environment's dynamics, it is usually not possible to compute an optimal policy by solving the Bellman optimality equation. An important aspect of the problem is the amount of computational power available to the agent, in particular, the amount of computation it can perform in a single time step. Another important constraint is the memory available, specially since a large amount of memory is often required to build up approximations of value functions, policies, and models.</p>
<p><em>tabular case</em>: task with small, finite state sets, which allow one to form approximations using arrays or tables with one entry for each state (or state-action pair), i.e., tabular methods.</p>
<ul>
<li>In tasks that have state sets too large to use tabular methods, we must instead rely on approximation using compact parameterized function representations.</li>
</ul>
<p>The online nature of RL makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states.</p>
<h3>Section 3.8: Summary</h3>
<p>RL is about an <em>agent</em> learning how to behave in order to achieve a goal by interacting with its <em>environment</em> over a sequence of discrete time steps. The specification of their interface defines a particular task:</p>
<ul>
<li><em>Actions</em>: choices made by the agent;</li>
<li><em>States</em>: the basis for making the choices;</li>
<li><em>Rewards</em>: the basis for evaluating the choices.</li>
</ul>
<p>A <em>policy</em> is a stochastic rule by which the agent selects the actions as function of states. The <em>optimal policy</em> is that which best achieves the agent's objective, maximizing the amount of reward it receives over time.</p>
<p>The <em>return</em> is the function of future rewards that the agent seeks to maximize (in expected value). Its definition depends on the nature of the task and whether one wishes to <em>discount</em> delayed reward. The undiscounted formulation is appropriate for <em>episodic tasks</em>, and the discounted formulation is appropriate for tabular <em>continuing tasks</em>.</p>
<p>A policy's <em>value functions</em> ($$v^<em>$$ and $$q^</em>$$) assign to each state, or stateâ€“action pair, the largest expected return achievable by any policy. Whereas the optimal value functions are unique for a given MDP, there can be many optimal policies. In most cases, we seek approximations of the optimal value functions and policies, not their exact values.</p>
<h2>Chapter 4: Dynamic Programming</h2>
<p>In this (and future) chapters, we will usually assume that the environment is a finite MDP. This means that we assume $$|\mathcal{S}| &#x3C; \infty, |\mathcal{A}| &#x3C; \infty, |\mathcal{R}| &#x3C; \infty,$$ and that its dynamics are given by a set of probabilities $$p(s', r \vert s, a), \forall s \in \mathcal{S}, \forall a \in \mathcal{A}, \forall r \in \mathcal{R}, s' \in \mathcal{S}^+$$.</p>
<p>Beyond finite MDPs, Dynamic Programming (DP) ideas can be applied to problems with a continuous state and action spaces, by exact solutions for these types of problems are only possible in special cases.</p>
<p>The key idea of DP (and RL in general) is the use of value functions to both structure and organize the search for good policies. DP algorithms are obtained by turning Bellman equations into assignments, i.e., into update rules for improving approximations of the desired value functions. Remember that we can easily obtain optimal policies if we find the optimal value functions, $$v^* \lor q^*$$, which satisfy the Bellman optimality equations, $$\forall s \in \mathcal{S}, \forall a \in \mathcal{A}(s), s' \in \mathcal{S}^+$$:</p>
<p>$$
\begin{align}
v^<em>(s) &#x26;= \max_a \mathbb{E}[R_{t+1} + \gamma \cdot v^</em>(S_{t+1} \vert S_t = s, A_t = a)] \nonumber\
&#x26;= \max_a \sum_{s', r} p(s' r, \vert s, a) [r + \gamma \cdot v^<em>(s')],\
&#x26;\lor \nonumber\
q^</em>(s, a) &#x26;= \mathbb{E}[R_{t+1} + \gamma \max_{'a} q^<em>(S_{t+1}, a') \vert S_t = s, S_t =a], \nonumber\
&#x26;= \sum_{s', r} p(s', r \vert s, a) [r + \gamma \max_{a'} q^</em>(s', a')].
\end{align}
$$</p>
<h3>Section 4.1: Policy Evaluation (Prediction)</h3>
<p><strong>Policy evaluation:</strong> computing the state-value function $$v^{\pi}$$ for an arbitrary policy $$\pi$$. This is also referred to as the <em>prediction problem</em>. Recall from <a href="#chapter-3-finite-markov-decision-processes">Chapter 3</a> that, $$\forall s \in \mathcal{S}$$,</p>
<p>$$
\begin{align}
v^{\pi}(s) &#x26;\doteq \mathbb{E}<em>{\pi}[G_t \vert S_t = s], \nonumber\
&#x26;= \mathbb{E}</em>{\pi}[R_{t+1} + \gamma G_{t+1} \vert S_t = s], \nonumber\
&#x26;= \mathbb{E}<em>{\pi}[R</em>{t+1} + \gamma v^{\pi} (S_{t+1} \vert S_t = s)], \
&#x26;= \sum_a \pi(a \vert s) \sum_{s', r} p(s', r \vert s, a)[r + \gamma v^{\pi}(s')],
\end{align}
$$</p>
<p>where $$\pi(a\vert s)$$ is the probability of taking action $$a$$ in state $$s$$ under policy $$\pi$$, and the expectations are subscripted by $$\pi$$ to indicate that they are conditional on following policy $$\pi$$. If $$\gamma &#x3C; 1$$ or eventual termination is guaranteed from all states under the policy $$\pi$$, then w.h.t. that $$v^{\pi}$$ exists and is unique.</p>
<p>If the environment's dynamics are completely known, then the previous equation is a system of $$|\mathcal{S}|$$ simultaneous linear equations in $$|\mathcal{S}|$$ unknowns (the $$v^{\pi}(s), s \in \mathcal{S}$$). Consider a sequence of approximate value functions $$v_0, v_1, \dots,$$ each mapping $$S^+ \rightarrow \mathbb{R}$$. The initial approximation $$v_0$$ is chosen arbitrarily (except the terminal state, which, if it exists, must have value $0$), and, $$\forall s \in \mathcal{S},$$ each successive approximation is obtained by using the Bellman equation for $$v^{\pi}$$ as an update rule:</p>
<p>$$
\begin{align}
v_{k+1}(s) &#x26;\doteq \mathbb{E}<em>{\pi}[R</em>{t+1} + \gamma \cdot v_k (S_{t+1}) \vert S_t = s] \nonumber\
&#x26;= \sum_a \pi(a \vert s)\sum_{s', r} p(s', r \vert s,a )[r + \gamma \cdot v_k(s')].
\end{align}
$$</p>
<p>Since the Bellman equation for $$v^{\pi}$$ assures us of the equality in the case that $$v_k = v^{\pi},$$ this is a fixed point for this update rule. Then, when under the same conditions that guarantee the existence of $$v^{\pi},$$ w.h.t. $$\lim_{k \rightarrow \infty} {v_k} = v^{\pi}$$. This algorithm, called <strong>iterative policy evaluation</strong>, produces each successive approximation $$v_{k+1}$$ from $$v_k$$, which we call an <em>expected update</em> (since they are based on an expectation over all possible states), by applying the same operation to each state $$s$$: replacing the old value of $$s$$ with a new value obtained from the old values of the successor states of $$s$$, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated.</p>
<pre><code>def iterative_policy_evaluation(policy, mdp, theta, gamma):
	assert theta > 0;
	
	state_values[len(mdp.states)];
	state_values[len(mdp.states)] = 0;
	for x in range(len(mdp.states) - 1):
		state_values[x] = random_value();
		
	gradient;
	do {
		gradient = 0;
		for s in mdp.states:
			v = state_values[s.id];
			state_values[s.id] = 0;
			for tmp_s in mdp.state:
				for a in mdp.actions:
					state_values[s.id] += policy[s.id][a.id] * mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);
			
			gradient = max(gradient, absolute_value(v - state_values[s.id]));
	} while(gradient >= theta);

	return state_values;
</code></pre>
<h3>Section 4.2: Policy Improvement</h3>
<p>Knowing the value function $$v^{\pi}$$ for an arbitrary deterministic policy $$\pi$$, and given some state $$s$$, how can we determine whether or not we should change the policy to deterministically choose an action $$a \neq \pi(s)$$? One possible way, is to consider selecting $a$ in $s$ and afterwards following the existing policy $$\pi$$. The value of this manner of behaving is given by</p>
<p>$$
\begin{align}
q^{\pi}(s, a) &#x26;\doteq \mathbb{E}[R_{t+1} + \gamma v^{\pi}(S_{t+1} \vert S_t = s, A_t = a)] \nonumber\
&#x26;= \sum_{s', r} p(s', r \vert s, a)[r + \gamma \cdot v^{\pi}(s')].
\end{align}
$$</p>
<p>The key criterion is whether this is greater than or less than $$v^{\pi}$$. If it is greater, i.e., if it is better to select $$a$$ once in $$s$$ and thereafter follow $$\pi$$ than it is to always follow $$\pi$$, then one can expect that it would be better to select $$a$$ ever time $$s$$ is encountered, and that such new policy would be a better one overall. This fact is a special case of a general result called the <em>policy improvement theorem</em>.</p>
<p>Let $$\pi$$ and $$\pi'$$ be any pair of deterministic policies s.t.:</p>
<p>$$
\begin{equation}
q^{\pi}(s, \pi'(s)) \geq v^{\pi}(s), \forall s \in \mathcal{S}.
\end{equation}
$$</p>
<p>Then the policy $$\pi'$$ must be as good as, or better than, $$\pi$$, i.e., it must obtain greater or equal expected return from all states:</p>
<p>$$
\begin{equation}
v^{\pi'}(s) \geq v^{\pi}(s), s \in \mathcal{S}.
\end{equation}
$$</p>
<p>Additionally, if there is strict inequality of $$q^{\pi}(s, \pi'(s))$$ at any state, then there must be strict inequality of $$v^{\pi'}(s)$$ at that state. This theorem applies to the two policies considered: a deterministic policy $$\pi$$, and a changed policy $$\pi$$, identical to $$\pi$$ in everything except $$\pi'(s) = a \neq \pi(s)$$. Since $$v^{\pi'} = v^{\pi}, \forall s' \in \mathcal{S} \setminus {s},$$ if $$q^{\pi}(s, a) > v^{\pi}(s)$$, then the changed policy $$\pi'$$ must be better than $$\pi$$.</p>
<p>In order to understand the idea of the proof behind the policy improvement theorem, we can start from $$q^{\pi}(s, \pi'(s)) \geq v^{\pi}(s)$$, then keep expanding the l.h.s. with the equation for $$q^{\pi}(s, a)$$ and reapplying $$q^{\pi}(s, \pi'(s)) \geq v^{\pi}(s)$$:</p>
<p>$$
\begin{align}
v^{\pi}(s) &#x26;\leq q^{\pi}(s, \pi'(s))\
&#x26;= \mathbb{E}[R_{t+1} + \gamma \cdot v^{\pi}(S_{t=1}) \vert S_t = s, A_t = \pi'(s)] \nonumber\
&#x26;= \mathbb{E}<em>{\pi'}[R</em>{t+1} + \gamma \cdot v^{\pi}(S_{t+1}) \vert S_t = s] \nonumber\
&#x26;\leq \mathbb{E}<em>{\pi'}[R</em>{t+1} + \gamma \cdot q^{\pi}(S_{t+1}, \pi'(S_{t+1})) \vert S_t = s] \nonumber\
&#x26;= \mathbb{E}<em>{\pi'}\bigg[R</em>{t+1} + \gamma \cdot \mathbb{E}[R_{t+2} + \gamma \cdot v^{\pi}(S_{t+2}) \vert S_{t+1}, A_{t+1} = \pi'(S_{t+1})] \vert S_t = s \bigg] \nonumber\
&#x26;\leq \mathbb{E}<em>{\pi'}[R</em>{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \gamma^3 R_{t+4} + \dots \vert S_t = s] \nonumber\
&#x26;\dots \nonumber\
&#x26;= v^{\pi'}(s).
\end{align}
$$</p>
<p>Now knowing how, given a policy policy and its value function, to evaluate a change in the policy at a single, we to extend this result to consider changes at all states - selecting at each state the action that appears best according to $$q^{\pi}(s, a)$$, i.e., to consider the new <em>greedy</em> policy $$\pi'$$, given by</p>
<p>$$
\begin{align}
\pi'(s) &#x26;\doteq \argmax_a \  q^{\pi}(s, a) \nonumber\
&#x26;= \argmax_a \ \mathbb{E}[R_{t+1} + \gamma \cdot v^{\pi}(S_{t+1}) \vert S_t = s, A_t = a] \nonumber\
&#x26;= \argmax_a \sum_{s', r} p(s', r \vert s, a)[r + \gamma \cdot v^{\pi}(s')].
\end{align}
$$</p>
<p>The greedy policy takes the action that looks best according to $$v^{\pi}$$ after a single step of lookahead. Since, by construction, the greedy policy meets the conditions of the policy improvement theorem, we know that it is as good as - or better than - the original policy.</p>
<p><strong>Policy improvement</strong>:  process of making a new policy that improves on an original policy by making it greedy w.r.t. the value function of the original policy.</p>
<p>Supposing that the new greedy policy $$\pi'$$ is as good as, but not better than, the old policy $$\pi$$, then $$v^{\pi}(s) = v^{\pi'}$$, and, $$\forall s \in \mathcal{S}$$, it follows from the previous equation that</p>
<p>$$
\begin{align}
v^{\pi'}(s) &#x26;= \max_a \mathbb{E}[R_{t+1} + \gamma \cdot v^{\pi'}(S_{t+1} \vert S_t = s, A_t = a)] \nonumber\
&#x26;= \max_a \sum_{s', r} p(s', r \vert s, a)[r + \gamma \cdot v^{\pi'}(s')].
\end{align}
$$</p>
<p>As this is the same as the Bellman optimality equation, $$v^{\pi'}$$ must be $$v^*$$, and both $$\pi$$ and $$\pi'$$ must be optimal policies. Thus, policy improvement must give us a strictly better policy except when the original policy is already optimal.</p>
<p>The policy improvement theorem carries through as stated for the stochastic case. Also, if there are any ties in policy improvement steps - i.e., there are several actions at which the maximum is achieved - then we don't need to select a single action from among them, as each maximizing action can be given a portion of the probability of being selected in the new greedy policy (as long as we give zero probability to all submaximal actions).</p>
<h3>Section 4.3: Policy Iteration</h3>
<p>A policy $$\pi$$ can be iteratively improved to yield a better policy, thus leading to a sequence of monotonically improving policies and value functions:</p>
<p>$$
\begin{equation}
\pi_0 \xrightarrow{E} v^{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} v^{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \dots \xrightarrow{I} \pi^* \xrightarrow{E} v^*,
\end{equation}
$$</p>
<p>where $$\xrightarrow{E}$$ denotes a policy <em>evaluation</em> and $$\xrightarrow{I}$$ a policy <em>improvement</em>. Each policy is guaranteed to be a strict improvement over the previous one, unless it is already an optimal policy. Since a finite MDP has a finite number of deterministic policies, this process must converge to an optimal policy and the optimal value function in a finite number of iterations.</p>
<p>This way of finding an optimal policy is called <em>policy iteration</em>. Each policy evaluation is started with the value function for the previous policy, which usually results in a great increase in the algorithm's speed of converge (presumably due to the fact that the value function changes little from one policy to the next).</p>
<pre><code>def policy_evaluation(policy, mdp, state_values, theta, gamma):
	gradient;
	do {
		gradient = 0;
		for s in mdp.states:
			value = state_values[s.id];
			state_values[s.id] = 0;
			for tmp_s in mdp.states:
				state_values[s.id] += mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=argmax(policy[tmp_s.id])) * (mdp.reward + gamma * state_values[tmp_s.id]);
				
			gradient = max(gradient, absolute_value(value - state_values[s.id]))
	} while(gradient >= theta);

	return state_values;

def policy_improvement(policy, mdp, gamma):
	policy_stable = True;
	
	for s in mdp.states:
		old_action = argmax(policy[s.id]);
		for tmp_s in mdp.states:
			policy[s.id] = mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=) * (mdp.reward + gamma * state_values[tmp_s.id]);
		
		if old_action != argmax(policy[s]):
			policy_stable = False;

	return policy, policy_stable;

def policy_iteration(mdp, theta, gamma):
	assert theta > 0;
	
	state_values[len(mdp.states)];
	state_values[len(mdp.states)] = 0;
	policy[len(mdp.states)][len(mdp.actions)];
	for x in range(len(mdp.states) - 1):
		state_values[x] = random_value();
		for y in range(len(mdp.actions)):
			policy[x][y] = random_value();

		// Since a policy is probabilities, its values must sum to 1
		policy[x] = (policy[x] - min(policy[x]) / (max(policy[x]) - min(policy[x]))


	policy_stable = False;
	while (!policy_stable):
		state_values = policy_evaluation(policy, mdp, state_values, theta, gamma)
		policy, policy_stable = policy_improvement(policy, mdp, gamma)

	return policy
</code></pre>
<h3>Section 4.4: Value Iteration</h3>
<p>A drawback of policy iteration is that it may involve several policy evaluations, which can be a protracted computation in itself. If policy evaluation is done iteratively, then convergence to exactly $$v^{\pi}$$ occurs only at the limit. However, we can often truncate policy evaluation, e.g., policy evaluation iterations beyond the first three have no effect on the corresponding greedy policy.</p>
<p>The policy evaluation step of policy iteration can be truncated in several ways without losing the algorithm's convergence guarantees. One special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm, which is called <em>value iteration</em>, can be written as a simple update operation that combines the policy improvement and truncated policy evaluation steps:</p>
<p>$$
\begin{align}
v_{k+1}(s) &#x26;\doteq \max_a \mathbb{E}[R_{t+1} + \gamma \cdot v_k(S_{t+1}) \vert S_t = s, A_t = a] \nonumber\
&#x26;= \max_a \sum_{s', r} p(s', r \vert s, a)[r + \gamma \cdot v_k(s')], \forall s \in \mathcal{S}.
\end{align}
$$</p>
<p>For an arbitrary $$v_0$$, the sequence $${v_k}$$ can be shown to converge to $$v*$$ under the same conditions that guarantee the existence of $$v^*$$. Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule. Also, note how the value iteration update is identical to the policy evaluation update, except that it requires the maximu8m to be taken over all actions.</p>
<pre><code>def value_iteration(mdp, theta, gamma):
	assert theta > 0;

Â  Â  state_values[len(mdp.states)];
Â  Â  state_values[len(mdp.states)] = 0;
Â  Â  policy[len(mdp.states)];
Â  Â  for x in range(len(mdp.states) - 1):
Â  Â  Â  Â  state_values[x] = random_value();
Â  Â  Â  Â  policy[x] = 0;

	gradient;
	do {
		gradient = 0;
		for s in mdp.states:
			old_value = state_values[s.id];
			tmp_value[len(mdp.states)];
			for tmp_s in mdp.state:
				for a in mdp.actions:
					tmp_value[tmp_s.id] += mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);

			state_values[s.id] = max(tmp_value);
			gradient = max(gradient, absolute_value(old_value - state_values[s.id]));
	} while(gradient >= theta);

	for s in mdp.states:
		tmp_value[len(mdp.states)];
		for tmp_s in mdp.state:
			for a in mdp.actions:
				tmp_value[tmp_s.id] += mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);

		policy[s.id] = argmax(tmp_value);

	return policy;
</code></pre>
<p>In each of its sweeps, value iteration combines one sweep of policy evaluation and one of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. Since the max operation is the only difference between these updates, this just means that the max operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs.</p>
<h3>Section 4.5: Asynchronous Dynamic Programming</h3>
<p>A significant drawback to the previously discussed DP methods is the fact that they involve operations over the entire state set of the MDP, i.e., they require sweeps of the state set. As such, a single sweep can become prohibitively expensive, e.g., in the game of backgammon (that has $10^{20}$ states), even if we could perform the value iteration update on a million states per second, it would still take $$1000+$$ years to complete a single sweep.</p>
<p><em>Asynchronous</em> DP algorithm:  in-place iterative DP algorithm that is not organized in terms of systematic sweeps of the state set, i.e., the values of some states may be updated several times before the values of others are updated even once.</p>
<ul>
<li>To convergence correctly, such an algorithm must continue to update the values of all the state, i.e., it can't ignore any state after some point in the computation;</li>
<li>Doesn't need to get locked into any hopelessly long sweep before making progress improving a policy, allowing one to order the updates s.t. value information efficiently propagates from state to state (some ideas for this are introduced in <a href="#chapter-8-planning-and-learning-with-tabula-methods">Chapter 8</a>);</li>
<li>Can be run <em>at the same time that an agent is actually experience the MDP</em>, meaning that the agent's experience can be used to determine which states to update and the value and policy information can guide the agent's decision making.
<ul>
<li>Ex: apply updates to states as the agents visits them, thus focusing the algorithm's updates onto the parts state set parts most relevant to the agent.</li>
</ul>
</li>
</ul>
<p>Example (sweepless DP) algorithm: update the value, in place, of only one state $s_k$ on each step $k$, using the value iteration update.</p>
<ul>
<li>If $$0 \leq \gamma &#x3C; 1$$, asymptotic convergence to $v^*$ is guaranteed given only that all states occur in the sequence $${s_k}$$ an infinite number of times;</li>
<li>It is possible to intermix policy evaluation and value iteration updates to produce a kind of asynchronous truncated policy iteration.</li>
</ul>
<h3>Section 4.6: Generalized Policy Iteration</h3>
<h3>Section 4.6: Generalized Policy Iteration</h3>
<p>Policy iteration consists of two simultaneous and interacting processes:</p>
<ul>
<li>Policy evaluation: makes the value function consistent with the current policy;</li>
<li>Policy improvement: makes the policy greedy w.r.t. the current value function.</li>
</ul>
<p>These two processes need not be alternate and/or completed one after the other, since, as long as both processes continue to update all states, convergence to the optimal value function and (an) optimal policy is guaranteed.</p>
<p><em>Generalized policy iteration</em> (GPI): the general idea of letting policy evaluation and policy improvement processes interact, independent of the details of the two processes. Almost all RL methods can be described as GPI, i.e., they have identifiable policies and value functions, with the policy always being improved w.r.t. the value function, and the value function always being driven toward the value function for the policy.</p>
<p>If both the evaluation process and the improvement process stabilize, then the value function and policy must be optimal. The value function only stabilizes when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. This implies that the Bellman optimality equation, which means that the policy and the value function are guaranteed to be optimal.</p>
<figure align='center'>
	<img alt="Example of a policy iteration process." src="http://acfpeacekeeper.github.io/github-pages/assets/images/literature/example_pi_process.png" onerror="this.src='http://localhost:4000/assets/images/literature/example_pi_process.png';">
	<figcaption>Figure 3: Example of a policy iteration process.</figcaption>
</figure>
<h3>Section 4.7: Efficiency of Dynamic Programming</h3>
<p>While DP methods may not be the most practical solution for very large problems they can be quite efficient when compared with other methods for solving MDPs. If $$n$$ and $$k$$ denote the number of states and actions, respectively, a DP method takes less computational operations than some polynomial function of $$n$$ and $$k$$ to find an optimal policy, even though the total number of (deterministic) policies is $$k^n$$.</p>
<p>Direct search and linear programming methods can also be used to solve MDPs, and in some cases their worst-case convergence guarantees are even better than those of DP methods. However, linear programming methods become impractical at a much smaller number of states than DP methods do (by a factor of about 100).</p>
<p>While DP methods may seem of limited applicability due to the <em>curse of dimensionality</em> (the fact that the number of states often grows exponentially with the number of state variables), in practice, such methods can be used with today's computers to solve MDPs with millions of states. Also, these methods normally converge much faster than their theoretical worst-case runtimes, specially if they are started with good initial value functions or policies. For very large state spaces, <em>asynchronous</em> DP are often preferred.</p>
<h3>Section 4.8: Summary</h3>
<p><em>Policy evaluation</em> refers to the (usually) iterative computation of the value functions for a given policy. In turn, <em>policy improvement</em> refers to the computation of an improved policy given the value function for that policy. Putting these two together forms the backbone of the two most popular DP methods, <em>policy iteration</em> and <em>value iteration</em>.</p>
<p>Classical DP methods involve sweeps of <em>expected update</em> operations over each state of the state set. They are little more than the Bellman operations turned into assignment statements. Convergence occurs once the updates no longer result in changes in value.</p>
<p>Almost all RL methods can be viewed as a form of GPI: one process takes the policy as given and performs some form of policy evaluation, changing the value function to be more like the true value function for the policy; and the other process takes the value function as given (assuming its the policy's value function) and performs some form of policy improvement, changing the policy to make it better.</p>
<p><em>Asynchronous</em> DP methods are iterative methods that updates states in an arbitrary order, possibly stochastic and using out-of-date information.</p>
<p><strong>Bootstrapping</strong>: general idea of updating estimates based on other estimates.</p>
<ul>
<li>DP methods perform this as they update the estimates of the values of states based on the estimates of the values of successor states;</li>
<li>Many RL methods also perform this, even those that - unlike DP methods - do not require a complete and accurate model of the environment.</li>
</ul>
<h2>Chapter 5: Monte Carlo Methods</h2>
<p><strong>Monte Carlo</strong> (MC) method: an estimation method that obtain results by computing the average of repeated random samples. For RL, this means computing the average of complete returns (as opposed to methods that learn from partial returns).</p>
<ul>
<li>Requires only experience, i.e., sample sequences of states, actions, and rewards from an actual or simulated interaction with an environment;</li>
<li>Value estimates and policy changes are only computed when an episode terminates;</li>
<li>To ensure that well-defined returns are available, here we define MC methods only for episodic tasks (i.e., experience is divided into episodes that eventually terminate).</li>
</ul>
<p>Learning from <em>actual</em> experience is a very powerful tool, since it requires no prior knowledge of the environment's dynamics, yet can still achieve optimal behavior. Learning from <em>simulated</em> experience is also powerful, although a model is required to generate sample transitions (unlike in DP, which requires a complete probability distribution of all possible transitions).</p>
<p>MC methods sample and average <em>returns</em> for each state-action pair, similarly to the bandit methods of <a href="#chapter-2-multi-armed-bandits">Chapter 2</a>. The main difference is that there are now multiple different bandit problems which are all interrelated, i.e., the return after taking an action is one state depends on the actions taken in later states of the same episode (making the problem non-stationary from the P.o.V. of the earlier state). We adapt the idea of GPI to handle the non-stationarity, where instead of <em>computing</em> value function from knowledge of the MDP, we <em>learn</em> value functions from sample return with the MDP.</p>
<h3>Section 5.1: Monte Carlo Prediction</h3>
<p>Recall that the value of a state is the expected return - expected cumulative future discounted reward - starting from that state. The idea of estimating the expected return by averaging the returns observed after visits to that state underlies all MC methods. Each occurrence of a state $s$ in an episode is called a <em>visit</em> to $s$, and the first time it is visited in an episode is called the <em>first visit</em> to $s$.</p>
<p>The <em>first-visit</em> MC method estimates $v^{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the <em>every-visit</em> MC method averages the returns following all visits to $s$. By the law of large numbers, both methods converge to $v^{\pi}(s)$ as the number of (first-)visits to $s$ goes to infinity. The two methods are similar, but have slightly different theoretical properties. Every-visit MC extends more naturally to function approximation and eligibility traces.</p>
<pre><code>def first_visit_monte_carlo_prediction(policy, gamma, mdp):
	returns[len(mdp.states)] = [];
	state_values[len(mdp.states)];
Â  Â  state_values[len(mdp.states)] = 0;
Â  Â  for x in range(len(mdp.states) - 1):
Â  Â  Â  Â  state_values[x] = random_value();

	while(True):
		episode = generate_episode_following_policy(policy, mdp);
		G = 0;
		for step in episode[:0:-1]:
			G = gamma * G + step.reward;
			if step.state not in episode[:-1]:
				returns[step.state.id].append(G);
				state_value[step.state.id] = mean(returns)
</code></pre>
<h3>Section 5.2: Monte Carlo Estimation of Action Values</h3>
<p>Without a model, one must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. As such, one the primary goals for MC methods is to estimate $q^*$. The policy evaluation problem for action values is to estimate $q^{\pi}(s, a)$, i.e., the expected return when starting in state $s$, taking action $a$, and thereafter following policy $\pi$.</p>
<p>A stateâ€“action pair $(s, a)$ is said to be visited in an episode if ever the state $s$ is visited and action $a$ is taken in it. The every-visit MC method estimates the value of a stateâ€“action pair as the average of the returns that have followed all the visits to it. In turn, the first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected.</p>
<p>If $\pi$ is a deterministic policy, then in following it one will observe returns only for one of the actions from each state, and with no returns to average, the MC estimates of the other actions will not improve with experience. This is a serious problem, since to alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.</p>
<p>This is the general problem of maintaining <em>exploration</em>, as discussed in <a href="#chapter-2-multi-armed-bandits">Chapter 2</a>. For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by assuming <em>exploring starts</em>, which means that the episodes <em>start in a stateâ€“action pair</em>, and that every pair has a nonzero probability of being selected as the start.</p>
<p>The previous assumption cannot always be relied upon, particularly when learning directly from actual interaction with an environment. The most common alternative approach to assuring that all stateâ€“action pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state.</p>
<h3>Section 5.3: Monte Carlo Control</h3>
<h2>Chapter 6: Temporal-Difference Learning</h2>
<h2>Chapter 7: $$n$$-step Bootstrapping</h2>
<h2>Chapter 8: Planning and Learning with Tabular Methods</h2>
<h1>Part II: Approximate Solution Methods</h1>
<h2>Chapter 9: On-policy Prediction with Approximation</h2>
<h2>Chapter 10: On-policy Control with Approximation</h2>
<h2>Chapter 11: *Off-policy Methods with Approximation</h2>
<h2>Chapter 12: Eligibility Traces</h2>
<h2>Chapter 13: Policy Gradient Methods</h2>
<p><strong>Notation</strong> relevant for this chapter:</p>
<ul>
<li>Policy's parameter vector: $$\theta \in \mathbb{R}^{d'}$$;</li>
<li>Learned value function's weight vector: $$w \in \mathbb{R}^d$$;</li>
<li>Scalar performance measure w.r.t. the policy parameter: $$J(\theta)$$;</li>
<li>Probability that action $$a$$ is taken at time $$t$$, given that the environment is in state $$s$$ at time $$t$$: $$
\pi (a \vert s, \theta) = P (A_t = a|S_t = s,\theta_t = \theta)
$$.</li>
</ul>
<p><strong>Policy gradient</strong> methods seek to learn an approximation to the policy by maximizing performance. Their updates approximate gradient ascent such as:</p>
<p>$$
\begin{equation}
\theta_{t+1} = \theta_t + \alpha \cdot \hat{\nabla J(\theta_t)}, \ \hat{\nabla J(\theta_t)} \in \mathbb{R}^{d'}.
\end{equation}
$$</p>
<p>Methods that learn an approximation of both policy and value functions are called <strong>actor-critic</strong> methods. The <em>actor</em> is a reference to the learned policy and <em>critic</em> a reference to the learned (state-)value function.</p>
<h3>Section 13.1: Policy Approximation and its Advantages</h3>
<p>The policy can be parameterized in any way, as long as 2 conditions are met</p>
<ul>
<li>As long as $$\pi(a\vert s, \theta)$$ is differentiable w.r.t. its parameters:</li>
</ul>
<p>$$
\forall s \in S \ \forall a \in A(s) \ \exists \ \nabla \pi (a|s, \theta) : |\nabla \pi (a|s, \theta)| &#x3C; \infty \ \wedge \theta \in \mathbb{R}^{d'};
$$</p>
<ul>
<li>As long as it continues to perform exploration (to avoid a deterministic policy):</li>
</ul>
<p>$$
\pi (a|s, \theta) \ \in \  ]0, 1[.
$$</p>
<p>For small-to-medium discrete action spaces, it is common to form parameterized numerical preferences $$h(s, a, \theta) \in \mathbb{R}$$ for each $$(s, a)$$ pair. The probabilities of each action being selected can be calculated with, e.g., an exponential softmax distribution:</p>
<p>$$
\begin{equation}
\pi (a|s, \theta) \doteq \frac{\exp(h(s,a,\theta))}{\sum_b \exp(h(s,b,\theta))}.
\end{equation}
$$</p>
<p>This kind of policy parameterization is called <em>softmax in action preferences</em>.</p>
<p>The action preferences can be parameterized arbitrarily, e.g., as the output of a Deep Neural Network (DNN) with parameters $$\theta$$. They can also be linear in features:</p>
<p>$$
\begin{equation}
h(s,a,\theta) = \theta^T \cdot x(s, a), \ x(s, a) \in \mathbb{R}^{d'}.
\end{equation}
$$</p>
<p>The <em>choice of policy parameterization</em> can be a good way of <em>injecting prior knowledge</em> about the desired form of the policy into the RL system. Beyond this, parameterizing policies according to the softmax in action preferences has several advantages:</p>
<ul>
<li>Allows the approximate policy to approach a deterministic policy, unlike with $$\epsilon$$-greedy action selection (due to the $$\epsilon$$ probability of selecting a random action);</li>
<li>Enables the selection of actions with arbitrary probabilities (useful if the optimal policy is a stochastic policy, e.g., in a game of chance such as poker);</li>
<li>If the policy is easier to approximate then the action-value function, then policy-based methods learn faster and yield a superior asymptotic policy.</li>
</ul>
<h3>Section 13.2: The Policy Gradient Theorem</h3>
<p>Policy parameterization has an important theoretical advantage over $$\epsilon$$-greedy action selection: the continuity of the policy dependence on the parameters that enables policy gradient methods to approximate gradient ascent.</p>
<p>Due to the continuous policy parameterization the action probabilities change smoothly as a function of the learned parameters, unlike with $$\epsilon$$-greedy selection, where the action probabilities may change dramatically if an update changes which action has the maximal value.</p>
<p>In the episodic case, the performance measure is defined as the value of the start state of the episode. Taking a (non-random) state $$s_0$$ as the start state of the episode and $$v_{\pi_{\theta}}$$ as the true value function for the policy $$\pi_{\theta}$$, the performance can be defined as:</p>
<p>$$
\begin{equation}
J(\theta) \doteq v_{\pi_{\theta}} (s_0).
\end{equation}
$$</p>
<p>Since the policy parameters affect both the action selections and the distribution of states in which those selections are made, and performance also depends on both, it may be challenging to to change the policy parameters such that improvement is ensured, particularly since the effect of the policy on the state distribution is a function of the environment, and thus is typically unknown.</p>
<p>The <strong>policy gradient theorem</strong> provides a solution towards this challenge in the manner of an analytic expression for the gradient of performance w.r.t. the policy parameters, without the need for the derivative of the state distribution. The equation for episodic case of the policy gradient theorem is given by:</p>
<p>$$
\begin{equation}
\nabla J(\theta) \propto \sum_s \mu (s) \sum_a q^{\pi}(s, a) \cdot \nabla \pi(a|s, \theta),
\end{equation}
$$</p>
<p>where $$\mu$$ is the on-policy distribution under $$\pi$$. For the episodic case, the constant of proportionality is the average length of an episode, and for the continuing case it is 1.</p>
<h3>Section 13.3: REINFORCE: Monte Carlo Policy Gradient</h3>
<p>Since any constant of proportionality can be absorbed into the step size $$\alpha$$, all that is required is a way of sampling that approximates the policy gradient theorem. As the r.h.s. of the theorem is a sum over states weighted by their probability of occurring under the target policy $$\pi$$, w.h.t.:
$$
\begin{align}
\nabla J(\theta) &#x26;\propto \sum_s \sum_a q^{\pi}(s, a) \cdot \nabla \pi (a|s, \theta) \nonumber\
&#x26;= \mathbb{E}_{\pi} [\sum_a q^{\pi}(S_t, a) \cdot \nabla \pi(a|S_t, \theta)].
\end{align}
$$
Thus, we can instantiate the stochastic gradient ascent algorithm (known as the <em>all-actions</em> method, due to its update involving all of the actions) as:</p>
<p>$$
\begin{equation}
\theta_{t + 1} \doteq \theta_t + \alpha \sum_a \hat{q}(S_t, a, w) \cdot \nabla \pi (a|S_t, \theta),
\end{equation}
$$</p>
<p>where $$\hat{q}$$ is a learned approximation of $$q^{\pi}$$.
Unlike the previous method, the update step at time $$t$$ of the <strong>REINFORCE</strong> algorithm involves only $$A_t$$ (the action taken at time $$t$$). By multiplying and then dividing the summed terms by $$\pi (a\vert S_t, \theta)$$, we can introduce the weighting needed for an expectation under $$\pi$$., then, given that $$G_t$$ is the return, w.h.t.:</p>
<p>$$
\begin{align}
\nabla J(\theta) &#x26;\propto \mathbb{E}<em>{\pi} [\sum_a \pi(a|S_t, \theta) \cdot q^{\pi} (S_t,a) \cdot \frac{\nabla \pi(s|S_t, \theta)}{\pi(a|S_t, \theta)}] \nonumber\
&#x26;= \mathbb{E}</em>{\pi} [q^{\pi}(S_t, A_t) \cdot \frac{\nabla \pi(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}] \nonumber\
&#x26;= \mathbb{E}_{\pi} [G_t \cdot \frac{\nabla \pi(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}].
\end{align}
$$</p>
<p>Since the final (in brackets) expression is a quantity that can be sampled on each time step with expectation proportional to the gradient, such a sample can be used to instantiate the generic stochastic gradient ascent algorithm, yielding the REINFORCE update:</p>
<p>$$
\begin{align}
\theta_{t + 1} &#x26;\doteq \theta_t + \alpha \cdot \gamma^t \cdot G_t \cdot \frac{\nabla \pi(A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)} \nonumber\
&#x26;= \theta_t + \alpha \cdot \gamma^t \cdot G_t \cdot \nabla \ln \pi(A_t| S_t, \theta_t),
\end{align}
$$\
where $$\gamma$$ is the discount factor and $$\ln \pi(A_t|S_t, \theta_t)$$ is the <em>eligibility</em> vector. In this update, each increment is proportional to the return - causing the parameters to move most in the directions of the actions that yield the highest return - and is inversely proportional to the action probability - since the most frequent selected actions would have an advantage otherwise.</p>
<p>Since REINFORCE uses the complete return from time $$t$$ (including all future rewards until the end of an episode), it is considered a Monte Carlo algorithm and is only well defined in the episodic case with all updates made in retrospect after the episode's completion.</p>
<p>As a stochastic gradient method, REINFORCE assures an improvement in the expected performance (given a small enough $$\alpha$$) and convergence to a local optimum (under standard stochastic approximation conditions for decreasing $$\alpha$$). However, as a Monte Carlo method, REINFORCE may have high variance and subsequently produce slow learning.</p>
<pre><code>def episodic_reinforce(policy, environment, alpha, gamma):
	assert alpha > 0;
	policy.theta = initialize_policy_parameters();
	while(True):
		episode = environment.generate_episode();
		for step in episode:
			G = sum([R_k * gamma**(k-step.t-1) for k, R_k in enumerate(episode.rewards]));
			policy.theta += alpha * gamma**step.t * G * compute_gradient(log(policy[step.state][step.action]));
</code></pre>
<h3>Section 13.4: REINFORCE with Baseline</h3>
<p>Generalizing the policy gradient theorem to include a comparison of the action value to an arbitrary baseline $$b(s)$$ gives the following expression:</p>
<p>$$
\begin{equation}
\nabla J(\theta) = \sum_s \mu (s) \sum_a (q^{\pi}(s, a) - b(s)) \cdot \nabla \pi (a|s, \theta).
\end{equation}
$$</p>
<p>The baseline can be any arbitrary function (as long as it doesn't vary with $$a$$), since the equation will remain valid as the subtracted quantity is zero:</p>
<p>$$
\begin{equation}
\sum_a b(s)\cdot \nabla \pi(a|s, \theta) = b(s) \cdot \nabla \sum_a \pi(a|s, \theta) = b(s) \cdot \nabla 1 = 0.
\end{equation}
$$</p>
<p>The theorem can then be used to derive an update rule similar to the previous version of REINFORCE, but which includes a general baseline:</p>
<p>$$
\begin{equation}
\theta_{t + 1} \doteq \theta_t + \alpha \cdot \gamma^t \cdot (G_t - b(S_t)) \cdot \frac{\nabla \pi(A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)}.
\end{equation}
$$</p>
<p>While the baseline generally leaves the expected value of the update unchanged, it can have a significant effect on its variance and thus the learning speed.</p>
<p>The value of the baseline should follow that of the actions, i.e., if all actions have high/low values, then the baseline must also have high/low value, so as to differentiate the higher valued actions from the lower valued ones. A common choice for the baseline is an estimate of state value $$\hat{v}(S_t, w)$$, with $$w \in \mathbb{R}^d$$ being a learned weight vector.</p>
<p>The algorithm has two step sizes, $$\alpha_{\theta}$$ (which is the same as the step size $$\alpha$$ in previous equations) and $$\alpha_{w}$$. A good rule of thumb for setting the step size for values in the linear case is</p>
<p>$$
\begin{equation}
\alpha_w = 0.1/\mathbb{E}[||\nabla \hat{v}(S_t, w)]||_{\mu}^2.
\end{equation}
$$</p>
<p>The step size for the policy parameters $$\alpha_{\theta}$$ will depend on the range of variation of the rewards and on the policy parameterization.</p>
<pre><code>def episodic_reinforce_wbaseline(policy, state_value_function, environment, alpha_theta, alpha_w, gamma):
	assert alpha_theta > 0;
	assert alpha_w > 0;
	policy.theta = initialize_policy_parameters();
	state_value_function.w = initialize_state_value_parameters();
	while(True):
		episode = environment.generate_episode();
		for step in episode:
			G = sum([R_k * gamma**(k-step.t-1) for k, R_k in enumerate(episode.rewards]));
			delta = G - state_value_function(step.state);
			state_value_function.w += alpha_w * delta * compute_gradient(state_value_function(step.state));
			policy.theta += alpha_theta * gamma**step.t * delta * compute_gradient(log(policy[step.state][step.action]);)
</code></pre>
<h3>Section 13.5: Actor-Critic Methods</h3>
<p>In actor-critic methods, the state-value function is applied to the second state of the transition, unlike in REINFORCE, where the learned state-value function only estimates the value of the first state of each state transition and thus canÂ´t be used to assess that action. After discount and adding the estimated value of the second state to the reward, it constitutes the 1-step return $$G_{t:t+1}$$, which can be used to assess the action.</p>
<p>Even though the 1-step return introduces bias, it is often superior to the actual return in terms of its variance and computational congeniality. The bias can also be flexibly modulated through $$n$$-step returns and eligibility traces.</p>
<p>1-step actor-critic methods are analogs of the TD methods such as TD(0), Sarsa(0) and Q-learning. Such methods are appealing since they function in fully online and incremental manner, while avoiding the complexities of eligibility traces. In these methods, the full return of REINFORCE is replaced with the 1-step return (with a state-value function as the baseline) as follows:</p>
<p>$$
\begin{align}
\theta_{t+1} &#x26;\doteq \theta_t + \alpha (G_{t:t+1} - \hat{v}(S_t, w)) \frac{\nabla \pi (A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)} \nonumber\
&#x26;= \theta_t + \alpha (R_{t+1} + \gamma \cdot \hat{v}(S_{t+1}, w) - \hat{v}(S_t, w)) \frac{\nabla \pi (A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)} \nonumber\
&#x26;= \theta_t + \alpha \cdot \delta_t \cdot \frac{\nabla \pi (A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)}.
\end{align}
$$</p>
<p>A usual state value function learning method to pair with this is semi-gradient TD(0).
To generalize to the forward view of $$n$$-steps methods and to a $$\lambda$$-return, one only needs to replace the on-step return in the previous equation by $$G_{t:t+1}$$ or $$G_t^{\lambda}$$, respectively. The backward view of the $$\lambda$$-return algorithm is also simple, only requiring using separate eligibility traces for the actor and critic.</p>
<pre><code>def episodic_one_step_actor_critic(policy, state_value_function, environment, alpha_theta, alpha_w, gamma):
	assert alpha_theta > 0;
	assert alpha_w > 0;
	policy.theta = initialize_policy_parameters();
	state_value_function.w = initialize_state_value_parameters();
	while(True):
		s = environment.initialize();
		I = 1;
		while s.not_terminal():
			a = policy[s];
			next_state = environment.get_next_state(s, a);
			if next_state.is_terminal():
				value = 0;
			else:
				value = state_value_function[next_state];
			delta = environment.get_reward(s, a) + gamma * value;
			state_value_function.w += alpha_w * delta * compute_gradient(state_value_function[step.state]);
			policy.theta += alpha_theta * I * delta * compute_gradient(log(policy[step.state][step.action]));
			I = gamma * I;
			s = next_state;

def episodic_actor_critic_with_traces(policy, state_value_function, environment, lambda_theta, lambda_w, alpha_theta, alpha_w, gamma):
	assert 1 >= lambda_theta >= 0;
	assert 1 >= lambda_w >+ 0;
	assert alpha_theta > 0;
	assert alpha_w > 0;
	policy.theta = initialize_policy_parameters();
	state_value_function.w = initialize_state_value_parameters();
	while(True):
		s = environment.initialize();
		z_theta = 0;
		z_w = 0;
		I = 1;
		while s.not_terminal():
			a = policy[s];
			next_state = environment.get_next_state(s, a);
			if next_state.is_terminal():
				value = 0;
			else:
				value = state_value_function[next_state];
			delta = environment.get_reward(s, a) + gamma * value - state_value_function[s];
			z_w = gamma * lambda_w * z_w + compute_gradient(state_value_function[step.state]);
			z_theta = gamma * lambda_theta * z_theta + I * compute_gradient(log(policy[step.state][step.action]));
			state_value_function.w += alpha_w * delta * z_w;
			policy.theta += alpha_theta * delta * z_theta;
			I = gamma * I;
			s = next_state;
</code></pre>
<h3>Section 13.6: Policy Gradient for Continuing Problems</h3>
<p>For continuing problems without episode boundaries, the performance must be defined in terms of the average rate of reward per time step:</p>
<p>$$
\begin{align}
J(\theta) \doteq r(\pi) &#x26;\doteq \lim_{h \rightarrow \infty} \frac{1}{h} \sum_{t = 1}^h \mathbb{E}[R_t \vert S_0, A_{0:t - 1} \sim \pi] \nonumber\
&#x26;= \lim_{t \rightarrow \infty} \mathbb{E}[R_t \vert S_0, A_{0:t - 1} \sim \pi] \nonumber\
&#x26;= \sum_s \mu(s) \sum_a \pi (a \vert s) \sum_{s', r} p(s', r \vert s, a) \cdot r,
\end{align}
$$</p>
<p>where $$\mu$$ is the steady-state distribution under $$\pi$$, $$\mu(s) \doteq \lim_{t \rightarrow \infty} P(S_t = s \vert A_{0:t} \sim \pi)$$,
which is assumed to exist and - due to the ergodicity assumption - to be independent of $$S_0.$$ This is a special distribution under where, if you select actions according to $$\pi$$, you remain in the same distribution, as follows:</p>
<p>$$
\begin{equation}
\sum_s \mu(s) \sum_a \pi(a \vert s, \theta) \cdot p(s' \vert s, a) = \mu(s'), \ \forall s' \in S.
\end{equation}
$$</p>
<p>In the continuing case, we define values: $$v^{\pi} (s) \doteq \mathbb{E}_{\pi} (G_t \vert S_t = s)$$ and $$q^{\pi}(s, a) \doteq \mathbb{E} [G_t \vert S_t = s, A_t = a]$$, w.r.t. the differential return (s.t. the policy gradient theorem holds true for the continuing case):</p>
<p>$$
\begin{equation}
G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \dots \ .
\end{equation}
$$</p>
<pre><code>def continuing_actor_critic_with_traces(policy, state_value_function, environment, lambda_theta, lambda_w, alpha_theta, alpha_w, alpha_R):
	assert 1 >= lambda_theta >= 0;
	assert 1 >= lambda_w >+ 0;
	assert 1 >= lambda_R >+ 0;
	assert alpha_theta > 0;
	assert alpha_w > 0;
	R_bar = random_value();
	policy.theta = initialize_policy_parameters();
	state_value_function.w = initialize_state_value_parameters();
	s = environment.initialize();
	z_theta = 0;
	z_w = 0;
	while(True):
		a = policy[s];
		next_state = environment.get_next_state(s, a);
		delta = environment.get_reward(s, a) - R_bar + state_value_function[next_state] - state_value_function[s];
		R_bar += alpha_R * delta;
		z_w = lambda_w * z_w + compute_gradient(state_value_function[s]);
		z_theta = lambda_theta * z_theta + compute_gradient(log(policy[step.state][step.action]));
		state_value_function.w += alpha_w * delta * z_w;
		policy.theta += alpha_theta * delta * z_theta;
		s = next_state;
</code></pre>
<h3>Section 13.7: Policy Parameterization for Continuous Actions</h3>
<p>In policy-based methods, instead of computing learned probabilities for each and every action, instead we learn statistics of the probability distribution, e.g., the action set might be $$\mathbb{R}$$, with actions chosen from a normal (Gaussian) distribution.
The <em>Probability Density Function</em> (PDF) for this (normal) distribution can be written as</p>
<p>$$
\begin{equation}
p(x) \doteq \frac{1}{\sigma \sqrt{2 \pi}} \exp(-\frac{(x - \mu)^2}{2 \sigma^2}),
\end{equation}
$$</p>
<p>where $$\mu$$ is the mean and $$\sigma$$ the standard deviation of the normal distribution, and $$\pi \approx 3.14159$$.</p>
<p>In order to make a policy parameterization, the policy may be defined as the normal probability density over a real-valued scalar action, with $$\mu$$ and $$\sigma$$ given by the parametric function approximators that depend on the state, i.e.,</p>
<p>$$
\begin{equation}
\pi (a \vert s, \theta) \doteq \frac{1}{\sigma(s, \theta)\sqrt{2 \pi}} \exp \bigg(-\frac{(a - \mu(s, \theta))^2}{2 \sigma(s, \theta)^2} \bigg),
\end{equation}
$$</p>
<p>where $$\mu: \mathcal{S} \times \mathbb{R}^{d'} \rightarrow \mathbb{R}$$ and $$\sigma : \mathcal{S} \times \mathbb{R}^{d'} \rightarrow \mathbb{R}^+$$ are two parameterized function approximators. Now, we just need to give a form for these approximators. To do this, we divide the policy's parameters into two parts, i.e., $$\theta = [\theta_{\mu}, \theta_{\sigma}]$$, one part will be used to approximate $$\mu$$ and the other to approximate $$\sigma > 0$$, the latter of which is better approximated as the exponential of a linear function. Thus</p>
<p>$$
\begin{equation}
\mu (s, \theta) \doteq \theta_{\mu}^T x_{\mu} \wedge \sigma(s, \theta) \doteq \exp \bigg(\theta_{\sigma}^T x_{\sigma}(s) \bigg),
\end{equation}
$$</p>
<p>where $$x_{\mu}(s)$$ and $$x_{\sigma}(s)$$ are state feature vectors. With these additional definitions, all previously described algorithms can be applied to learn to select real-valued actions.</p>
<h3>Section 13.8: Summary</h3>
<p>Prior to this chapter: focus on <em>action-value methods</em> - which are methods that learn action values and then use them to select actions.</p>
<p>During this chapter: describes methods that learn parameterized policies that enable actions to be taken without consulting action-value estimates, with a focus on <em>policy-gradient methods</em> - which are methods that, on each step, update the policy parameter in the direction of an estimate of the gradient of the performance w.r.t. the policy parameter.</p>
<p>Advantages of methods that learn and store a policy parameter:</p>
<ul>
<li>They can learn specific probabilities for taking actions;</li>
<li>They can learn appropriate levels of exploration and approach deterministic policies asymptotically;</li>
<li>They can inherently handle continuous action spaces;</li>
<li>The policy may be simpler to represent parametrically than the value function on some problems;</li>
<li>The <em>policy gradient theorem</em> provides an exact formula (that doesn't involve derivatives of the state distribution) for how performance is affected by the policy parameter.</li>
</ul>
<p>A state-value function baseline reduces the variance of the REINFORCE method without introducing bias. If the state-value function is (also) used to assess the policy's action selections, then it is called a <em>critic</em>, the policy is called an <em>actor</em>, and the overall algorithm is called an <em>actor-critic method</em>. The critic introduces bias into the actorâ€™s gradient estimates, but this is often desirable since it substantially reduces variance (similar to the advantage bootstrapping TD methods have over Monte Carlo methods).</p>
<h1>Part III: Looking Deeper</h1>
<h2>Chapter 14: Psychology</h2>
<h2>Chapter 15: Neuroscience</h2>
<h2>Chapter 16: Applications and Case Studies</h2>
<h2>Chapter 17: Frontiers</h2>
</div></div><footer class="border-t border-slate-200 dark:border-slate-800 pt-8 mt-20 pb-8 text-center md:text-left text-slate-500 text-sm"><div class="flex flex-col md:flex-row justify-between items-center"><p>Â© <!-- -->2025<!-- --> ACF Peacekeeper. All rights reserved.</p><div class="mt-4 md:mt-0 space-x-6 flex"><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">RSS</a><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">Privacy</a><a href="#" class="hover:text-slate-900 dark:hover:text-slate-200 transition-colors">Sitemap</a></div></div></footer></main></div></div><script src="/github-pages/_next/static/chunks/webpack-014c7bd7f964e188.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/github-pages/_next/static/media/793968fa3513f5d6-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/github-pages/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/github-pages/_next/static/css/fc975114e5dc40a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[2846,[],\"\"]\n7:I[4707,[],\"\"]\n9:I[6423,[],\"\"]\na:I[2658,[\"185\",\"static/chunks/app/layout-cdee4d1479ae2af3.js\"],\"default\"]\nc:I[1060,[],\"\"]\n8:[\"slug\",\"2024-10-31-Notes-on-RL-an-Introduction\",\"d\"]\nd:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L4\",null,{\"buildId\":\"y5bEWIEVakTsI3Js7nchQ\",\"assetPrefix\":\"/github-pages\",\"urlParts\":[\"\",\"posts\",\"2024-10-31-Notes-on-RL-an-Introduction\"],\"initialTree\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"2024-10-31-Notes-on-RL-an-Introduction\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"2024-10-31-Notes-on-RL-an-Introduction\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"2024-10-31-Notes-on-RL-an-Introduction\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L5\",\"$L6\",null],null],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$8\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/github-pages/_next/static/css/fc975114e5dc40a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_b0711f __variable_c0ebee\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"style\",null,{\"children\":\"\\n          :root {\\n            --font-sans: var(--font-inter);\\n            --font-display: var(--font-lexend);\\n          }\\n        \"}]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lb\"],\"globalErrorComponent\":\"$c\",\"missingSlots\":\"$Wd\"}]\n"])</script><script>self.__next_f.push([1,"e:T1cec5,"])</script><script>self.__next_f.push([1,"\u003cp\u003e$$\n\\DeclareMathOperator*{\\argmin}{arg,min}\n\\DeclareMathOperator*{\\argmax}{arg,max}\n$$\u003c/p\u003e\n\u003cp\u003eHere are some notes I took when reading the second edition of the \u003ca href=\"http://acfpeacekeeper.github.io/github-pages/assets/docs/literature/books/RLbook2020.pdf\" onerror=\"this.href='http://localhost:4000/assets/docs/literature/books/RLbook2020.pdf'\"\u003eReinforcement Learning: An Introduction\u003c/a\u003e book.\\\nIf you want to get into Reinforcement Learning, or are just interested in Artificial Intelligence in general, I highly recommend that you read this book!\\\nIt does require some mathematical background to read and understand everything (mostly Linear Algebra, Probabilities, Statistics, and some Calculus), but it is overall one of the best - and most exhaustive - introductory books about Reinforcement Learning out there.\u003c/p\u003e\n\u003ch1\u003eChapter Index\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#chapter-1-introduction\"\u003eChapter 1: Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#part-i-tabular-solution-methods\"\u003ePart I: Tabular Solution Methods\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#chapter-2-multi-armed-bandits\"\u003eChapter 2: Multi-armed Bandits\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-21-a--armed-bandit-problem\"\u003eSection 2.1: A k-armed Bandit Problem\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-22-action-value-methods\"\u003eSection 2.2: Action-value Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-23-the-10-armed-test-bed\"\u003eSection 2.3: The 10-armed Test-bed\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-24-incremental-implementation\"\u003eSection 2.4: Incremental Implementation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-25-tracking-a-non-stationary-problem\"\u003eSection 2.5: Tracking a Non-stationary Problem\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-26-optimistic-initial-values\"\u003eSection 2.6: Optimistic Initial Values\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-27-upper-confidence-bound-action-selection\"\u003eSection 2.7: Upper-Confidence-Bound Action Selection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-28-gradient-bandit-algorithms\"\u003eSection 2.8: Gradient Bandit Algorithms\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-29-associative-search-contextual-bandits\"\u003eSection 2.9: Associative Search (Contextual Bandits)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-210-summary\"\u003eSection 2.10: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-3-finite-markov-decision-processes\"\u003eChapter 3: Finite Markov Decision Processes\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-31-the-agent-environment-interface\"\u003eSection 3.1: The Agent-Environment Interface\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-32-goals-and-rewards\"\u003eSection 3.2: Goals and Rewards\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-33-returns-and-episodes\"\u003eSection 3.3: Returns and Episodes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-34-unified-notation-for-episodic-and-continuing-tasks\"\u003eSection 3.4: Unified Notation for Episodic and Continuing Tasks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-35-policies-and-value-functions\"\u003eSection 3.5: Policies and Value Functions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-36-optimal-policies-and-optimal-value-functions\"\u003eSection 3.6: Optimal Policies and Optimal Value Functions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-37-optimality-and-approximation\"\u003eSection 3.7: Optimality and Approximation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-38-summary\"\u003eSection 3.8: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-4-dynamic-programming\"\u003eChapter 4: Dynamic Programming\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-41-policy-evaluation-prediction\"\u003eSection 4.1: Policy Evaluation (Prediction)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-42-policy-improvement\"\u003eSection 4.2: Policy Improvement\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-43-policy-iteration\"\u003eSection 4.3: Policy Iteration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-44-value-iteration\"\u003eSection 4.4: Value Iteration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-45-asynchronous-dynamic-programming\"\u003eSection 4.5: Asynchronous Dynamic Programming\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-46-generalized-policy-iteration\"\u003eSection 4.6: Generalized Policy Iteration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-47-efficiency-of-dynamic-programming\"\u003eSection 4.7: Efficiency of Dynamic Programming\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-48-summary\"\u003eSection 4.8: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-5-monte-carlo-methods\"\u003eChapter 5: Monte Carlo Methods\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-51-monte-carlo-prediction\"\u003eSection 5.1: Monte Carlo Prediction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-52-monte-carlo-estimation-of-action-values\"\u003eSection 5.2: Monte Carlo Estimation of Action Values\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-53-monte-carlo-control\"\u003eSection 5.3: Monte Carlo Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-54-monte-carlo-control-without-exploring-starts\"\u003eSection 5.4: Monte Carlo Control without Exploring Starts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-55-off-policy-prediction-via-importance-sampling\"\u003eSection 5.5: Off-policy Prediction via Importance Sampling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-56-incremental-implementation\"\u003eSection 5.6: Incremental Implementation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-57-off-policy-monte-carlo-control\"\u003eSection 5.7: Off-policy Monte Carlo Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-58-discounting-aware-importance-sampling\"\u003eSection 5.8: *Discounting-aware Importance Sampling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-59-per-decision-importance-sampling\"\u003eSection 5.9: *Per-decision Importance Sampling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-510-summary\"\u003eSection 5.10: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-6-temporal-difference-learning\"\u003eChapter  6: Temporal-Difference Learning\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-61-td-prediction\"\u003eSection 6.1: TD Prediction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-62-advantages-of-td-prediction-methods\"\u003eSection 6.2: Advantages of TD Prediction Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-63-optimality-of-td0\"\u003eSection 6.3: Optimality of TD(0)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-64-sarsa-on-policy-td-control\"\u003eSection 6.4: Sarsa: On-policy TD Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-65-q-learning-off-policy-td-control\"\u003eSection 6.5: Q-learning: Off-policy TD Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-66-expected-sarsa\"\u003eSection 6.6: Expected Sarsa\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-67-maximization-bias-and-double-learning\"\u003eSection 6.7: Maximization Bias and Double Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-68-games-afterstates-and-other-special-cases\"\u003eSection 6.8: Games, Afterstates, and Other Special Cases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-69-summary\"\u003eSection 6.9: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-7--step-bootstrapping\"\u003eChapter 7: n-step Bootstrapping\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-71--step-td-prediction\"\u003eSection 7.1: n-step TD Prediction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-72--step-sarsa\"\u003eSection 7.2: n-step Sarsa\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-73--step-off-policy-learning\"\u003eSection 7.3: n-step Off-policy Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-74-per-decision-methods-with-control-variates\"\u003eSection 7.4: *Per-decision Methods with Control Variates\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-75-off-policy-learning-without-importance-sampling-the--step-tree-backup-algorithm\"\u003eSection 7.5: Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-76-a-unifying-algorithm--step\"\u003eSection 7.6: *A Unifying Algorithm: n-step Q(sigma)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-77-summary\"\u003eSection 7.7: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-8-planning-and-learning-with-tabula-methods\"\u003eChapter 8: Planning and Learning with Tabular Methods\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-81-models-and-planning\"\u003eSection 8.1: Models and Planning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-82-dyna-integrated-planning-acting-and-learning\"\u003eSection 8.2: Dyna: Integrated Planning, Acting, and Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-83-when-the-model-is-wrong\"\u003eSection 8.3: When the Model Is Wrong\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-84-prioritized-sweeping\"\u003eSection 8.4: Prioritized Sweeping\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-85-expected-vs-sample-updates\"\u003eSection 8.5: Expected vs. Sample Updates\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-86-trajectory-sampling\"\u003eSection 8.6: Trajectory Sampling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-87-real-time-dynamic-programming\"\u003eSection 8.7: Real-Time Dynamic Programming\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-88-planning-at-decision-time\"\u003eSection 8.8: Planning at Decision Time\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-89-heuristic-search\"\u003eSection 8.9: Heuristic Search\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-810-rollout-algorithms\"\u003eSection 8.10: Rollout Algorithms\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-811-monte-carlo-tree-search\"\u003eSection 8.11: Monte Carlo Tree Search\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-812-summary-of-the-chapter\"\u003eSection 8.12: Summary of the Chapter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-813-summary-of-part-i-dimensions\"\u003eSection 8.13: Summary of Part I: Dimensions\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#part-ii-approximate-solution-methods\"\u003ePart II: Approximate Solution Methods\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#chapter-9-on-policy-prediction-with-approximation\"\u003eChapter 9: On-policy Prediction with Approximation\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-91-value-function-approximation\"\u003eSection 9.1: Value-function Approximation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-92-the-prediction-objective\"\u003eSection 9.2: The Prediction Objective (VE)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-93-stochastic-gradient-and-semi-gradient-methods\"\u003eSection 9.3: Stochastic-gradient and Semi-gradient Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-94-linear-methods\"\u003eSection 9.4: Linear Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-95-feature-construction-for-linear-methods\"\u003eSection 9.5: Feature Construction for Linear Methods\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-951-polynomials\"\u003eSection 9.5.1: Polynomials\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-952-fourier-basis\"\u003eSection 9.5.2: Fourier Basis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-953-coarse-coding\"\u003eSection 9.5.3: Coarse Coding\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-954-tile-coding\"\u003eSection 9.5.4: Tile Coding\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-955-radial-basis-functions\"\u003eSection 9.5.5: Radial Basis Functions\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-96-selecting-step-size-parameters-manually\"\u003eSection 9.6: Selecting Step-Size Parameters Manually\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-97-non-linear-function-approximation-artificial-neural-networks\"\u003eSection 9.7: Non-linear Function Approximation: Artificial Neural Networks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-98-least-squares-td\"\u003eSection 9.8: Least-Squares TD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-99-memory-based-function-approximation\"\u003eSection 9.9: Memory-based Function Approximation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-910-kernel-based-function-approximation\"\u003eSection 9.10: Kernel-based Function Approximation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-911-looking-deeper-at-on-policy-learning-interests-and-emphasis\"\u003eSection 9.11: Looking Deeper at On-policy Learning: Interests and Emphasis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-912-summary\"\u003eSection 9.12: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-10-on-policy-control-with-approximation\"\u003eChapter 10: On-policy Control with Approximation\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-101-episodic-semi-gradient-control\"\u003eSection 10.1: Episodic Semi-gradient Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-102-semi-gradient--step-sarsa\"\u003eSection 10.2: Semi-gradient n-step Sarsa\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-103-average-reward-a-new-problem-setting-for-continuing-tasks\"\u003eSection 10.3: Average Reward: A New Problem Setting for Continuing Tasks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-104-deprecating-the-discounted-setting\"\u003eSection 10.4: Deprecating the Discounted Setting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-105-differential-semi-gradient--step-sarsa\"\u003eSection 10.5: Differential Semi-gradient n-step Sarsa\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-106-summary\"\u003eSection 10.6: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-11-off-policy-methods-with-approximation\"\u003eChapter 11: *Off-policy Methods with Approximation\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-111-semi-gradient-methods\"\u003eSection 11.1: Semi-gradient Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-112-examples-of-off-policy-divergence\"\u003eSection 11.2: Examples of Off-policy Divergence\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-113-the-deadly-triad\"\u003eSection 11.3: The Deadly Triad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-114-linear-value-function-geometry\"\u003eSection 11.4: Linear Value-function Geometry\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-115-gradient-descent-in-the-bellman-error\"\u003eSection 11.5: Gradient Descent in the Bellman Error\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-116-the-bellman-error-is-not-learnable\"\u003eSection 11.6: The Bellman Error is Not Learnable\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-117-gradient-td-methods\"\u003eSection 11.7: Gradient-TD Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-118-emphatic-td-methods\"\u003eSection 11.8: Emphatic-TD Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-119-reducing-variance\"\u003eSection 11.9: Reducing Variance\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1110-summary\"\u003eSection 11.10: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-12-eligibility-traces\"\u003eChapter 12: Eligibility Traces\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-121-the--return\"\u003eSection 12.1: The lambda-return\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-122-td\"\u003eSection 12.2: TD(lambda)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-123--step-truncated--return-methods\"\u003eSection 12.3: n-step Truncated lambda-return Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-124-redoing-updates-online--return-algorithm\"\u003eSection 12.4: Redoing Updates: Online lambda-return Algorithm\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-125-true-online-td\"\u003eSection 12.5: True Online TD(lambda)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-126-dutch-traces-in-monte-carlo-learning\"\u003eSection 12.6: *Dutch Traces in Monte Carlo Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-127-sarsa\"\u003eSection 12.7: Sarsa(lambda)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-128-variable-and\"\u003eSection 12.8: Variable lambda and gamma\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-129-off-policy-traces-with-control-variates\"\u003eSection 12.9: Off-policy Traces with Control Variates\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1210-watkins-q-to-tree-backup\"\u003eSection 12.10: Watkin's Q(lambda) to Tree-Backup(lambda)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1211-stable-off-policy-methods-with-traces\"\u003eSection 12.11: Stable Off-policy Methods with Traces\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1212-implementation-issues\"\u003eSection 12.12: Implementation Issues\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1213-conclusions\"\u003eSection 12.13: Conclusions\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-13-policy-gradient-methods\"\u003eChapter 13: Policy Gradient Methods\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-131-policy-approximation-and-its-advantages\"\u003eSection 13.1: Policy Approximation and its Advantages\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-132-the-policy-gradient-theorem\"\u003eSection 13.2: The Policy Gradient Theorem\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-133-reinforce-monte-carlo-policy-gradient\"\u003eSection 13.3: REINFORCE: Monte Carlo Policy Gradient\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-134-reinforce-with-baseline\"\u003eSection 13.4: REINFORCE with Baseline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-135-actor-critic-methods\"\u003eSection 13.5: Actor-Critic Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-136-policy-gradient-for-continuing-problems\"\u003eSection 13.6: Policy Gradient for Continuing Problems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-137-policy-parameterization-for-continuous-actions\"\u003eSection 13.7: Policy Parameterization for Continuous Actions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-138-summary\"\u003eSection 13.8: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#part-iii-looking-deeper\"\u003ePart III: Looking Deeper\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#chapter-14-psychology\"\u003eChapter 14: Psychology\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-141-prediction-and-control\"\u003eSection 14.1: Prediction and Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-142-classical-conditioning\"\u003eSection 14.2: Classical Conditioning\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-1421-blocking-and-higher-order-conditioning\"\u003eSection 14.2.1: Blocking and Higher-order Conditioning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1422-the-rescorla-wagner-model\"\u003eSection 14.2.2: The Rescorla-Wagner Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1423-the-td-model\"\u003eSection 14.2.3: The TD Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1424-td-model-simulations\"\u003eSection 14.2.4: TD Model Simulations\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-143-instrumental-conditioning\"\u003eSection 14.3: Instrumental Conditioning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-144-delayed-reinforcement\"\u003eSection 14.4: Delayed Reinforcement\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-145-cognitive-maps\"\u003eSection 14.5: Cognitive Maps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-146-habitual-and-goal-directed-behavior\"\u003eSection 14.6: Habitual and Goal-directed Behavior\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-147-summary\"\u003eSection 14.7: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-15-neuroscience\"\u003eChapter 15: Neuroscience\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-151-neuroscience-basics\"\u003eSection 15.1: Neuroscience Basics\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-152-reward-signals-reinforcement-signals-values-and-prediction-errors\"\u003eSection 15.2: Reward Signals, Reinforcement Signals, Values, and Prediction Errors\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-153-the-reward-prediction-error-hypothesis\"\u003eSection 15.3: The Reward Prediction Error Hypothesis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-154-dopamine\"\u003eSection 15.4: Dopamine\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-155-experimental-support-for-the-reward-prediction-error-hypothesis\"\u003eSection 15.5: Experimental Support for the Reward Prediction Error Hypothesis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-156-td-errordopamine-correspondence\"\u003eSection 15.6: TD Error/Dopamine Correspondence\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-157-neural-actor%E2%80%93critic\"\u003eSection 15.7: Neural Actorâ€“Critic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-158-actor-and-critic-learning-rules\"\u003eSection 15.8: Actor and Critic Learning Rules\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-159-hedonistic-neurons\"\u003eSection 15.9: Hedonistic Neurons\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1510-collective-reinforcement-learning\"\u003eSection 15.10: Collective Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1511-model-based-methods-in-the-brain\"\u003eSection 15.11: Model-based Methods in the Brain\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1512-addiction\"\u003eSection 15.12: Addiction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1513-summary\"\u003eSection 15.13: Summary\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-16-applications-and-case-studies\"\u003eChapter 16: Applications and Case Studies\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-161-td-gammon\"\u003eSection 16.1: TD-Gammon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-162-samuels-checkers-player\"\u003eSection 16.2: Samuel's Checkers Player\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-163-watsons-daily-double-wagering\"\u003eSection 16.3: Watson's Daily-Double Wagering\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-164-optimizing-memory-control\"\u003eSection 16.4: Optimizing Memory Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-165-human-level-video-game-play\"\u003eSection 16.5: Human-level Video Game Play\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-166-mastering-the-game-of-go\"\u003eSection 16.6: Mastering the Game of Go\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-1661-alphago\"\u003eSection 16.6.1: AlphaGo\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1662-alphago-zero\"\u003eSection 16.6.2: AlphaGo Zero\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-167-personalized-web-services\"\u003eSection 16.7: Personalized Web Services\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-168-thermal-soaring\"\u003eSection 16.8: Thermal Soaring\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#chapter-17-frontiers\"\u003eChapter 17: Frontiers\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#section-171-general-value-functions-and-auxiliary-tasks\"\u003eSection 17.1: General Value Functions and Auxiliary Tasks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-172-temporal-abstraction-via-options\"\u003eSection 17.2: Temporal Abstraction via Options\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-173-observations-and-state\"\u003eSection 17.3: Observations and State\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-174-designing-reward-signals\"\u003eSection 17.4: Designing Reward Signals\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-175-remaining-issues\"\u003eSection 17.5: Remaining Issues\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-176-reinforcement-learning-and-the-future-of-artificial-intelligence\"\u003eSection 17.6: Reinforcement Learning and the Future of Artificial Intelligence\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003eChapter 1: Introduction\u003c/h1\u003e\n\u003cp\u003eDef. \u003cstrong\u003eReinforcement Learning (RL)\u003c/strong\u003e: an agent learns how to map situations to actions through \u003cem\u003etrial-and-error\u003c/em\u003e or \u003cem\u003eplanned\u003c/em\u003e interaction with a (possibly) uncertain environment, so as to maximize a numerical reward value (i.e., achieve his goal or goals).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eDelayed reward\u003c/em\u003e is another important characteristic of RL, since any action taken may influence (not only the immediate reward value, but also) any subsequent rewards;\u003c/li\u003e\n\u003cli\u003eRL can be formalized as the optimal control of incompletely-known Markov Decision Processes (MDPs).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBesides RL, other \u003cstrong\u003eMachine Learning (ML)\u003c/strong\u003e paradigms include \u003cem\u003eSupervised Learning\u003c/em\u003e - predicting the correct label, given the corresponding set of features - and \u003cem\u003eUnsupervised Learning\u003c/em\u003e - finding hidden patterns in a collection of unlabeled features.\u003c/p\u003e\n\u003cp\u003eA challenge unique to the RL paradigm is that of the trade-off between \u003cstrong\u003eexploration versus exploitation\u003c/strong\u003e. This challenge arises due to the fact that an agent prefers to take the actions that have previously given the highest rewards (\u003cem\u003eexploitation\u003c/em\u003e), but it must also try out other actions in order to have more knowledge about which actions it should select (\u003cem\u003eexploration\u003c/em\u003e).\u003c/p\u003e\n\u003cp\u003eA RL system has four main elements beyond the interactive \u003cstrong\u003eagent\u003c/strong\u003e and the \u003cstrong\u003eenvironment\u003c/strong\u003e, which are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003epolicy\u003c/strong\u003e $$\\pi_t: s \\rightarrow a$$, which in stochastic cases specifies a probability for each action;\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003ereward\u003c/strong\u003e $$r(s, a)$$, an immediate signal that specifies how good it is for an agent to have chosen a certain action in a given state (may also be stochastic);\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003evalue function\u003c/strong\u003e $$v(s)$$ that specifies the total reward an agent is expected to accumulate in the future if he starts at a given state, i.e., predicted long-term reward;\u003c/li\u003e\n\u003cli\u003eA (optional) \u003cstrong\u003eworld model\u003c/strong\u003e used by model-based methods (opposed to purely trial-and-error model-free methods) for planning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003ePart I: Tabular Solution Methods\u003c/h1\u003e\n\u003ch2\u003eChapter 2: Multi-armed Bandits\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eNon-associative\u003c/em\u003e setting: a problem setting that involves learning to act in only 1 situation.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eAssociative\u003c/em\u003e setting: a problem setting where the best action depends on the situation.\u003c/p\u003e\n\u003ch3\u003eSection 2.1: A $$k$$-armed Bandit Problem\u003c/h3\u003e\n\u003cp\u003eSetting of the $$k$$-armed bandit learning problem (analogous to a slot machine with $$k$$ levers):\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChoose 1 action from among $$k$$ different options;\u003c/li\u003e\n\u003cli\u003eReceive a (numerical) reward from a stationary probability distribution which depends on the action selected;\u003c/li\u003e\n\u003cli\u003eRepeat steps 1 and 2 with the purpose of maximizing the expected total reward over some time period (e.g., 1000 action selections or \u003cem\u003etime steps\u003c/em\u003e).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eValue\u003c/strong\u003e of an action: the expected or mean reward received if that action is selected\u003c/p\u003e\n\u003cp\u003eLetting $$A_t$$ be the action taken at time step $$t$$ and $$R_t$$ the corresponding reward, then the value $$q^{*}(a)$$ of an arbitrary action $$a$$ is given by:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nq^{*} (a) \\doteq \\mathbb{E} [R_t | A_t = a].\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eSince we do not know the true value of each action, we need to estimate them in such a way that the estimates are close to the real values. The estimated value of an action $$a$$ at time step $$t$$ is denoted by $$Q_t (a)$$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGreedy\u003c/strong\u003e action: the action with the highest estimated value at a given time step\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoosing this action equates to the agent \u003cstrong\u003eexploiting\u003c/strong\u003e his current knowledge of the values of the actions;\u003c/li\u003e\n\u003cli\u003eSelecting 1 of the non-greedy actions enables the agent to improve his estimates of the non-greedy action's value, i.e., \u003cstrong\u003eexploration\u003c/strong\u003e;\u003c/li\u003e\n\u003cli\u003eExploitation maximizes the reward on 1 step, but it needs to be intercalated with exploration steps so as to maximize the greater total reward in the long term.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSection 2.2: Action-value Methods\u003c/h3\u003e\n\u003cp\u003eDef. \u003cstrong\u003eAction-value Methods\u003c/strong\u003e: methods used to estimate the values of actions and to use those estimates to select an action to take at a given time step.\u003c/p\u003e\n\u003cp\u003eLetting $$\\mathbb{1}_{predicate}$$ be the random variable which equals $$1$$ if the $$predicate$$ is true and $$0$$ otherwise, the value of an action can be estimated by averaging the rewards received: \u003c!-- TODO: check if equations inside text inside equations need double $ signs--\u003e\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nQ_t (a) \\doteq \\frac{\\text{sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$ taken prior to $t$}} = \\frac{\\sum_{i = 1}^{t - 1} R_i \\cdot \\mathbb{1}\u003cem\u003e{A_i = a}}{\\sum\u003c/em\u003e{i = 1}^{t - 1} \\mathbb{1}_{A_i = a}}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eIf the denominator is zero (action has never been taken), then $$Q_t(a)$$ is defined as an arbitrary default value (e.g., zero). By the law of large numbers, as the denominator goes to infinity, $$Q_t(a)$$ converges to $$q^{*}(a)$$. This is called the \u003cem\u003esample-average\u003c/em\u003e method for estimating action values.\u003c/p\u003e\n\u003cp\u003eThe simplest action selection rule is to always select a greedy action and - if there is more than 1 action with the same highest value - to break ties in some arbitrary way (e.g., randomly). This action selection method can be written as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nA_t = \\argmax_a Q_t (a).\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThis selection method never performs exploration. A simple alternative that does so is to select the greedy action most of the time (probability $$1 - \\epsilon$$) and (with probability $$\\epsilon$$) to randomly select any possible action with equal probability. Methods that use this near-greedy action selection rule are dubbed $$\\epsilon$$-greedy methods.\u003c/p\u003e\n\u003ch3\u003eSection 2.3: The 10-armed Test-bed\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eNon-stationary\u003c/strong\u003e setting: problem setting where the true values of the actions (or the reward probabilities) change over time.\u003c/p\u003e\n\u003cp\u003eGiven a set of 2000 randomly generated $$k$$-armed bandit problems (with $$k = 10$$), for each problem in the set, the action values $$q^{\u003cem\u003e}(a), \\ a = {1, 2, \\dots, 10},$$ were selected from a normal (Gaussian) distribution with $$\\mu = 0, \\  \\sigma^2 = 1$$. When a learning method is applied to this problem selects action $$A_t$$ at time step $$t$$, the actual reward ($$R_t$$) was drawn from a normal distribution with $$\\mu = q^{\u003c/em\u003e}(A_t), \\ \\sigma^2 = 1$$.\u003c/p\u003e\n\u003cp\u003eThe performance of the learning methods is measured as it improves with experience over 1000 time steps of the bandit problem, which makes up a single run. To obtain an accurate measure of the learning algorithms' behavior, 2000 runs are performed and the results for the bandit problems are averaged.\u003c/p\u003e\n\u003cp\u003eA greedy action selection method is compared against 2 $$\\epsilon$$-greedy methods (with $$\\epsilon = 0.01 \\lor \\epsilon = 0.1$$). All methods begin with initial action-value estimates of zero and update these estimates using the sample-average technique.\u003c/p\u003e\n\u003cp\u003eWhile the greedy method improved slightly faster than the other 2, it converged to a reward-per-step of 1, which is lower than the best value of around 1.54 achieved by the $$\\epsilon$$-greedy method (with $$\\epsilon = 0.1$$). The method with $$\\epsilon = 0.1$$ improved faster than the method with $$\\epsilon = 0.01$$, since it explored more earlier. However, the method with $$\\epsilon = 0.01$$ converges to a higher reward-per-step in the long run, since the method with $$\\epsilon = 0.1$$ never selects the optimal action more than 91% of the time.\nIt is possible to perform $$\\epsilon$$ annealing to try to get fast learning at the start combined with convergence to a higher reward average.\u003c/p\u003e\n\u003cp\u003eIt takes more exploration to find the optimal actions in cases with noisy rewards (i.e., high reward variance), meaning that $$\\epsilon$$-greedy methods perform even better in those cases, when compared to the greedy method. Also, although the greedy method is theoretically optimal in the deterministic case (i.e., with $$\\sigma^2 = 0$$), this property does not hold in non-stationary bandit problems, making exploration a necessity even in deterministic settings.\u003c/p\u003e\n\u003ch3\u003eSection 2.4: Incremental Implementation\u003c/h3\u003e\n\u003cp\u003eFor a single action, let $$R_i$$ denote the reward received after the $$i^{th}$$ selection of \u003cem\u003ethis action\u003c/em\u003e and $$Q_n$$ the estimate of its action value after it has been selected $$n - 1$$ times, written as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nQ_n \\doteq \\frac{R_1 + R_2 + \\dots + R_{n - 1}}{n - 1}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eInstead of maintaining a record of all the rewards and performing the computation for the estimated value whenever needed (resulting in the growth of both computational and memory requirements), we can devise incremental formulas to update the averages with a small and constant computation to process each new reward. Given $$Q_n$$ and the $$n^{th}$$ reward $$R_n$$, the new average of all $$n$$ rewards can be computed as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nQ_{n + 1} \u0026#x26;= \\frac{1}{n} \\sum_{i = 1}^n R_i \\nonumber\\\n\u0026#x26;= \\frac{1}{n}(R_n + \\sum_{i = 1}^{n - 1} R_i) \\nonumber\\\n\u0026#x26;= \\frac{1}{n}(R_n + (n - 1) \\cdot \\frac{1}{n - 1} \\cdot \\sum_{i = 1}^{n - 1} R_i) \\nonumber\\\n\u0026#x26;= \\frac{1}{n} (R_n + (n - 1) \\cdot Q_n) \\nonumber\\\n\u0026#x26;= \\frac{1}{n} (R_n + n \\cdot Q_n - Q_n) \\nonumber\\\n\u0026#x26;= Q_n + \\frac{1}{n} [R_n - Q_n], \\ n \u003e 1 \\\nQ_2 \u0026#x26;= R_1, \\ Q_1 \\in \\mathbb{R}.\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eThis implementation only needs memory for $$Q_n$$ and $$n$$, and only performs a small computation for each new reward.\nThe general form of the previous update rule is given by:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nNewEstimate \\leftarrow OldEstimate + StepSize [Target - OldEstimate],\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$[Target - OldEstimate]$$ is an \u003cem\u003eerror\u003c/em\u003e in the estimate, which is reduced by taking a step towards the (possibly noisy) target value.\nThe step-size parameter is generally denoted by $$\\alpha$$ or $$\\alpha_t (a)$$.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef bandit_problem(int k, float epsilon, bandits):\n\tQ_a = [0]*k;\n\tN_a = [0]*k;\n\n\twhile(True):\n\t\tif random_float(0, 1) \u0026#x3C;= epsilon:\n\t\t\tA = random_int(0, k)\n\t\telse:\n\t\t\tA = argmax(Q_a)\n\t\tR = bandits[A].get_reward()\n\t\tN_a[A]++;\n\t\tQ_a[A] += (1/N_a[A]) * (R - Q_a[A]);\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 2.5: Tracking a Non-stationary Problem\u003c/h3\u003e\n\u003cp\u003eWhen reward probabilities change over time, it makes sense to give more weight to recent rewards than to those receive long ago. This can be done by using a constant step-size parameter, e.g., for updating an average $$Q_n$$ of the $$n - 1$$ past rewards w.h.t.:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nQ_{n + 1} \\doteq Q_n + \\alpha [R_n - Q_n],\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere the step-size parameter $$\\alpha \\in \\  ]0, 1]$$ is constant. Given this, $$Q_{n + 1}$$ becomes a weighted average (since the sum of weights = 1) of the past rewards and initial estimate $$Q_1$$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nQ_{n + 1} \u0026#x26;= Q_n + \\alpha [R_n - Q_n] \\nonumber\\\n\u0026#x26;= \\alpha \\cdot R_n + (1- \\alpha) Q_n \\nonumber\\\n\u0026#x26;= \\alpha \\cdot R_n + (1 - \\alpha) [\\alpha \\cdot R_{n - 1} + (1 - \\alpha)Q_{n - 1}] \\nonumber\\\n\u0026#x26;= \\alpha \\cdot R_n + (1 - \\alpha) \\cdot \\alpha \\cdot R_{n - 1} + (1 - \\alpha)^2 Q_{n - 1} \\nonumber\\\n\u0026#x26;= \\alpha \\cdot R_n + (1 - \\alpha) \\cdot \\alpha \\cdot R_{n - 1} + \\dots + (1 - \\alpha)^{n - 1} \\cdot \\alpha \\cdot R_1 + (1 - \\alpha)^n \\cdot Q_1 \\nonumber\\\n\u0026#x26;= (1 - \\alpha)^n \\cdot Q_1 + \\sum_{i = 1}^n \\alpha \\cdot (1 - \\alpha)^{n - i} \\cdot R_i.\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eSince $$1 - \\alpha \u0026#x3C; 1$$, the weight given to $$R_i$$ decreases as the number of intervening rewards increases. Also, the weight decays exponentially in proportion to the exponent on $$1 - \\alpha$$ and, if $$1 - \\alpha = 0$$, the entire weight goes onto the very last reward $$R_n$$. This method is sometimes called an \u003cem\u003eexponential recency-weighted average\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eLetting $$\\alpha_n (a)$$ denote the step-size parameter to process the reward obtained after the $$n^{th}$$ selection of action $$a$$, for the sample-average method, w.h.t. $$\\alpha_n (a) = 1/n$$, whose convergence to the true action values is guaranteed by the law of large numbers. However, convergence is \u003cstrong\u003eNOT\u003c/strong\u003e guaranteed for all choices of the $${\\alpha_n (a)}$$ sequence. Through a result in stochastic approximation theory, we obtain the conditions required to assure convergence with probability 1:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\sum_{n = 1}^{\\infty} \\alpha_n (a) = \\infty \\quad \\land \\quad \\sum_{n = 1}^{\\infty} \\alpha_n^2 (a) \u0026#x3C; \\infty,\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere the first condition is required to guarantee that the steps are big enough to overcome any initial conditions or random fluctuations that would otherwise result in getting stuck at saddle points, and the second condition guarantees that the steps will eventually become small enough to assure convergence.\u003c/p\u003e\n\u003cp\u003eThe second condition is not met for the constant step-size parameter case, i.e., $$\\alpha_n (a) = \\alpha$$. This means that the estimates will never completely converge, which is actually a desirable property for non-stationary problems (the most common type of problem in RL), since the estimates continue to vary in response to the most recently received rewards, accounting for the changes in reward probabilities over time. Also, the sequences of step-size parameters that meet both of the above conditions often lead to slow convergence rates, meaning that these are seldomly used in applications and empirical research.\u003c/p\u003e\n\u003ch3\u003eSection 2.6: Optimistic Initial Values\u003c/h3\u003e\n\u003cp\u003eAll previous methods are somewhat dependent on the initial action-value estimates $$Q_1 (a)$$, i.e., they are \u003cem\u003ebiased\u003c/em\u003e by their initial estimates. This bias decreases over time as various actions are selected. However, while for sample-average methods the bias eventually disappear after all actions have been taken at least once, the bias is permanent for methods with a constant $$\\alpha$$.\u003c/p\u003e\n\u003cp\u003eThis property means that, when using methods with a constant $$\\alpha$$, the user must select the values for the initial estimates, which provides a way to supply some prior knowledge about the expected rewards, at the possible cost of being harder to tune.\u003c/p\u003e\n\u003cp\u003eBy selecting optimistic initial action-values, i.e., $$Q_1 (a) \u003e\u003e R_1 (a), \\forall a$$, the agent will always be disappointed since the rewards will always be far less than the first estimates, regardless of which actions are selected. This encourages exploration, as the agent will select all possible actions before the value estimates converge, even if greedy actions are selected at every single time step.\u003c/p\u003e\n\u003cp\u003eThis technique for encouraging exploration is named \u003cem\u003eoptimistic initial values\u003c/em\u003e and is a simple, yet effective trick when used on stationary problems (e.g., with $$Q_1(a) = 5$$ it outperforms a $$\\epsilon$$-greedy method with $$Q_1(a) = 0$$ and $$\\epsilon = 0.1$$).\u003c/p\u003e\n\u003cp\u003eAn important caveat is that, since the drive for exploration in the previous technique is dependent on the initials conditions and disappears after a certain time, it cannot adequately deal with non-stationary problems, where exploration is always required due to the dynamic nature of the reward probabilities. This drawback is present in all methods that treat the beginning of time as a special event (e.g., the sample-average methods).\u003c/p\u003e\n\u003ch3\u003eSection 2.7: Upper-Confidence-Bound Action Selection\u003c/h3\u003e\n\u003cp\u003eWhile $$\\epsilon$$-greedy methods encourage exploration, they do so equally, without any preference for whether the action selected is nearly greedy or particularly uncertain. However, it is possible to select the non-greedy actions while taking into account both how close their value estimates are to the maximal action-value and the estimation uncertainty. An effective way of doing this is to select actions according to the following equation:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nA_t \\doteq \\argmax_a [Q_t(a) + c \\cdot \\sqrt{\\frac{\\ln t}{N_t(a)}}],\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$N_t(a)$$ denotes the number of times the action $$a$$ has been selected prior to time $$t$$ and the number $$c \u003e 0$$ controls the degree of exploration. If $$N_t(a) = 0$$, then $$a$$ is considered to be a maximizing action.\u003c/p\u003e\n\u003cp\u003eThis \u003cstrong\u003eupper confidence bound (UCB)\u003c/strong\u003e action selection is based on the idea that the square-root term is a measure of the uncertainty or variance of action $$a$$ value's estimate. As such, the max'ed over quantity becomes a sort of upper bound on the possible true value of action $$a$$ with $$c$$ determining the confidence level, and thus the uncertainty is reduced each time the action $$a$$ is selected.\u003c/p\u003e\n\u003cp\u003eThe natural logarithm results in smaller increases over time, meaning that actions with lower value estimates or that have been frequently selected, will be selected with decreasing frequency.\u003c/p\u003e\n\u003cp\u003eUCB often performs better than $$\\epsilon$$-greedy action selection (except in the first $$k$$ steps), but it is harder to extend beyond bandits into the general RL settings. This is due to its difficulties in dealing with more advanced settings, such as non-stationary problems and (function approximation) with large state spaces.\u003c/p\u003e\n\u003ch3\u003eSection 2.8: Gradient Bandit Algorithms\u003c/h3\u003e\n\u003cp\u003eBeyond using action-value estimates to select actions, it is also possible to learn a numerical \u003cem\u003epreference\u003c/em\u003e for each action $$a$$, denoted $$H_t(a) \\in \\mathbb{R}$$, which has no interpretation w.r.t. reward. As such, only the relative preference of 1 action over another is important. The action probabilities are determined according to a softmax distribution as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nPr{A_t = a} \\doteq \\frac{\\exp(H_t(a))}{\\sum_{b = 1}^k \\exp(H_t(b))} \\doteq \\pi_t (a),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\pi_t(a)$$ is the probability of taking action $$a$$ at time $$t$$. All actions have an equal probability of being selected at first (i.e., $$H_1(a) = 0, \\forall a$$).\u003c/p\u003e\n\u003cp\u003eThere exists a natural learning algorithm for softmax action preferences based on the idea of \u003cstrong\u003eStochastic Gradient Ascent (SGA)\u003c/strong\u003e, where, at each time step, after selecting action $$A_t$$, and receiving the reward $$R_t$$, the action preferences are updated as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nH_{t + 1}(A_t) \u0026#x26;\\doteq H_t(A_t) + \\alpha (R_t - \\bar{R_t}) (1 - \\pi_t(A_t)), \u0026#x26;\\text{and}\\\nH_{t + 1}(a) \u0026#x26;\\doteq H_t(a) - \\alpha (R_t \\bar{R_t}) \\pi_t(a), \u0026#x26;\\forall a \\neq A_t,\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\alpha \u003e 0$$ is a step-size parameter and $$\\bar{R}_t \\in \\mathbb{R}$$ - which serves as baseline to compare against the reward - is the average of the rewards up to but not including time $$t$$ (with $$\\bar{R}_1 \\doteq R_1$$). If $$R_t \u003e \\bar{R}_t, \\  t \\neq 1$$, then the probability of taking $$A_t$$ in the future is increased, otherwise, the probability of taking $$A_t$$ is decreased if $$R_t \u0026#x3C; \\bar{R}_t$$. Also, the unselected actions probabilities are updated in the opposite direction.\u003c/p\u003e\n\u003cp\u003eSince only the relative preferences are taken into account, adding an arbitrary constant value to all the action preferences has no effect on the action probabilities. Also, since the reward baseline term instantaneously adapts to new values of the mean, shifting the mean (e.g., $$\\mu_{new} = \\mu_{old} + 4$$) of the distribution (while keeping the unit variance) has no effect on the gradient bandit algorithm. However, omitting the baseline term results in a significantly degraded performance.\u003c/p\u003e\n\u003ch4\u003eThe Bandit Gradient Algorithm as SGA\u003c/h4\u003e\n\u003cp\u003eIn exact \u003cstrong\u003eGradient Ascent (GA)\u003c/strong\u003e, each action preference $$H_t(a)$$ would be incremented in proportion to the increment's effect on performance, given by:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nH_{t + 1} (a) \\doteq H_t(a) + \\alpha \\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t (a)},\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere the measure of performance is the expected reward:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\mathbb{E}[R_t] = \\sum_x \\pi_t (x) \\cdot q^{*} (x),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eand the measure of the increment's effect is the \u003cem\u003epartial derivative\u003c/em\u003e of this performance measure w.r.t. the action preference. Since $$q^{*}(x)$$ is not known, it is impossible to use exact GA. As such, the updates will instead take the form of those used in SGA.\u003c/p\u003e\n\u003cp\u003eThe exact performance gradient can be written as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\frac{\\partial \\mathbb{E} [R_t]}{\\partial H_t(a)} = \\frac{\\partial [\\sum_x \\pi_t(x) \\cdot q^{\u003cem\u003e}(x)]}{\\partial H_t(a)} = \\sum_x q^{\u003c/em\u003e}(x) \\frac{\\partial \\pi_t(x)}{\\partial H_t(a)} = \\sum_x (q^{*}(x) - B_t) \\frac{\\partial \\pi_t(x)}{\\partial H_t(a)},\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere the baseline $$B_t$$ can be any scalar value that doesn't depend on $$x$$. Since the sum of probabilities is always one, the sum of the changes $$\\sum_x \\frac{\\partial \\pi_t (x)}{\\partial H_t (a)} = 0$$, and the baseline can be added without changing the equality.\nWe continue by multiplying each term of the sum by $$\\pi_t(x)/\\pi_t(x)$$, as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)} \u0026#x26;= \\sum_x \\pi_t(x) \\cdot (q^{\u003cem\u003e}(x) - B_t) \\cdot \\frac{\\partial \\pi_t (x)}{\\partial H_t(x)} / \\pi_t(x) \\nonumber\\\n\u0026#x26;= \\mathbb{E} [(q^{\u003c/em\u003e}(A_t) - B_t) \\cdot \\frac{\\partial \\pi_t (A_t)}{\\partial H_t(a)}/\\pi_t(A_t)] \\nonumber\\\n\u0026#x26;= \\mathbb{E}[(R_t - \\bar{R_t}) \\cdot \\frac{\\partial \\pi_t(A_t)}{\\partial H_t(a)}/\\pi_t(A_t)] \\nonumber\\\n\u0026#x26;= \\mathbb{E} [(R_t - \\bar{R}\u003cem\u003et) \\cdot \\pi_t(A_t) \\cdot (\\mathbb{1}\u003c/em\u003e{a = A_t} - \\pi_t(a))/\\pi_t(A_t)] \\nonumber\\\n\u0026#x26;= \\mathbb{E}[(R_t - \\bar{R}\u003cem\u003et) \\cdot (\\mathbb{1}\u003c/em\u003e{a = A_t} - \\pi_t(a))],\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ewhere the chosen baseline is $$B_t = \\bar{R_t}$$ and $$R_t$$ is substituted for $$q^{\u003cem\u003e}(A_t)$$, which is allowed since $$\\mathbb{E}[R_t|A_t] = q^{\u003c/em\u003e}(A_t)$$.\nBy substituting a sample of the expectation above for the performance gradient, w.h.t.:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nH_{t + 1}(a) = H_t (a) + \\alpha \\cdot (R_t - \\bar{R}\u003cem\u003et) \\cdot (\\mathbb{1}\u003c/em\u003e{a = A_t} - \\pi_t(a)), \\quad \\forall a,\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhich is equivalent to the original algorithm.\nBy recalling the standard quotient rule for derivatives\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\frac{\\partial}{\\partial x}[\\frac{f(x)}{g(x)}] = \\frac{\\frac{\\partial f(x)}{\\partial x}g(x) - f(x)\\frac{\\partial g(x)}{\\partial x}}{g(x)^2},\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewe can then write\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\frac{\\partial \\pi_t(x)}{\\partial H_t(a)} \u0026#x26;= \\frac{\\partial}{\\partial H_t(a)} \\pi_t(x) \\nonumber\\\n\u0026#x26;= \\frac{\\partial}{\\partial H_t(a)} [\\frac{\\exp(H_t(x))}{\\sum_{y=1}^k \\exp(H_t(y))}] \\nonumber\\\n\u0026#x26;= \\frac{\\frac{\\partial \\exp(H_t(x))}{\\partial H_t(a)} \\sum_{y=1}^k \\exp(H_t(y)) - \\exp(H_t(x)) \\cdot \\exp (H_t(a))}{(\\sum_{y=1}^k \\exp(H_t(y)))^2} \\nonumber\\\n\u0026#x26;= \\frac{\\mathbb{1}\u003cem\u003e{a=x \\exp(H_t(x))} \\sum\u003c/em\u003e{y=1}^k \\exp(H_t(y)) - \\exp(H_t(x)) \\exp(H_t(a))}{(\\sum_{y=1}^k \\exp(H_t(y)))^2} \\nonumber\\\n\u0026#x26;= \\frac{\\mathbb{1}\u003cem\u003e{a = x \\exp(H_t(x))}}{\\sum\u003c/em\u003e{y=1}^k \\exp(H_t(y))} - \\frac{\\exp(H_t(y)) \\exp(H_t(a))}{(\\sum_{y=1}^k \\exp(H_t(y)))^2} \\nonumber\\\n\u0026#x26;= \\mathbb{1}\u003cem\u003e{a = x} \\pi_t(x) - \\pi_t(x) \\pi_t(a) \\nonumber\\\n\u0026#x26;= \\pi_t(x) (\\mathbb{1}\u003c/em\u003e{a=x} - \\pi_t(a)),\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ethus showing that the expected updated of the gradient bandit algorithm is equivalent to the gradient of the expected reward, making the the algorithm a instance of SGA.\u003c/p\u003e\n\u003ch3\u003eSection 2.9: Associative Search (Contextual Bandits)\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eAssociative search\u003c/em\u003e tasks - which involve learning about which actions are the best through trial-and-error and associating these actions with which situations they work the best in - are often called \u003cem\u003econtextual bandits\u003c/em\u003e. These tasks serve as an intermediate between the $$k$$-armed bandit problem and the full RL problem, since each action affects only the immediate reward - like the former - and also involves learning a policy, like the latter.\u003c/p\u003e\n\u003cp\u003eAn example of an associative task is a one composed of several $$k$$-armed bandit problems, each identified by a given color, where at each step you are confronted with one of the $$k$$-armed bandit problems at random. If the action values change as the color changes, you can then learn a policy that maps a color to the best associated actions.\u003c/p\u003e\n\u003ch3\u003eSection 2.10: Summary\u003c/h3\u003e\n\u003cp\u003eW.r.t. performance (average reward) in the $$k$$-bandit problem, with $$k = 10$$ and taking into account the first 1000 steps, w.h.t. UCB $$\\geq$$ Greedy with optimistic initialization $$\\alpha = 0.1 \\ \\geq$$ Gradient bandit $$\\geq$$ $$\\epsilon$$-greedy.\u003c/p\u003e\n\u003cp\u003eAnother approach to balance exploration and exploitation in $$k$$-armed bandit problems is the Bayesian method known as \u003cem\u003eGittins\u003c/em\u003e index. It assumes a known prior distribution over the actions values and then updates the distribution after each step (assuming that the true action values are stationary).\u003c/p\u003e\n\u003ch2\u003eChapter 3: Finite Markov Decision Processes\u003c/h2\u003e\n\u003cp\u003eMarkov Decision Processes (MDPs) are a formalization of sequential decision making where actions influence not only the immediate reward, but also future rewards. As such, this is an associative problem that takes into account the need to trade-off immediate and delayed reward. While in bandit problems we estimated the value $$q^{\u003cem\u003e}(a), \\ \\forall a \\in \\mathcal{A},$$ in an MDP, we estimate the value $$q^{\u003c/em\u003e}(s, a), \\ \\forall a \\in \\mathcal{A}, \\forall s \\in \\mathcal{S},$$ or the value $$v^{*}(s), \\forall s \\in \\mathcal{S}$$ given optimal action selections. Such state-dependent values are important to assign credit for long-term rewards to individual action selections.\u003c/p\u003e\n\u003ch3\u003eSection 3.1: The Agent-Environment Interface\u003c/h3\u003e\n\u003cp\u003eAn MDP involves a learner and decision maker (i.e., the \u003cem\u003eagent\u003c/em\u003e) that interacts with its surroundings (i.e., the \u003cem\u003eenvironment\u003c/em\u003e) by continually selecting actions and having the environment respond by presenting new situations (or states) to the agent and giving rise to rewards, which the agent seeks to maximize over time. This process is illustrated in Figure 1.\u003c/p\u003e\n\u003cfigure align='center'\u003e\n    \u003cimg alt=\"The agent-environment interaction in a Markov decision process.\" src=\"http://acfpeacekeeper.github.io/github-pages/assets/images/literature/rl_mdp.png\" onerror=\"this.src='http://localhost:4000/assets/images/literature/rl_mdp.png';\"\u003e\n\t\u003cfigcaption\u003eFigure 1: The agent-environment interaction in a MDP.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe agent and environment interact with each other in a sequence of discrete time steps $$t = 0, 1, \\dots, n.$$ At each time step $$t$$, the agent receives a representation of the environment's \u003cem\u003estate\u003c/em\u003e $$S_t \\in \\mathcal{S},$$ and on that basis selects an \u003cem\u003eaction\u003c/em\u003e $$A_t \\in \\mathcal{A}(s).$$ At the next time step $$t + 1$$, the agent receives a reward $$R_{t + 1} \\in \\mathcal{R} \\subset \\mathbb{R}$$ and finds itself in another state $$s_{t+1}.$$  Then, the MDP and agent give rise to a sequence or \u003cem\u003etrajectory\u003c/em\u003e like this:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nS_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eIn a \u003cem\u003efinite\u003c/em\u003e MDP, the random variables $$R_t$$ and $$S_t$$ have well defined discrete probability distributions that depend only on the previous state and action, i.e., for particular values of these random variables $$s' \\in \\mathcal{A}, \\ r \\in \\mathcal{R},$$ there is a probability of those values occurring at time step $$t,$$ given particular values of the previous state and action:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\np(s', r \\vert s, a) \\doteq P(S_t = s', R_t \\vert S_{t-1} = s, A_{t-1} = a), \\ \\forall s', s \\in \\mathcal{S}, \\forall r \\in \\mathcal{R}, \\forall a \\in \\mathcal{A}(s).\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe function $$p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$$, which completely characterizes the MDP environment's \u003cem\u003edynamics\u003c/em\u003e, is an ordinary deterministic function with four arguments. This function $$p$$ specifies a probability distribution for each choice of $$s$$ and $$a$$, i.e.,\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\vert s, a) = 1, \\ \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}(s).\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMarkov property\u003c/strong\u003e: the state must include information about all aspects of the past agent-environment interaction that make a difference for the future. In practice, this means that the probability of each possible value for $$S_t$$ and $$R_t$$ depends only on the previous state $$S_{t-1}$$ and action $$A_{t-1}$$.\n- While most methods in this book assume this property to be true, there are methods that don't rely on it. \u003ca href=\"#chapter-17-frontiers\"\u003eChapter 17\u003c/a\u003e considers how to efficiently learn a Markov state from non-Markov observations.\u003c/p\u003e\n\u003cp\u003eFrom the dynamics function $$p,$$ we can compute the \u003cem\u003estate-transition probabilities\u003c/em\u003e $$p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1],$$\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\np(s' \\vert s, a) \\doteq P(S_t = s' \\vert S_{t-1} = s, A_{t-1} = a) = \\sum_{r \\in \\mathcal{R}} p(s', r \\vert s, a).\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eWe can also compute the expected rewards for state-action pairs as a two-argument function $$r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nr(s, a) \\doteq \\mathbb{E} [R_t \\vert S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r \\vert s, a),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eand the expected rewards for the state-action-new_state triples as a three-argument function $$r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R},$$\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nr(s, a, s') \\doteq \\mathbb{E} [R_t \\vert S_{t-1} = s, A_{t-1} = a, S_t = s'] = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s', r \\vert s, a)}{p(s' \\vert s, a)}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe MDP framework is flexible and can be applied to problems in many different ways, e.g., time steps don't need to refer to fixed intervals of real time; they can refer to arbitrary sequential stages of decision making and acting. In general, actions can be any decision we want to learn how to make, and states can be anything we can know that might useful in making them.\u003c/p\u003e\n\u003cp\u003eThe boundary between agent and environment is usually drawn closer to the agent than the physical boundary of a robot or an animal's body, e.g., the motors and mechanical linkages of a robot and its sensing hardware should normally be considered parts of the environment, rather than of the agent. In a similar sense, rewards, which are usually computed inside the physical bodies of natural and artificial learning systems, are considered external to the agent. In general, anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.\u003c/p\u003e\n\u003cp\u003eWe don't assume that everything in the environment is unknown to the agent, e.g., the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. However, the reward computation is considered to be external to the agent, because it defines the task facing the agent and must thus be beyond its ability to arbitrarily change.\u003c/p\u003e\n\u003cp\u003eThe MDP framework proposes that any detail of whatever objective (of a problem of learning goal-directed behavior) one is trying to achieve can be reduced to three signal passing back and forth between an agent and its environment:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAction: signal that represents the choices made by the agent;\u003c/li\u003e\n\u003cli\u003eState: signal that represents the basis on which the choices are made;\u003c/li\u003e\n\u003cli\u003eReward: signal that defines the agent's goal.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSection 3.2: Goals and Rewards\u003c/h3\u003e\n\u003cp\u003eIn RL, the purpose of the agent is formalized in terms of a simple number, the \u003cem\u003ereward\u003c/em\u003e (at each time step $$R_t \\in \\mathbb{R}$$), which passes from the environment to the agent. The agent's goal is to maximize the total cumulative reward, something stated in the \u003cem\u003ereward hypothesis\u003c/em\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eThat all of what we mean by goals and purposes can be well thought of as\nthe maximization of the expected value of the cumulative sum of a received\nscalar signal (called reward).\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn order for the agent to achieve our goals, it is critical that the reward signals defined truly indicate what we want accomplished. Of particular importance, one must not use the reward signal to impart prior knowledge to the agent. Using chess as an example, we should naturally define the reward as $$+1$$ for winning, $$-1$$ for losing and $$0$$ for draws and all non-terminal positions. We should \u003cstrong\u003eNOT\u003c/strong\u003e give rewards for sub-goals like taking an opponent's chess piece, otherwise the agent might find a way to maximize its reward, even at the cost of losing the game. The reward signal defines \u003cstrong\u003ewhat\u003c/strong\u003e you want the agent to achieve, not \u003cem\u003ehow\u003c/em\u003e you want the agent to achieve it.\u003c/p\u003e\n\u003ch3\u003eSection 3.3: Returns and Episodes\u003c/h3\u003e\n\u003cp\u003eWe seek to maximize the \u003cem\u003eexpected return\u003c/em\u003e, where the return $$G_t$$ is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nG_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_T,\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$T$$ is a final time step. This approach makes sense in applications where the agent-environment interaction can be naturally broken down into sub-sequences, called \u003cstrong\u003eepisodes,\u003c/strong\u003e like the plays of a game or trips to a maze. Each episode ends in a special state, called the \u003cem\u003eterminal\u003c/em\u003e state, and the resets to a starting state or a sample from standard distribution of starting states. Since the next episode begins independently of how the previous one ended, episodes can all be considered to end in the same terminal state, just with different rewards for different outcomes.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEpisodic task\u003c/strong\u003e: tasks that have discrete independent episodes, i.e., where the outcome of the ending of an episode doesn't effect the start state of the next episode.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe sometimes distinguish the set of all non-terminal states $$\\mathcal{S}$$ from the set of all states plus the terminal state $$\\mathcal{S}^+$$;\u003c/li\u003e\n\u003cli\u003eThe time of termination $$T$$ is a random variable that can vary from episode to episode.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eContinuing task\u003c/strong\u003e: tasks where the agent-environment interaction cannot be naturally broken down into identifiable episodes, and goes on continually without limit.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWith the current formulation, the final time step for these tasks would be $$T = \\infty$$ and thus, the total return we are trying to maximize could easily be infinite;\u003c/li\u003e\n\u003cli\u003eTo deal with this, we add the concept of \u003cem\u003ediscounting\u003c/em\u003e to the formulation. Now, the agent must try to select actions that maximize the sum of discounted rewards received.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFollowing this formulation, the agent must then chose $$A_t$$ s.t. the expected \u003cem\u003ediscounted return\u003c/em\u003e is maximal:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nG_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k = 0}^{\\infty} \\gamma^k \\cdot R_{t+k+1},\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\gamma \\in [0, 1]$$ is a parameter called the \u003cem\u003ediscount rate\u003c/em\u003e. This parameter determines the current value of future rewards, i.e., a reward received $$k$$ time steps in future is only worth $$\\gamma^{k+1}$$ times what it would be worth if it were received now. If $$\\gamma \u0026#x3C; 1$$ and the reward sequence $${R_k}$$ is bounded, then the infinite sum of discounted rewards has a finite value.  If $$\\gamma = 0,$$ the agent is only concerned with maximizing its immediate reward, i.e., its objective is to learn how to select $$A_t$$ s.t. it maximizes only $$R_{t+1}.$$ As $$\\gamma \\rightarrow 1,$$ the return objective takes future rewards more strongly into account, i.e., the agent becomes more farsighted. The returns at successive time steps are related to each other s.t.\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nG_t \u0026#x26;\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots \\nonumber\\\n\u0026#x26;= R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\dots) \\nonumber\\\n\u0026#x26;= R_{t+1} + \\gamma G_{t+1}.\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eThis works for all time steps $$t \u0026#x3C; T,$$ even if the termination occurs at $$t + 1,$$ provided we define $$G_T = 0$$.\nIf the reward is a constant $$+1,$$ then the return is\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nG_t = \\sum_{k = 0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003ch3\u003eSection 3.4: Unified Notation for Episodic and Continuing Tasks\u003c/h3\u003e\n\u003cp\u003eTo be precise about episodic tasks, instead of considering one long sequence of time steps, we need to consider a series of episodes, where each episode consists of a finite sequence of time steps. As such, we refer to $$S_{t, i}$$ as the state representation at time step $$t$$ of episode $$i$$ (the same for $$A_{t, i}, R_{t, i}, \\pi_{t, i}, T_i, \\dots$$). In practice however, since we are almost always considering a particular episode or stating a fact that is true for all episodes, we can drop the explicit reference to the episode number.\u003c/p\u003e\n\u003cp\u003eWe can unify the finite sum of terms for the total return in the episodic case and the infinite sum for the total reward in the continuing case by considering episode termination to be the entering of a special \u003cem\u003eabsorbing state\u003c/em\u003e that transitions only to itself and generates only rewards of zero, as exemplified in Figure 2.\u003c/p\u003e\n\u003cfigure align='center'\u003e\n\u003cimg alt=\"Example of an MDP with a absorbing state.\" src=\"http://acfpeacekeeper.github.io/github-pages/assets/images/literature/absorbing_state.png\" onerror=\"this.src='http://localhost:4000/assets/images/literature/absorbing_state.png';\"\u003e\n\u003cfigcaption\u003eFigure 2: Example of an MDP with an absorbing state.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIncluding the possibility that $$T = \\infty \\oplus \\gamma = 1$$ (where $$\\oplus$$ is the XOR operator), we can write\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nG_t \\doteq \\sum_{k = t + 1}^T \\gamma^{k - t - 1} R_k.\n\\end{equation}\n$$\u003c/p\u003e\n\u003ch3\u003eSection 3.5: Policies and Value Functions\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eValue function\u003c/strong\u003e: a function of states (or state-action pairs) that estimates how good it it is for the agent to be in a given state, defined in terms of future rewards that can be expected.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSince the rewards the agent can expect to receive depend on what actions it takes, value functions are defined w.r.t. a particular way of acting, called a policy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePolicy\u003c/strong\u003e: a mapping from states to probabilities of selecting each possible action, i.e., if the agent is following policy $$\\pi$$ at time $$t,$$ then $$\\pi(a \\vert s)$$ is the probability that $$A_t = a$$ if $$S_t = s.$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt is a function that defines a probability distribution over $$a \\in \\mathcal{A}(s)$$ for each $$s \\in \\mathcal{S}.$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cem\u003evalue function\u003c/em\u003e of a state $$s$$ under a policy $$\\pi,$$ denoted $$^{\\pi}(s),$$ is the expected return when starting in $$s$$ and following $$\\pi$$ thereafter. For MDPs, $$v^{\\pi}$$ can be formally defined as\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nv^{\\pi}(s) \\doteq \\mathbb{E}\u003cem\u003e{\\pi} [G_t \\vert S_t = s] = \\mathbb{E}\u003c/em\u003e{\\pi} \\bigg[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t + k + 1} \\vert S_t = s \\bigg], \\ \\forall s \\in \\mathcal{S},\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\mathbb{E}_{\\pi}[\\cdot]$$ denotes the expected value of random variable given that the agent follows policy $$\\pi,$$ and $$t$$ is any time step (the value of the terminal state is always zero). The function $$v^{\\pi}$$ is called the \u003cem\u003estate-value function for policy $$\\pi$$.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe can also define the value of taking action $$a$$ in state $$s$$ while following a policy $$\\pi,$$ denoted $$q^{\\pi}(s, a),$$ as the expected return starting from $$s,$$ taking the action $$a,$$ and following policy $$\\pi$$ afterwards:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nq^{\\pi}(s, a) \\doteq \\mathbb{E}\u003cem\u003e{\\pi} [G_t \\vert S_t = s, A_t = a] = \\mathbb{E}\u003c/em\u003e{\\pi} \\bigg[\\sum_{k=0}^{\\infty} \\gamma^k R_{t + k + 1} \\vert S_t = s, A_t =a \\bigg].\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe function $$q^{\\pi}$$ is called the \u003cem\u003eaction-value function for policy $$\\pi$$\u003c/em\u003e. Both $$q^{\\pi}$$ and $$v^{\\pi}$$ can be estimated from experience, e.g., using \u003cem\u003eMonte Carlo methods\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMonte Carlo methods\u003c/strong\u003e: estimation methods that involve averaging over many random samples of a random variable.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor examples, if an agent follows policy $$\\pi$$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state's actual value $$v^{\\pi}(s),$$ as the number of times that state is encountered approaches infinity.  If separate averages are kept for each action taken in each state, these averages will also converge to the action values $$q^{\\pi}(s, a)$$;\u003c/li\u003e\n\u003cli\u003eSince keeping an average for each state and state-action pair is usually not practical, the agent can instead maintain $$v^{\\pi}$$ and $$q^{\\pi}$$ as parameterized functions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA fundamental property of commonly used value functions is that they satisfy recursive relationships similar to that of the return. For any policy $$\\pi$$ and state $$s,$$ the following consistency condition holds between the value of $$s$$ and the value of its possible successor states:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv^{\\pi}(s) \u0026#x26;\\doteq \\mathbb{E}\u003cem\u003e{\\pi} [G_t \\vert S_t = s] \\nonumber\\\n\u0026#x26;= \\mathbb{E}\u003c/em\u003e{\\pi} [R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\nonumber\\\n\u0026#x26;= \\sum_a \\pi(a \\vert s) \\sum_{s'} \\sum_r p(s', r \\vert s, a) \\bigg[r + \\gamma \\mathbb{E}\u003cem\u003e{\\pi} [G\u003c/em\u003e{t+1} \\vert S_{t+1} = s']\\bigg] \\nonumber\\\n\u0026#x26;= \\sum_a \\pi(a \\vert s) \\sum_{s', r} p(s', r \\vert s, a) [r + \\gamma v^{\\pi}(s')], \\quad \\forall s \\in \\mathcal{S},\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ewhere it is implicit that the actions $$a$$ are taken from the set $$\\mathcal{A},$$ that the next state $$s'$$ are taken from the set $$\\mathcal{S}$$ (or from $$\\mathcal{S}^+$$ in the case of an episodic problem), and that the rewards $$r$$ are taken from the set $$\\mathcal{R}.$$ The final expression, which is a sum over all values of the three variables $$a$$, $$s'$$, and $$r$$, can be read as an expected value. For each triple, we compute its probability $$\\pi(a \\vert s) p(s', r \\vert s, a),$$ weight the quantity in brackets by that probability and then sum over all possibilities to get an expected value.\u003c/p\u003e\n\u003cp\u003eThe last equation, called the \u003cem\u003eBellman equation for $$v^{\\pi}$$,\u003c/em\u003e expresses a relationship between the value of a state and the values of its successor states (similar to a look-ahead).  The Bellman equation averages over all the possibilities, weighting each by its probability of occurring, and it states that the value of the start state must equal the discounted value of the expected next state, plus the reward expected along the way. The value function $$v^{\\pi}$$ is the unique solution to this equation.\u003c/p\u003e\n\u003ch3\u003eSection 3.6: Optimal Policies and Optimal Value Functions\u003c/h3\u003e\n\u003cp\u003eSolving a RL task roughly means finding a policy that maximizes the total reward over the long run. For finite MDPs, we can precisely define an \u003cem\u003eoptimal policy $$\\pi^\u003c/em\u003e$$* as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eValue function define a partial ordering over policies;\u003c/li\u003e\n\u003cli\u003eA policy $$\\pi$$ is defined to be better than or equal to a policy $$\\pi'$$, i.e., $$\\pi \\geq \\pi'$$, iff $$v^{\\pi}(s) \\geq v_{\\pi'}(s), \\ \\forall s \\in \\mathcal{S}$$;\u003c/li\u003e\n\u003cli\u003e$$\\exists \\pi^\u003cem\u003e: \\pi^\u003c/em\u003e \\geq \\pi, \\ \\forall \\pi \\in \\mathcal{S}$$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlthough there may be more than one optimal policy, we denote them all by $$\\pi^\u003cem\u003e$$. They share the same state-value function, called the \u003cem\u003eoptimal state-value function\u003c/em\u003e, denoted $$v^\u003c/em\u003e$$, and defined as\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nv^*(s) \\doteq \\max_{\\pi} v^{\\pi}(s), \\quad \\forall s \\in \\mathcal{S}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eOptimal policies also share the same \u003cem\u003eoptimal action-value function\u003c/em\u003e, denoted $$q^*$$. For the state-action pair $$(s, a)$$, this function gives the expected return for taking action $$a$$ in state $$s$$ and following an optimal policy afterwards. This function can defined as\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nq^\u003cem\u003e(s, a) \u0026#x26;\\doteq \\max_{\\pi} q^{\\pi}(s, a), \\quad \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}(s),\\\n\u0026#x26;= \\mathbb{E}[R_{t+1} + \\gamma v^\u003c/em\u003e(S_{t+1}) \\vert S_t = s, A_t = a].\n\\end{align}\n$$\u003c/p\u003e\n\u003ch3\u003eSection 3.7: Optimality and Approximation\u003c/h3\u003e\n\u003cp\u003eEven if we have a complete and accurate model of the environment's dynamics, it is usually not possible to compute an optimal policy by solving the Bellman optimality equation. An important aspect of the problem is the amount of computational power available to the agent, in particular, the amount of computation it can perform in a single time step. Another important constraint is the memory available, specially since a large amount of memory is often required to build up approximations of value functions, policies, and models.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003etabular case\u003c/em\u003e: task with small, finite state sets, which allow one to form approximations using arrays or tables with one entry for each state (or state-action pair), i.e., tabular methods.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn tasks that have state sets too large to use tabular methods, we must instead rely on approximation using compact parameterized function representations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe online nature of RL makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states.\u003c/p\u003e\n\u003ch3\u003eSection 3.8: Summary\u003c/h3\u003e\n\u003cp\u003eRL is about an \u003cem\u003eagent\u003c/em\u003e learning how to behave in order to achieve a goal by interacting with its \u003cem\u003eenvironment\u003c/em\u003e over a sequence of discrete time steps. The specification of their interface defines a particular task:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eActions\u003c/em\u003e: choices made by the agent;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eStates\u003c/em\u003e: the basis for making the choices;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eRewards\u003c/em\u003e: the basis for evaluating the choices.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA \u003cem\u003epolicy\u003c/em\u003e is a stochastic rule by which the agent selects the actions as function of states. The \u003cem\u003eoptimal policy\u003c/em\u003e is that which best achieves the agent's objective, maximizing the amount of reward it receives over time.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003ereturn\u003c/em\u003e is the function of future rewards that the agent seeks to maximize (in expected value). Its definition depends on the nature of the task and whether one wishes to \u003cem\u003ediscount\u003c/em\u003e delayed reward. The undiscounted formulation is appropriate for \u003cem\u003eepisodic tasks\u003c/em\u003e, and the discounted formulation is appropriate for tabular \u003cem\u003econtinuing tasks\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eA policy's \u003cem\u003evalue functions\u003c/em\u003e ($$v^\u003cem\u003e$$ and $$q^\u003c/em\u003e$$) assign to each state, or stateâ€“action pair, the largest expected return achievable by any policy. Whereas the optimal value functions are unique for a given MDP, there can be many optimal policies. In most cases, we seek approximations of the optimal value functions and policies, not their exact values.\u003c/p\u003e\n\u003ch2\u003eChapter 4: Dynamic Programming\u003c/h2\u003e\n\u003cp\u003eIn this (and future) chapters, we will usually assume that the environment is a finite MDP. This means that we assume $$|\\mathcal{S}| \u0026#x3C; \\infty, |\\mathcal{A}| \u0026#x3C; \\infty, |\\mathcal{R}| \u0026#x3C; \\infty,$$ and that its dynamics are given by a set of probabilities $$p(s', r \\vert s, a), \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}, \\forall r \\in \\mathcal{R}, s' \\in \\mathcal{S}^+$$.\u003c/p\u003e\n\u003cp\u003eBeyond finite MDPs, Dynamic Programming (DP) ideas can be applied to problems with a continuous state and action spaces, by exact solutions for these types of problems are only possible in special cases.\u003c/p\u003e\n\u003cp\u003eThe key idea of DP (and RL in general) is the use of value functions to both structure and organize the search for good policies. DP algorithms are obtained by turning Bellman equations into assignments, i.e., into update rules for improving approximations of the desired value functions. Remember that we can easily obtain optimal policies if we find the optimal value functions, $$v^* \\lor q^*$$, which satisfy the Bellman optimality equations, $$\\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}(s), s' \\in \\mathcal{S}^+$$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv^\u003cem\u003e(s) \u0026#x26;= \\max_a \\mathbb{E}[R_{t+1} + \\gamma \\cdot v^\u003c/em\u003e(S_{t+1} \\vert S_t = s, A_t = a)] \\nonumber\\\n\u0026#x26;= \\max_a \\sum_{s', r} p(s' r, \\vert s, a) [r + \\gamma \\cdot v^\u003cem\u003e(s')],\\\n\u0026#x26;\\lor \\nonumber\\\nq^\u003c/em\u003e(s, a) \u0026#x26;= \\mathbb{E}[R_{t+1} + \\gamma \\max_{'a} q^\u003cem\u003e(S_{t+1}, a') \\vert S_t = s, S_t =a], \\nonumber\\\n\u0026#x26;= \\sum_{s', r} p(s', r \\vert s, a) [r + \\gamma \\max_{a'} q^\u003c/em\u003e(s', a')].\n\\end{align}\n$$\u003c/p\u003e\n\u003ch3\u003eSection 4.1: Policy Evaluation (Prediction)\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ePolicy evaluation:\u003c/strong\u003e computing the state-value function $$v^{\\pi}$$ for an arbitrary policy $$\\pi$$. This is also referred to as the \u003cem\u003eprediction problem\u003c/em\u003e. Recall from \u003ca href=\"#chapter-3-finite-markov-decision-processes\"\u003eChapter 3\u003c/a\u003e that, $$\\forall s \\in \\mathcal{S}$$,\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv^{\\pi}(s) \u0026#x26;\\doteq \\mathbb{E}\u003cem\u003e{\\pi}[G_t \\vert S_t = s], \\nonumber\\\n\u0026#x26;= \\mathbb{E}\u003c/em\u003e{\\pi}[R_{t+1} + \\gamma G_{t+1} \\vert S_t = s], \\nonumber\\\n\u0026#x26;= \\mathbb{E}\u003cem\u003e{\\pi}[R\u003c/em\u003e{t+1} + \\gamma v^{\\pi} (S_{t+1} \\vert S_t = s)], \\\n\u0026#x26;= \\sum_a \\pi(a \\vert s) \\sum_{s', r} p(s', r \\vert s, a)[r + \\gamma v^{\\pi}(s')],\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\pi(a\\vert s)$$ is the probability of taking action $$a$$ in state $$s$$ under policy $$\\pi$$, and the expectations are subscripted by $$\\pi$$ to indicate that they are conditional on following policy $$\\pi$$. If $$\\gamma \u0026#x3C; 1$$ or eventual termination is guaranteed from all states under the policy $$\\pi$$, then w.h.t. that $$v^{\\pi}$$ exists and is unique.\u003c/p\u003e\n\u003cp\u003eIf the environment's dynamics are completely known, then the previous equation is a system of $$|\\mathcal{S}|$$ simultaneous linear equations in $$|\\mathcal{S}|$$ unknowns (the $$v^{\\pi}(s), s \\in \\mathcal{S}$$). Consider a sequence of approximate value functions $$v_0, v_1, \\dots,$$ each mapping $$S^+ \\rightarrow \\mathbb{R}$$. The initial approximation $$v_0$$ is chosen arbitrarily (except the terminal state, which, if it exists, must have value $0$), and, $$\\forall s \\in \\mathcal{S},$$ each successive approximation is obtained by using the Bellman equation for $$v^{\\pi}$$ as an update rule:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv_{k+1}(s) \u0026#x26;\\doteq \\mathbb{E}\u003cem\u003e{\\pi}[R\u003c/em\u003e{t+1} + \\gamma \\cdot v_k (S_{t+1}) \\vert S_t = s] \\nonumber\\\n\u0026#x26;= \\sum_a \\pi(a \\vert s)\\sum_{s', r} p(s', r \\vert s,a )[r + \\gamma \\cdot v_k(s')].\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eSince the Bellman equation for $$v^{\\pi}$$ assures us of the equality in the case that $$v_k = v^{\\pi},$$ this is a fixed point for this update rule. Then, when under the same conditions that guarantee the existence of $$v^{\\pi},$$ w.h.t. $$\\lim_{k \\rightarrow \\infty} {v_k} = v^{\\pi}$$. This algorithm, called \u003cstrong\u003eiterative policy evaluation\u003c/strong\u003e, produces each successive approximation $$v_{k+1}$$ from $$v_k$$, which we call an \u003cem\u003eexpected update\u003c/em\u003e (since they are based on an expectation over all possible states), by applying the same operation to each state $$s$$: replacing the old value of $$s$$ with a new value obtained from the old values of the successor states of $$s$$, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef iterative_policy_evaluation(policy, mdp, theta, gamma):\n\tassert theta \u003e 0;\n\t\n\tstate_values[len(mdp.states)];\n\tstate_values[len(mdp.states)] = 0;\n\tfor x in range(len(mdp.states) - 1):\n\t\tstate_values[x] = random_value();\n\t\t\n\tgradient;\n\tdo {\n\t\tgradient = 0;\n\t\tfor s in mdp.states:\n\t\t\tv = state_values[s.id];\n\t\t\tstate_values[s.id] = 0;\n\t\t\tfor tmp_s in mdp.state:\n\t\t\t\tfor a in mdp.actions:\n\t\t\t\t\tstate_values[s.id] += policy[s.id][a.id] * mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);\n\t\t\t\n\t\t\tgradient = max(gradient, absolute_value(v - state_values[s.id]));\n\t} while(gradient \u003e= theta);\n\n\treturn state_values;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 4.2: Policy Improvement\u003c/h3\u003e\n\u003cp\u003eKnowing the value function $$v^{\\pi}$$ for an arbitrary deterministic policy $$\\pi$$, and given some state $$s$$, how can we determine whether or not we should change the policy to deterministically choose an action $$a \\neq \\pi(s)$$? One possible way, is to consider selecting $a$ in $s$ and afterwards following the existing policy $$\\pi$$. The value of this manner of behaving is given by\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nq^{\\pi}(s, a) \u0026#x26;\\doteq \\mathbb{E}[R_{t+1} + \\gamma v^{\\pi}(S_{t+1} \\vert S_t = s, A_t = a)] \\nonumber\\\n\u0026#x26;= \\sum_{s', r} p(s', r \\vert s, a)[r + \\gamma \\cdot v^{\\pi}(s')].\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eThe key criterion is whether this is greater than or less than $$v^{\\pi}$$. If it is greater, i.e., if it is better to select $$a$$ once in $$s$$ and thereafter follow $$\\pi$$ than it is to always follow $$\\pi$$, then one can expect that it would be better to select $$a$$ ever time $$s$$ is encountered, and that such new policy would be a better one overall. This fact is a special case of a general result called the \u003cem\u003epolicy improvement theorem\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eLet $$\\pi$$ and $$\\pi'$$ be any pair of deterministic policies s.t.:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nq^{\\pi}(s, \\pi'(s)) \\geq v^{\\pi}(s), \\forall s \\in \\mathcal{S}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThen the policy $$\\pi'$$ must be as good as, or better than, $$\\pi$$, i.e., it must obtain greater or equal expected return from all states:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nv^{\\pi'}(s) \\geq v^{\\pi}(s), s \\in \\mathcal{S}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eAdditionally, if there is strict inequality of $$q^{\\pi}(s, \\pi'(s))$$ at any state, then there must be strict inequality of $$v^{\\pi'}(s)$$ at that state. This theorem applies to the two policies considered: a deterministic policy $$\\pi$$, and a changed policy $$\\pi$$, identical to $$\\pi$$ in everything except $$\\pi'(s) = a \\neq \\pi(s)$$. Since $$v^{\\pi'} = v^{\\pi}, \\forall s' \\in \\mathcal{S} \\setminus {s},$$ if $$q^{\\pi}(s, a) \u003e v^{\\pi}(s)$$, then the changed policy $$\\pi'$$ must be better than $$\\pi$$.\u003c/p\u003e\n\u003cp\u003eIn order to understand the idea of the proof behind the policy improvement theorem, we can start from $$q^{\\pi}(s, \\pi'(s)) \\geq v^{\\pi}(s)$$, then keep expanding the l.h.s. with the equation for $$q^{\\pi}(s, a)$$ and reapplying $$q^{\\pi}(s, \\pi'(s)) \\geq v^{\\pi}(s)$$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv^{\\pi}(s) \u0026#x26;\\leq q^{\\pi}(s, \\pi'(s))\\\n\u0026#x26;= \\mathbb{E}[R_{t+1} + \\gamma \\cdot v^{\\pi}(S_{t=1}) \\vert S_t = s, A_t = \\pi'(s)] \\nonumber\\\n\u0026#x26;= \\mathbb{E}\u003cem\u003e{\\pi'}[R\u003c/em\u003e{t+1} + \\gamma \\cdot v^{\\pi}(S_{t+1}) \\vert S_t = s] \\nonumber\\\n\u0026#x26;\\leq \\mathbb{E}\u003cem\u003e{\\pi'}[R\u003c/em\u003e{t+1} + \\gamma \\cdot q^{\\pi}(S_{t+1}, \\pi'(S_{t+1})) \\vert S_t = s] \\nonumber\\\n\u0026#x26;= \\mathbb{E}\u003cem\u003e{\\pi'}\\bigg[R\u003c/em\u003e{t+1} + \\gamma \\cdot \\mathbb{E}[R_{t+2} + \\gamma \\cdot v^{\\pi}(S_{t+2}) \\vert S_{t+1}, A_{t+1} = \\pi'(S_{t+1})] \\vert S_t = s \\bigg] \\nonumber\\\n\u0026#x26;\\leq \\mathbb{E}\u003cem\u003e{\\pi'}[R\u003c/em\u003e{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\gamma^3 R_{t+4} + \\dots \\vert S_t = s] \\nonumber\\\n\u0026#x26;\\dots \\nonumber\\\n\u0026#x26;= v^{\\pi'}(s).\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eNow knowing how, given a policy policy and its value function, to evaluate a change in the policy at a single, we to extend this result to consider changes at all states - selecting at each state the action that appears best according to $$q^{\\pi}(s, a)$$, i.e., to consider the new \u003cem\u003egreedy\u003c/em\u003e policy $$\\pi'$$, given by\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\pi'(s) \u0026#x26;\\doteq \\argmax_a \\  q^{\\pi}(s, a) \\nonumber\\\n\u0026#x26;= \\argmax_a \\ \\mathbb{E}[R_{t+1} + \\gamma \\cdot v^{\\pi}(S_{t+1}) \\vert S_t = s, A_t = a] \\nonumber\\\n\u0026#x26;= \\argmax_a \\sum_{s', r} p(s', r \\vert s, a)[r + \\gamma \\cdot v^{\\pi}(s')].\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eThe greedy policy takes the action that looks best according to $$v^{\\pi}$$ after a single step of lookahead. Since, by construction, the greedy policy meets the conditions of the policy improvement theorem, we know that it is as good as - or better than - the original policy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePolicy improvement\u003c/strong\u003e:  process of making a new policy that improves on an original policy by making it greedy w.r.t. the value function of the original policy.\u003c/p\u003e\n\u003cp\u003eSupposing that the new greedy policy $$\\pi'$$ is as good as, but not better than, the old policy $$\\pi$$, then $$v^{\\pi}(s) = v^{\\pi'}$$, and, $$\\forall s \\in \\mathcal{S}$$, it follows from the previous equation that\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv^{\\pi'}(s) \u0026#x26;= \\max_a \\mathbb{E}[R_{t+1} + \\gamma \\cdot v^{\\pi'}(S_{t+1} \\vert S_t = s, A_t = a)] \\nonumber\\\n\u0026#x26;= \\max_a \\sum_{s', r} p(s', r \\vert s, a)[r + \\gamma \\cdot v^{\\pi'}(s')].\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eAs this is the same as the Bellman optimality equation, $$v^{\\pi'}$$ must be $$v^*$$, and both $$\\pi$$ and $$\\pi'$$ must be optimal policies. Thus, policy improvement must give us a strictly better policy except when the original policy is already optimal.\u003c/p\u003e\n\u003cp\u003eThe policy improvement theorem carries through as stated for the stochastic case. Also, if there are any ties in policy improvement steps - i.e., there are several actions at which the maximum is achieved - then we don't need to select a single action from among them, as each maximizing action can be given a portion of the probability of being selected in the new greedy policy (as long as we give zero probability to all submaximal actions).\u003c/p\u003e\n\u003ch3\u003eSection 4.3: Policy Iteration\u003c/h3\u003e\n\u003cp\u003eA policy $$\\pi$$ can be iteratively improved to yield a better policy, thus leading to a sequence of monotonically improving policies and value functions:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\pi_0 \\xrightarrow{E} v^{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v^{\\pi_1} \\xrightarrow{I} \\pi_2 \\xrightarrow{E} \\dots \\xrightarrow{I} \\pi^* \\xrightarrow{E} v^*,\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\xrightarrow{E}$$ denotes a policy \u003cem\u003eevaluation\u003c/em\u003e and $$\\xrightarrow{I}$$ a policy \u003cem\u003eimprovement\u003c/em\u003e. Each policy is guaranteed to be a strict improvement over the previous one, unless it is already an optimal policy. Since a finite MDP has a finite number of deterministic policies, this process must converge to an optimal policy and the optimal value function in a finite number of iterations.\u003c/p\u003e\n\u003cp\u003eThis way of finding an optimal policy is called \u003cem\u003epolicy iteration\u003c/em\u003e. Each policy evaluation is started with the value function for the previous policy, which usually results in a great increase in the algorithm's speed of converge (presumably due to the fact that the value function changes little from one policy to the next).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef policy_evaluation(policy, mdp, state_values, theta, gamma):\n\tgradient;\n\tdo {\n\t\tgradient = 0;\n\t\tfor s in mdp.states:\n\t\t\tvalue = state_values[s.id];\n\t\t\tstate_values[s.id] = 0;\n\t\t\tfor tmp_s in mdp.states:\n\t\t\t\tstate_values[s.id] += mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=argmax(policy[tmp_s.id])) * (mdp.reward + gamma * state_values[tmp_s.id]);\n\t\t\t\t\n\t\t\tgradient = max(gradient, absolute_value(value - state_values[s.id]))\n\t} while(gradient \u003e= theta);\n\n\treturn state_values;\n\ndef policy_improvement(policy, mdp, gamma):\n\tpolicy_stable = True;\n\t\n\tfor s in mdp.states:\n\t\told_action = argmax(policy[s.id]);\n\t\tfor tmp_s in mdp.states:\n\t\t\tpolicy[s.id] = mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=) * (mdp.reward + gamma * state_values[tmp_s.id]);\n\t\t\n\t\tif old_action != argmax(policy[s]):\n\t\t\tpolicy_stable = False;\n\n\treturn policy, policy_stable;\n\ndef policy_iteration(mdp, theta, gamma):\n\tassert theta \u003e 0;\n\t\n\tstate_values[len(mdp.states)];\n\tstate_values[len(mdp.states)] = 0;\n\tpolicy[len(mdp.states)][len(mdp.actions)];\n\tfor x in range(len(mdp.states) - 1):\n\t\tstate_values[x] = random_value();\n\t\tfor y in range(len(mdp.actions)):\n\t\t\tpolicy[x][y] = random_value();\n\n\t\t// Since a policy is probabilities, its values must sum to 1\n\t\tpolicy[x] = (policy[x] - min(policy[x]) / (max(policy[x]) - min(policy[x]))\n\n\n\tpolicy_stable = False;\n\twhile (!policy_stable):\n\t\tstate_values = policy_evaluation(policy, mdp, state_values, theta, gamma)\n\t\tpolicy, policy_stable = policy_improvement(policy, mdp, gamma)\n\n\treturn policy\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 4.4: Value Iteration\u003c/h3\u003e\n\u003cp\u003eA drawback of policy iteration is that it may involve several policy evaluations, which can be a protracted computation in itself. If policy evaluation is done iteratively, then convergence to exactly $$v^{\\pi}$$ occurs only at the limit. However, we can often truncate policy evaluation, e.g., policy evaluation iterations beyond the first three have no effect on the corresponding greedy policy.\u003c/p\u003e\n\u003cp\u003eThe policy evaluation step of policy iteration can be truncated in several ways without losing the algorithm's convergence guarantees. One special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm, which is called \u003cem\u003evalue iteration\u003c/em\u003e, can be written as a simple update operation that combines the policy improvement and truncated policy evaluation steps:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nv_{k+1}(s) \u0026#x26;\\doteq \\max_a \\mathbb{E}[R_{t+1} + \\gamma \\cdot v_k(S_{t+1}) \\vert S_t = s, A_t = a] \\nonumber\\\n\u0026#x26;= \\max_a \\sum_{s', r} p(s', r \\vert s, a)[r + \\gamma \\cdot v_k(s')], \\forall s \\in \\mathcal{S}.\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eFor an arbitrary $$v_0$$, the sequence $${v_k}$$ can be shown to converge to $$v*$$ under the same conditions that guarantee the existence of $$v^*$$. Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule. Also, note how the value iteration update is identical to the policy evaluation update, except that it requires the maximu8m to be taken over all actions.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef value_iteration(mdp, theta, gamma):\n\tassert theta \u003e 0;\n\nÂ  Â  state_values[len(mdp.states)];\nÂ  Â  state_values[len(mdp.states)] = 0;\nÂ  Â  policy[len(mdp.states)];\nÂ  Â  for x in range(len(mdp.states) - 1):\nÂ  Â  Â  Â  state_values[x] = random_value();\nÂ  Â  Â  Â  policy[x] = 0;\n\n\tgradient;\n\tdo {\n\t\tgradient = 0;\n\t\tfor s in mdp.states:\n\t\t\told_value = state_values[s.id];\n\t\t\ttmp_value[len(mdp.states)];\n\t\t\tfor tmp_s in mdp.state:\n\t\t\t\tfor a in mdp.actions:\n\t\t\t\t\ttmp_value[tmp_s.id] += mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);\n\n\t\t\tstate_values[s.id] = max(tmp_value);\n\t\t\tgradient = max(gradient, absolute_value(old_value - state_values[s.id]));\n\t} while(gradient \u003e= theta);\n\n\tfor s in mdp.states:\n\t\ttmp_value[len(mdp.states)];\n\t\tfor tmp_s in mdp.state:\n\t\t\tfor a in mdp.actions:\n\t\t\t\ttmp_value[tmp_s.id] += mdp.reward_probabilities(prev_state=s, cur_state=tmp_s, action=a) * (state.reward + gamma * state_values[state.id]);\n\n\t\tpolicy[s.id] = argmax(tmp_value);\n\n\treturn policy;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn each of its sweeps, value iteration combines one sweep of policy evaluation and one of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. Since the max operation is the only difference between these updates, this just means that the max operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs.\u003c/p\u003e\n\u003ch3\u003eSection 4.5: Asynchronous Dynamic Programming\u003c/h3\u003e\n\u003cp\u003eA significant drawback to the previously discussed DP methods is the fact that they involve operations over the entire state set of the MDP, i.e., they require sweeps of the state set. As such, a single sweep can become prohibitively expensive, e.g., in the game of backgammon (that has $10^{20}$ states), even if we could perform the value iteration update on a million states per second, it would still take $$1000+$$ years to complete a single sweep.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eAsynchronous\u003c/em\u003e DP algorithm:  in-place iterative DP algorithm that is not organized in terms of systematic sweeps of the state set, i.e., the values of some states may be updated several times before the values of others are updated even once.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTo convergence correctly, such an algorithm must continue to update the values of all the state, i.e., it can't ignore any state after some point in the computation;\u003c/li\u003e\n\u003cli\u003eDoesn't need to get locked into any hopelessly long sweep before making progress improving a policy, allowing one to order the updates s.t. value information efficiently propagates from state to state (some ideas for this are introduced in \u003ca href=\"#chapter-8-planning-and-learning-with-tabula-methods\"\u003eChapter 8\u003c/a\u003e);\u003c/li\u003e\n\u003cli\u003eCan be run \u003cem\u003eat the same time that an agent is actually experience the MDP\u003c/em\u003e, meaning that the agent's experience can be used to determine which states to update and the value and policy information can guide the agent's decision making.\n\u003cul\u003e\n\u003cli\u003eEx: apply updates to states as the agents visits them, thus focusing the algorithm's updates onto the parts state set parts most relevant to the agent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample (sweepless DP) algorithm: update the value, in place, of only one state $s_k$ on each step $k$, using the value iteration update.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf $$0 \\leq \\gamma \u0026#x3C; 1$$, asymptotic convergence to $v^*$ is guaranteed given only that all states occur in the sequence $${s_k}$$ an infinite number of times;\u003c/li\u003e\n\u003cli\u003eIt is possible to intermix policy evaluation and value iteration updates to produce a kind of asynchronous truncated policy iteration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSection 4.6: Generalized Policy Iteration\u003c/h3\u003e\n\u003ch3\u003eSection 4.6: Generalized Policy Iteration\u003c/h3\u003e\n\u003cp\u003ePolicy iteration consists of two simultaneous and interacting processes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePolicy evaluation: makes the value function consistent with the current policy;\u003c/li\u003e\n\u003cli\u003ePolicy improvement: makes the policy greedy w.r.t. the current value function.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese two processes need not be alternate and/or completed one after the other, since, as long as both processes continue to update all states, convergence to the optimal value function and (an) optimal policy is guaranteed.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eGeneralized policy iteration\u003c/em\u003e (GPI): the general idea of letting policy evaluation and policy improvement processes interact, independent of the details of the two processes. Almost all RL methods can be described as GPI, i.e., they have identifiable policies and value functions, with the policy always being improved w.r.t. the value function, and the value function always being driven toward the value function for the policy.\u003c/p\u003e\n\u003cp\u003eIf both the evaluation process and the improvement process stabilize, then the value function and policy must be optimal. The value function only stabilizes when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. This implies that the Bellman optimality equation, which means that the policy and the value function are guaranteed to be optimal.\u003c/p\u003e\n\u003cfigure align='center'\u003e\n\t\u003cimg alt=\"Example of a policy iteration process.\" src=\"http://acfpeacekeeper.github.io/github-pages/assets/images/literature/example_pi_process.png\" onerror=\"this.src='http://localhost:4000/assets/images/literature/example_pi_process.png';\"\u003e\n\t\u003cfigcaption\u003eFigure 3: Example of a policy iteration process.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch3\u003eSection 4.7: Efficiency of Dynamic Programming\u003c/h3\u003e\n\u003cp\u003eWhile DP methods may not be the most practical solution for very large problems they can be quite efficient when compared with other methods for solving MDPs. If $$n$$ and $$k$$ denote the number of states and actions, respectively, a DP method takes less computational operations than some polynomial function of $$n$$ and $$k$$ to find an optimal policy, even though the total number of (deterministic) policies is $$k^n$$.\u003c/p\u003e\n\u003cp\u003eDirect search and linear programming methods can also be used to solve MDPs, and in some cases their worst-case convergence guarantees are even better than those of DP methods. However, linear programming methods become impractical at a much smaller number of states than DP methods do (by a factor of about 100).\u003c/p\u003e\n\u003cp\u003eWhile DP methods may seem of limited applicability due to the \u003cem\u003ecurse of dimensionality\u003c/em\u003e (the fact that the number of states often grows exponentially with the number of state variables), in practice, such methods can be used with today's computers to solve MDPs with millions of states. Also, these methods normally converge much faster than their theoretical worst-case runtimes, specially if they are started with good initial value functions or policies. For very large state spaces, \u003cem\u003easynchronous\u003c/em\u003e DP are often preferred.\u003c/p\u003e\n\u003ch3\u003eSection 4.8: Summary\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003ePolicy evaluation\u003c/em\u003e refers to the (usually) iterative computation of the value functions for a given policy. In turn, \u003cem\u003epolicy improvement\u003c/em\u003e refers to the computation of an improved policy given the value function for that policy. Putting these two together forms the backbone of the two most popular DP methods, \u003cem\u003epolicy iteration\u003c/em\u003e and \u003cem\u003evalue iteration\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eClassical DP methods involve sweeps of \u003cem\u003eexpected update\u003c/em\u003e operations over each state of the state set. They are little more than the Bellman operations turned into assignment statements. Convergence occurs once the updates no longer result in changes in value.\u003c/p\u003e\n\u003cp\u003eAlmost all RL methods can be viewed as a form of GPI: one process takes the policy as given and performs some form of policy evaluation, changing the value function to be more like the true value function for the policy; and the other process takes the value function as given (assuming its the policy's value function) and performs some form of policy improvement, changing the policy to make it better.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eAsynchronous\u003c/em\u003e DP methods are iterative methods that updates states in an arbitrary order, possibly stochastic and using out-of-date information.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBootstrapping\u003c/strong\u003e: general idea of updating estimates based on other estimates.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDP methods perform this as they update the estimates of the values of states based on the estimates of the values of successor states;\u003c/li\u003e\n\u003cli\u003eMany RL methods also perform this, even those that - unlike DP methods - do not require a complete and accurate model of the environment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eChapter 5: Monte Carlo Methods\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eMonte Carlo\u003c/strong\u003e (MC) method: an estimation method that obtain results by computing the average of repeated random samples. For RL, this means computing the average of complete returns (as opposed to methods that learn from partial returns).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRequires only experience, i.e., sample sequences of states, actions, and rewards from an actual or simulated interaction with an environment;\u003c/li\u003e\n\u003cli\u003eValue estimates and policy changes are only computed when an episode terminates;\u003c/li\u003e\n\u003cli\u003eTo ensure that well-defined returns are available, here we define MC methods only for episodic tasks (i.e., experience is divided into episodes that eventually terminate).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLearning from \u003cem\u003eactual\u003c/em\u003e experience is a very powerful tool, since it requires no prior knowledge of the environment's dynamics, yet can still achieve optimal behavior. Learning from \u003cem\u003esimulated\u003c/em\u003e experience is also powerful, although a model is required to generate sample transitions (unlike in DP, which requires a complete probability distribution of all possible transitions).\u003c/p\u003e\n\u003cp\u003eMC methods sample and average \u003cem\u003ereturns\u003c/em\u003e for each state-action pair, similarly to the bandit methods of \u003ca href=\"#chapter-2-multi-armed-bandits\"\u003eChapter 2\u003c/a\u003e. The main difference is that there are now multiple different bandit problems which are all interrelated, i.e., the return after taking an action is one state depends on the actions taken in later states of the same episode (making the problem non-stationary from the P.o.V. of the earlier state). We adapt the idea of GPI to handle the non-stationarity, where instead of \u003cem\u003ecomputing\u003c/em\u003e value function from knowledge of the MDP, we \u003cem\u003elearn\u003c/em\u003e value functions from sample return with the MDP.\u003c/p\u003e\n\u003ch3\u003eSection 5.1: Monte Carlo Prediction\u003c/h3\u003e\n\u003cp\u003eRecall that the value of a state is the expected return - expected cumulative future discounted reward - starting from that state. The idea of estimating the expected return by averaging the returns observed after visits to that state underlies all MC methods. Each occurrence of a state $s$ in an episode is called a \u003cem\u003evisit\u003c/em\u003e to $s$, and the first time it is visited in an episode is called the \u003cem\u003efirst visit\u003c/em\u003e to $s$.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003efirst-visit\u003c/em\u003e MC method estimates $v^{\\pi}(s)$ as the average of the returns following first visits to $s$, whereas the \u003cem\u003eevery-visit\u003c/em\u003e MC method averages the returns following all visits to $s$. By the law of large numbers, both methods converge to $v^{\\pi}(s)$ as the number of (first-)visits to $s$ goes to infinity. The two methods are similar, but have slightly different theoretical properties. Every-visit MC extends more naturally to function approximation and eligibility traces.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef first_visit_monte_carlo_prediction(policy, gamma, mdp):\n\treturns[len(mdp.states)] = [];\n\tstate_values[len(mdp.states)];\nÂ  Â  state_values[len(mdp.states)] = 0;\nÂ  Â  for x in range(len(mdp.states) - 1):\nÂ  Â  Â  Â  state_values[x] = random_value();\n\n\twhile(True):\n\t\tepisode = generate_episode_following_policy(policy, mdp);\n\t\tG = 0;\n\t\tfor step in episode[:0:-1]:\n\t\t\tG = gamma * G + step.reward;\n\t\t\tif step.state not in episode[:-1]:\n\t\t\t\treturns[step.state.id].append(G);\n\t\t\t\tstate_value[step.state.id] = mean(returns)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 5.2: Monte Carlo Estimation of Action Values\u003c/h3\u003e\n\u003cp\u003eWithout a model, one must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. As such, one the primary goals for MC methods is to estimate $q^*$. The policy evaluation problem for action values is to estimate $q^{\\pi}(s, a)$, i.e., the expected return when starting in state $s$, taking action $a$, and thereafter following policy $\\pi$.\u003c/p\u003e\n\u003cp\u003eA stateâ€“action pair $(s, a)$ is said to be visited in an episode if ever the state $s$ is visited and action $a$ is taken in it. The every-visit MC method estimates the value of a stateâ€“action pair as the average of the returns that have followed all the visits to it. In turn, the first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected.\u003c/p\u003e\n\u003cp\u003eIf $\\pi$ is a deterministic policy, then in following it one will observe returns only for one of the actions from each state, and with no returns to average, the MC estimates of the other actions will not improve with experience. This is a serious problem, since to alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.\u003c/p\u003e\n\u003cp\u003eThis is the general problem of maintaining \u003cem\u003eexploration\u003c/em\u003e, as discussed in \u003ca href=\"#chapter-2-multi-armed-bandits\"\u003eChapter 2\u003c/a\u003e. For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by assuming \u003cem\u003eexploring starts\u003c/em\u003e, which means that the episodes \u003cem\u003estart in a stateâ€“action pair\u003c/em\u003e, and that every pair has a nonzero probability of being selected as the start.\u003c/p\u003e\n\u003cp\u003eThe previous assumption cannot always be relied upon, particularly when learning directly from actual interaction with an environment. The most common alternative approach to assuring that all stateâ€“action pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state.\u003c/p\u003e\n\u003ch3\u003eSection 5.3: Monte Carlo Control\u003c/h3\u003e\n\u003ch2\u003eChapter 6: Temporal-Difference Learning\u003c/h2\u003e\n\u003ch2\u003eChapter 7: $$n$$-step Bootstrapping\u003c/h2\u003e\n\u003ch2\u003eChapter 8: Planning and Learning with Tabular Methods\u003c/h2\u003e\n\u003ch1\u003ePart II: Approximate Solution Methods\u003c/h1\u003e\n\u003ch2\u003eChapter 9: On-policy Prediction with Approximation\u003c/h2\u003e\n\u003ch2\u003eChapter 10: On-policy Control with Approximation\u003c/h2\u003e\n\u003ch2\u003eChapter 11: *Off-policy Methods with Approximation\u003c/h2\u003e\n\u003ch2\u003eChapter 12: Eligibility Traces\u003c/h2\u003e\n\u003ch2\u003eChapter 13: Policy Gradient Methods\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eNotation\u003c/strong\u003e relevant for this chapter:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePolicy's parameter vector: $$\\theta \\in \\mathbb{R}^{d'}$$;\u003c/li\u003e\n\u003cli\u003eLearned value function's weight vector: $$w \\in \\mathbb{R}^d$$;\u003c/li\u003e\n\u003cli\u003eScalar performance measure w.r.t. the policy parameter: $$J(\\theta)$$;\u003c/li\u003e\n\u003cli\u003eProbability that action $$a$$ is taken at time $$t$$, given that the environment is in state $$s$$ at time $$t$$: $$\n\\pi (a \\vert s, \\theta) = P (A_t = a|S_t = s,\\theta_t = \\theta)\n$$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePolicy gradient\u003c/strong\u003e methods seek to learn an approximation to the policy by maximizing performance. Their updates approximate gradient ascent such as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\hat{\\nabla J(\\theta_t)}, \\ \\hat{\\nabla J(\\theta_t)} \\in \\mathbb{R}^{d'}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eMethods that learn an approximation of both policy and value functions are called \u003cstrong\u003eactor-critic\u003c/strong\u003e methods. The \u003cem\u003eactor\u003c/em\u003e is a reference to the learned policy and \u003cem\u003ecritic\u003c/em\u003e a reference to the learned (state-)value function.\u003c/p\u003e\n\u003ch3\u003eSection 13.1: Policy Approximation and its Advantages\u003c/h3\u003e\n\u003cp\u003eThe policy can be parameterized in any way, as long as 2 conditions are met\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAs long as $$\\pi(a\\vert s, \\theta)$$ is differentiable w.r.t. its parameters:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\n\\forall s \\in S \\ \\forall a \\in A(s) \\ \\exists \\ \\nabla \\pi (a|s, \\theta) : |\\nabla \\pi (a|s, \\theta)| \u0026#x3C; \\infty \\ \\wedge \\theta \\in \\mathbb{R}^{d'};\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAs long as it continues to perform exploration (to avoid a deterministic policy):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\n\\pi (a|s, \\theta) \\ \\in \\  ]0, 1[.\n$$\u003c/p\u003e\n\u003cp\u003eFor small-to-medium discrete action spaces, it is common to form parameterized numerical preferences $$h(s, a, \\theta) \\in \\mathbb{R}$$ for each $$(s, a)$$ pair. The probabilities of each action being selected can be calculated with, e.g., an exponential softmax distribution:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\pi (a|s, \\theta) \\doteq \\frac{\\exp(h(s,a,\\theta))}{\\sum_b \\exp(h(s,b,\\theta))}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThis kind of policy parameterization is called \u003cem\u003esoftmax in action preferences\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe action preferences can be parameterized arbitrarily, e.g., as the output of a Deep Neural Network (DNN) with parameters $$\\theta$$. They can also be linear in features:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nh(s,a,\\theta) = \\theta^T \\cdot x(s, a), \\ x(s, a) \\in \\mathbb{R}^{d'}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003echoice of policy parameterization\u003c/em\u003e can be a good way of \u003cem\u003einjecting prior knowledge\u003c/em\u003e about the desired form of the policy into the RL system. Beyond this, parameterizing policies according to the softmax in action preferences has several advantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAllows the approximate policy to approach a deterministic policy, unlike with $$\\epsilon$$-greedy action selection (due to the $$\\epsilon$$ probability of selecting a random action);\u003c/li\u003e\n\u003cli\u003eEnables the selection of actions with arbitrary probabilities (useful if the optimal policy is a stochastic policy, e.g., in a game of chance such as poker);\u003c/li\u003e\n\u003cli\u003eIf the policy is easier to approximate then the action-value function, then policy-based methods learn faster and yield a superior asymptotic policy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSection 13.2: The Policy Gradient Theorem\u003c/h3\u003e\n\u003cp\u003ePolicy parameterization has an important theoretical advantage over $$\\epsilon$$-greedy action selection: the continuity of the policy dependence on the parameters that enables policy gradient methods to approximate gradient ascent.\u003c/p\u003e\n\u003cp\u003eDue to the continuous policy parameterization the action probabilities change smoothly as a function of the learned parameters, unlike with $$\\epsilon$$-greedy selection, where the action probabilities may change dramatically if an update changes which action has the maximal value.\u003c/p\u003e\n\u003cp\u003eIn the episodic case, the performance measure is defined as the value of the start state of the episode. Taking a (non-random) state $$s_0$$ as the start state of the episode and $$v_{\\pi_{\\theta}}$$ as the true value function for the policy $$\\pi_{\\theta}$$, the performance can be defined as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nJ(\\theta) \\doteq v_{\\pi_{\\theta}} (s_0).\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eSince the policy parameters affect both the action selections and the distribution of states in which those selections are made, and performance also depends on both, it may be challenging to to change the policy parameters such that improvement is ensured, particularly since the effect of the policy on the state distribution is a function of the environment, and thus is typically unknown.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003epolicy gradient theorem\u003c/strong\u003e provides a solution towards this challenge in the manner of an analytic expression for the gradient of performance w.r.t. the policy parameters, without the need for the derivative of the state distribution. The equation for episodic case of the policy gradient theorem is given by:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\nabla J(\\theta) \\propto \\sum_s \\mu (s) \\sum_a q^{\\pi}(s, a) \\cdot \\nabla \\pi(a|s, \\theta),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\mu$$ is the on-policy distribution under $$\\pi$$. For the episodic case, the constant of proportionality is the average length of an episode, and for the continuing case it is 1.\u003c/p\u003e\n\u003ch3\u003eSection 13.3: REINFORCE: Monte Carlo Policy Gradient\u003c/h3\u003e\n\u003cp\u003eSince any constant of proportionality can be absorbed into the step size $$\\alpha$$, all that is required is a way of sampling that approximates the policy gradient theorem. As the r.h.s. of the theorem is a sum over states weighted by their probability of occurring under the target policy $$\\pi$$, w.h.t.:\n$$\n\\begin{align}\n\\nabla J(\\theta) \u0026#x26;\\propto \\sum_s \\sum_a q^{\\pi}(s, a) \\cdot \\nabla \\pi (a|s, \\theta) \\nonumber\\\n\u0026#x26;= \\mathbb{E}_{\\pi} [\\sum_a q^{\\pi}(S_t, a) \\cdot \\nabla \\pi(a|S_t, \\theta)].\n\\end{align}\n$$\nThus, we can instantiate the stochastic gradient ascent algorithm (known as the \u003cem\u003eall-actions\u003c/em\u003e method, due to its update involving all of the actions) as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\theta_{t + 1} \\doteq \\theta_t + \\alpha \\sum_a \\hat{q}(S_t, a, w) \\cdot \\nabla \\pi (a|S_t, \\theta),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\hat{q}$$ is a learned approximation of $$q^{\\pi}$$.\nUnlike the previous method, the update step at time $$t$$ of the \u003cstrong\u003eREINFORCE\u003c/strong\u003e algorithm involves only $$A_t$$ (the action taken at time $$t$$). By multiplying and then dividing the summed terms by $$\\pi (a\\vert S_t, \\theta)$$, we can introduce the weighting needed for an expectation under $$\\pi$$., then, given that $$G_t$$ is the return, w.h.t.:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\nabla J(\\theta) \u0026#x26;\\propto \\mathbb{E}\u003cem\u003e{\\pi} [\\sum_a \\pi(a|S_t, \\theta) \\cdot q^{\\pi} (S_t,a) \\cdot \\frac{\\nabla \\pi(s|S_t, \\theta)}{\\pi(a|S_t, \\theta)}] \\nonumber\\\n\u0026#x26;= \\mathbb{E}\u003c/em\u003e{\\pi} [q^{\\pi}(S_t, A_t) \\cdot \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}] \\nonumber\\\n\u0026#x26;= \\mathbb{E}_{\\pi} [G_t \\cdot \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}].\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eSince the final (in brackets) expression is a quantity that can be sampled on each time step with expectation proportional to the gradient, such a sample can be used to instantiate the generic stochastic gradient ascent algorithm, yielding the REINFORCE update:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\theta_{t + 1} \u0026#x26;\\doteq \\theta_t + \\alpha \\cdot \\gamma^t \\cdot G_t \\cdot \\frac{\\nabla \\pi(A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} \\nonumber\\\n\u0026#x26;= \\theta_t + \\alpha \\cdot \\gamma^t \\cdot G_t \\cdot \\nabla \\ln \\pi(A_t| S_t, \\theta_t),\n\\end{align}\n$$\\\nwhere $$\\gamma$$ is the discount factor and $$\\ln \\pi(A_t|S_t, \\theta_t)$$ is the \u003cem\u003eeligibility\u003c/em\u003e vector. In this update, each increment is proportional to the return - causing the parameters to move most in the directions of the actions that yield the highest return - and is inversely proportional to the action probability - since the most frequent selected actions would have an advantage otherwise.\u003c/p\u003e\n\u003cp\u003eSince REINFORCE uses the complete return from time $$t$$ (including all future rewards until the end of an episode), it is considered a Monte Carlo algorithm and is only well defined in the episodic case with all updates made in retrospect after the episode's completion.\u003c/p\u003e\n\u003cp\u003eAs a stochastic gradient method, REINFORCE assures an improvement in the expected performance (given a small enough $$\\alpha$$) and convergence to a local optimum (under standard stochastic approximation conditions for decreasing $$\\alpha$$). However, as a Monte Carlo method, REINFORCE may have high variance and subsequently produce slow learning.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef episodic_reinforce(policy, environment, alpha, gamma):\n\tassert alpha \u003e 0;\n\tpolicy.theta = initialize_policy_parameters();\n\twhile(True):\n\t\tepisode = environment.generate_episode();\n\t\tfor step in episode:\n\t\t\tG = sum([R_k * gamma**(k-step.t-1) for k, R_k in enumerate(episode.rewards]));\n\t\t\tpolicy.theta += alpha * gamma**step.t * G * compute_gradient(log(policy[step.state][step.action]));\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 13.4: REINFORCE with Baseline\u003c/h3\u003e\n\u003cp\u003eGeneralizing the policy gradient theorem to include a comparison of the action value to an arbitrary baseline $$b(s)$$ gives the following expression:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\nabla J(\\theta) = \\sum_s \\mu (s) \\sum_a (q^{\\pi}(s, a) - b(s)) \\cdot \\nabla \\pi (a|s, \\theta).\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe baseline can be any arbitrary function (as long as it doesn't vary with $$a$$), since the equation will remain valid as the subtracted quantity is zero:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\sum_a b(s)\\cdot \\nabla \\pi(a|s, \\theta) = b(s) \\cdot \\nabla \\sum_a \\pi(a|s, \\theta) = b(s) \\cdot \\nabla 1 = 0.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe theorem can then be used to derive an update rule similar to the previous version of REINFORCE, but which includes a general baseline:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\theta_{t + 1} \\doteq \\theta_t + \\alpha \\cdot \\gamma^t \\cdot (G_t - b(S_t)) \\cdot \\frac{\\nabla \\pi(A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)}.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eWhile the baseline generally leaves the expected value of the update unchanged, it can have a significant effect on its variance and thus the learning speed.\u003c/p\u003e\n\u003cp\u003eThe value of the baseline should follow that of the actions, i.e., if all actions have high/low values, then the baseline must also have high/low value, so as to differentiate the higher valued actions from the lower valued ones. A common choice for the baseline is an estimate of state value $$\\hat{v}(S_t, w)$$, with $$w \\in \\mathbb{R}^d$$ being a learned weight vector.\u003c/p\u003e\n\u003cp\u003eThe algorithm has two step sizes, $$\\alpha_{\\theta}$$ (which is the same as the step size $$\\alpha$$ in previous equations) and $$\\alpha_{w}$$. A good rule of thumb for setting the step size for values in the linear case is\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\alpha_w = 0.1/\\mathbb{E}[||\\nabla \\hat{v}(S_t, w)]||_{\\mu}^2.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eThe step size for the policy parameters $$\\alpha_{\\theta}$$ will depend on the range of variation of the rewards and on the policy parameterization.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef episodic_reinforce_wbaseline(policy, state_value_function, environment, alpha_theta, alpha_w, gamma):\n\tassert alpha_theta \u003e 0;\n\tassert alpha_w \u003e 0;\n\tpolicy.theta = initialize_policy_parameters();\n\tstate_value_function.w = initialize_state_value_parameters();\n\twhile(True):\n\t\tepisode = environment.generate_episode();\n\t\tfor step in episode:\n\t\t\tG = sum([R_k * gamma**(k-step.t-1) for k, R_k in enumerate(episode.rewards]));\n\t\t\tdelta = G - state_value_function(step.state);\n\t\t\tstate_value_function.w += alpha_w * delta * compute_gradient(state_value_function(step.state));\n\t\t\tpolicy.theta += alpha_theta * gamma**step.t * delta * compute_gradient(log(policy[step.state][step.action]);)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 13.5: Actor-Critic Methods\u003c/h3\u003e\n\u003cp\u003eIn actor-critic methods, the state-value function is applied to the second state of the transition, unlike in REINFORCE, where the learned state-value function only estimates the value of the first state of each state transition and thus canÂ´t be used to assess that action. After discount and adding the estimated value of the second state to the reward, it constitutes the 1-step return $$G_{t:t+1}$$, which can be used to assess the action.\u003c/p\u003e\n\u003cp\u003eEven though the 1-step return introduces bias, it is often superior to the actual return in terms of its variance and computational congeniality. The bias can also be flexibly modulated through $$n$$-step returns and eligibility traces.\u003c/p\u003e\n\u003cp\u003e1-step actor-critic methods are analogs of the TD methods such as TD(0), Sarsa(0) and Q-learning. Such methods are appealing since they function in fully online and incremental manner, while avoiding the complexities of eligibility traces. In these methods, the full return of REINFORCE is replaced with the 1-step return (with a state-value function as the baseline) as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\theta_{t+1} \u0026#x26;\\doteq \\theta_t + \\alpha (G_{t:t+1} - \\hat{v}(S_t, w)) \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} \\nonumber\\\n\u0026#x26;= \\theta_t + \\alpha (R_{t+1} + \\gamma \\cdot \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)) \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} \\nonumber\\\n\u0026#x26;= \\theta_t + \\alpha \\cdot \\delta_t \\cdot \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)}.\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eA usual state value function learning method to pair with this is semi-gradient TD(0).\nTo generalize to the forward view of $$n$$-steps methods and to a $$\\lambda$$-return, one only needs to replace the on-step return in the previous equation by $$G_{t:t+1}$$ or $$G_t^{\\lambda}$$, respectively. The backward view of the $$\\lambda$$-return algorithm is also simple, only requiring using separate eligibility traces for the actor and critic.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef episodic_one_step_actor_critic(policy, state_value_function, environment, alpha_theta, alpha_w, gamma):\n\tassert alpha_theta \u003e 0;\n\tassert alpha_w \u003e 0;\n\tpolicy.theta = initialize_policy_parameters();\n\tstate_value_function.w = initialize_state_value_parameters();\n\twhile(True):\n\t\ts = environment.initialize();\n\t\tI = 1;\n\t\twhile s.not_terminal():\n\t\t\ta = policy[s];\n\t\t\tnext_state = environment.get_next_state(s, a);\n\t\t\tif next_state.is_terminal():\n\t\t\t\tvalue = 0;\n\t\t\telse:\n\t\t\t\tvalue = state_value_function[next_state];\n\t\t\tdelta = environment.get_reward(s, a) + gamma * value;\n\t\t\tstate_value_function.w += alpha_w * delta * compute_gradient(state_value_function[step.state]);\n\t\t\tpolicy.theta += alpha_theta * I * delta * compute_gradient(log(policy[step.state][step.action]));\n\t\t\tI = gamma * I;\n\t\t\ts = next_state;\n\ndef episodic_actor_critic_with_traces(policy, state_value_function, environment, lambda_theta, lambda_w, alpha_theta, alpha_w, gamma):\n\tassert 1 \u003e= lambda_theta \u003e= 0;\n\tassert 1 \u003e= lambda_w \u003e+ 0;\n\tassert alpha_theta \u003e 0;\n\tassert alpha_w \u003e 0;\n\tpolicy.theta = initialize_policy_parameters();\n\tstate_value_function.w = initialize_state_value_parameters();\n\twhile(True):\n\t\ts = environment.initialize();\n\t\tz_theta = 0;\n\t\tz_w = 0;\n\t\tI = 1;\n\t\twhile s.not_terminal():\n\t\t\ta = policy[s];\n\t\t\tnext_state = environment.get_next_state(s, a);\n\t\t\tif next_state.is_terminal():\n\t\t\t\tvalue = 0;\n\t\t\telse:\n\t\t\t\tvalue = state_value_function[next_state];\n\t\t\tdelta = environment.get_reward(s, a) + gamma * value - state_value_function[s];\n\t\t\tz_w = gamma * lambda_w * z_w + compute_gradient(state_value_function[step.state]);\n\t\t\tz_theta = gamma * lambda_theta * z_theta + I * compute_gradient(log(policy[step.state][step.action]));\n\t\t\tstate_value_function.w += alpha_w * delta * z_w;\n\t\t\tpolicy.theta += alpha_theta * delta * z_theta;\n\t\t\tI = gamma * I;\n\t\t\ts = next_state;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 13.6: Policy Gradient for Continuing Problems\u003c/h3\u003e\n\u003cp\u003eFor continuing problems without episode boundaries, the performance must be defined in terms of the average rate of reward per time step:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nJ(\\theta) \\doteq r(\\pi) \u0026#x26;\\doteq \\lim_{h \\rightarrow \\infty} \\frac{1}{h} \\sum_{t = 1}^h \\mathbb{E}[R_t \\vert S_0, A_{0:t - 1} \\sim \\pi] \\nonumber\\\n\u0026#x26;= \\lim_{t \\rightarrow \\infty} \\mathbb{E}[R_t \\vert S_0, A_{0:t - 1} \\sim \\pi] \\nonumber\\\n\u0026#x26;= \\sum_s \\mu(s) \\sum_a \\pi (a \\vert s) \\sum_{s', r} p(s', r \\vert s, a) \\cdot r,\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\mu$$ is the steady-state distribution under $$\\pi$$, $$\\mu(s) \\doteq \\lim_{t \\rightarrow \\infty} P(S_t = s \\vert A_{0:t} \\sim \\pi)$$,\nwhich is assumed to exist and - due to the ergodicity assumption - to be independent of $$S_0.$$ This is a special distribution under where, if you select actions according to $$\\pi$$, you remain in the same distribution, as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\sum_s \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\cdot p(s' \\vert s, a) = \\mu(s'), \\ \\forall s' \\in S.\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003eIn the continuing case, we define values: $$v^{\\pi} (s) \\doteq \\mathbb{E}_{\\pi} (G_t \\vert S_t = s)$$ and $$q^{\\pi}(s, a) \\doteq \\mathbb{E} [G_t \\vert S_t = s, A_t = a]$$, w.r.t. the differential return (s.t. the policy gradient theorem holds true for the continuing case):\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\nG_t \\doteq R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\dots \\ .\n\\end{equation}\n$$\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef continuing_actor_critic_with_traces(policy, state_value_function, environment, lambda_theta, lambda_w, alpha_theta, alpha_w, alpha_R):\n\tassert 1 \u003e= lambda_theta \u003e= 0;\n\tassert 1 \u003e= lambda_w \u003e+ 0;\n\tassert 1 \u003e= lambda_R \u003e+ 0;\n\tassert alpha_theta \u003e 0;\n\tassert alpha_w \u003e 0;\n\tR_bar = random_value();\n\tpolicy.theta = initialize_policy_parameters();\n\tstate_value_function.w = initialize_state_value_parameters();\n\ts = environment.initialize();\n\tz_theta = 0;\n\tz_w = 0;\n\twhile(True):\n\t\ta = policy[s];\n\t\tnext_state = environment.get_next_state(s, a);\n\t\tdelta = environment.get_reward(s, a) - R_bar + state_value_function[next_state] - state_value_function[s];\n\t\tR_bar += alpha_R * delta;\n\t\tz_w = lambda_w * z_w + compute_gradient(state_value_function[s]);\n\t\tz_theta = lambda_theta * z_theta + compute_gradient(log(policy[step.state][step.action]));\n\t\tstate_value_function.w += alpha_w * delta * z_w;\n\t\tpolicy.theta += alpha_theta * delta * z_theta;\n\t\ts = next_state;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSection 13.7: Policy Parameterization for Continuous Actions\u003c/h3\u003e\n\u003cp\u003eIn policy-based methods, instead of computing learned probabilities for each and every action, instead we learn statistics of the probability distribution, e.g., the action set might be $$\\mathbb{R}$$, with actions chosen from a normal (Gaussian) distribution.\nThe \u003cem\u003eProbability Density Function\u003c/em\u003e (PDF) for this (normal) distribution can be written as\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\np(x) \\doteq \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp(-\\frac{(x - \\mu)^2}{2 \\sigma^2}),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\mu$$ is the mean and $$\\sigma$$ the standard deviation of the normal distribution, and $$\\pi \\approx 3.14159$$.\u003c/p\u003e\n\u003cp\u003eIn order to make a policy parameterization, the policy may be defined as the normal probability density over a real-valued scalar action, with $$\\mu$$ and $$\\sigma$$ given by the parametric function approximators that depend on the state, i.e.,\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\pi (a \\vert s, \\theta) \\doteq \\frac{1}{\\sigma(s, \\theta)\\sqrt{2 \\pi}} \\exp \\bigg(-\\frac{(a - \\mu(s, \\theta))^2}{2 \\sigma(s, \\theta)^2} \\bigg),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$\\mu: \\mathcal{S} \\times \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}$$ and $$\\sigma : \\mathcal{S} \\times \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}^+$$ are two parameterized function approximators. Now, we just need to give a form for these approximators. To do this, we divide the policy's parameters into two parts, i.e., $$\\theta = [\\theta_{\\mu}, \\theta_{\\sigma}]$$, one part will be used to approximate $$\\mu$$ and the other to approximate $$\\sigma \u003e 0$$, the latter of which is better approximated as the exponential of a linear function. Thus\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{equation}\n\\mu (s, \\theta) \\doteq \\theta_{\\mu}^T x_{\\mu} \\wedge \\sigma(s, \\theta) \\doteq \\exp \\bigg(\\theta_{\\sigma}^T x_{\\sigma}(s) \\bigg),\n\\end{equation}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $$x_{\\mu}(s)$$ and $$x_{\\sigma}(s)$$ are state feature vectors. With these additional definitions, all previously described algorithms can be applied to learn to select real-valued actions.\u003c/p\u003e\n\u003ch3\u003eSection 13.8: Summary\u003c/h3\u003e\n\u003cp\u003ePrior to this chapter: focus on \u003cem\u003eaction-value methods\u003c/em\u003e - which are methods that learn action values and then use them to select actions.\u003c/p\u003e\n\u003cp\u003eDuring this chapter: describes methods that learn parameterized policies that enable actions to be taken without consulting action-value estimates, with a focus on \u003cem\u003epolicy-gradient methods\u003c/em\u003e - which are methods that, on each step, update the policy parameter in the direction of an estimate of the gradient of the performance w.r.t. the policy parameter.\u003c/p\u003e\n\u003cp\u003eAdvantages of methods that learn and store a policy parameter:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThey can learn specific probabilities for taking actions;\u003c/li\u003e\n\u003cli\u003eThey can learn appropriate levels of exploration and approach deterministic policies asymptotically;\u003c/li\u003e\n\u003cli\u003eThey can inherently handle continuous action spaces;\u003c/li\u003e\n\u003cli\u003eThe policy may be simpler to represent parametrically than the value function on some problems;\u003c/li\u003e\n\u003cli\u003eThe \u003cem\u003epolicy gradient theorem\u003c/em\u003e provides an exact formula (that doesn't involve derivatives of the state distribution) for how performance is affected by the policy parameter.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA state-value function baseline reduces the variance of the REINFORCE method without introducing bias. If the state-value function is (also) used to assess the policy's action selections, then it is called a \u003cem\u003ecritic\u003c/em\u003e, the policy is called an \u003cem\u003eactor\u003c/em\u003e, and the overall algorithm is called an \u003cem\u003eactor-critic method\u003c/em\u003e. The critic introduces bias into the actorâ€™s gradient estimates, but this is often desirable since it substantially reduces variance (similar to the advantage bootstrapping TD methods have over Monte Carlo methods).\u003c/p\u003e\n\u003ch1\u003ePart III: Looking Deeper\u003c/h1\u003e\n\u003ch2\u003eChapter 14: Psychology\u003c/h2\u003e\n\u003ch2\u003eChapter 15: Neuroscience\u003c/h2\u003e\n\u003ch2\u003eChapter 16: Applications and Case Studies\u003c/h2\u003e\n\u003ch2\u003eChapter 17: Frontiers\u003c/h2\u003e\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto py-12 px-4\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold font-display text-slate-900 dark:text-white mb-4\",\"children\":\"Notes on Reinforcement Learning: An Introduction (2nd edition)\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-slate-500 mb-8\",\"children\":[\"Published on \",\"2024-10-31\",\" | Category: \",\"Uncategorized\"]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none\",\"dangerouslySetInnerHTML\":{\"__html\":\"$e\"}}]]}]\nb:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"2\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>